{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#tpot2-alpha","title":"TPOT2 ALPHA","text":"<p>TPOT stands for Tree-based Pipeline Optimization Tool. TPOT2 is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. Consider TPOT2 your Data Science Assistant.</p> <p>TPOT2 is a rewrite of TPOT with some additional functionality. Notably, we added support for graph-based pipelines and additional parameters to better specify the desired search space.  TPOT2 is currently in Alpha. This means that there will likely be some backwards incompatible changes to the API as we develop. Some implemented features may be buggy. There is a list of known issues written at the bottom of this README. Some features have placeholder names or are listed as \"Experimental\" in the doc string. These are features that may not be fully implemented and may or may not work with all other features.</p> <p>If you are interested in using the current stable release of TPOT, you can do that here: https://github.com/EpistasisLab/tpot/. </p>"},{"location":"#license","title":"License","text":"<p>Please see the repository license for the licensing and usage information for TPOT2. Generally, we have licensed TPOT2 to make it as widely usable as possible.</p>"},{"location":"#installation","title":"Installation","text":"<p>TPOT2 requires a working installation of Python.</p>"},{"location":"#creating-a-conda-environment-optional","title":"Creating a conda environment (optional)","text":"<p>We recommend using conda environments for installing TPOT2, though it would work equally well if manually installed without it.</p> <p>More information on making anaconda environments found here.</p> <pre><code>conda create --name tpot2env python=3.10\nconda activate tpot2env\n</code></pre>"},{"location":"#note-for-m1-mac-or-other-arm-based-cpu-users","title":"Note for M1 Mac or other Arm-based CPU users","text":"<p>You need to install the lightgbm package directly from conda using the following command before installing TPOT2. </p> <p>This is to ensure that you get the version that is compatible with your system.</p> <pre><code>conda install --yes -c conda-forge 'lightgbm&gt;=3.3.3'\n</code></pre>"},{"location":"#developerlatest-branch-installation","title":"Developer/Latest Branch Installation","text":"<pre><code>pip install -e /path/to/tpot2repo\n</code></pre> <p>If you downloaded with git pull, then the repository folder will be named TPOT2. (Note: this folder is the one that includes setup.py inside of it and not the folder of the same name inside it). If you downloaded as a zip, the folder may be called tpot2-main. </p>"},{"location":"#usage","title":"Usage","text":"<p>See the Tutorials Folder for more instructions and examples.</p>"},{"location":"#best-practices","title":"Best Practices","text":""},{"location":"#1","title":"1","text":"<p>TPOT2 uses dask for parallel processing. When Python is parallelized, each module is imported within each processes. Therefore it is important to protect all code within a <code>if __name__ == \"__main__\"</code> when running TPOT2 from a script. This is not required when running TPOT2 from a notebook.</p> <p>For example:</p> <pre><code>#my_analysis.py\n\nimport tpot2\nif __name__ == \"__main__\":\n    X, y = load_my_data()\n    est = tpot2.TPOTClassifier()\n    est.fit(X,y)\n    #rest of analysis\n</code></pre>"},{"location":"#2","title":"2","text":"<p>When designing custom objective functions, avoid the use of global variables.</p> <p>Don't Do: <pre><code>global_X = [[1,2],[4,5]]\nglobal_y = [0,1]\ndef foo(est):\n    return my_scorer(est, X=global_X, y=global_y)\n</code></pre></p> <p>Instead use a partial</p> <pre><code>from functools import partial\n\ndef foo_scorer(est, X, y):\n    return my_scorer(est, X, y)\n\nif __name__=='__main__':\n    X = [[1,2],[4,5]]\n    y = [0,1]\n    final_scorer = partial(foo_scorer, X=X, y=y)\n</code></pre> <p>Similarly when using lambda functions.</p> <p>Dont Do:</p> <pre><code>def new_objective(est, a, b)\n    #definition\n\na = 100\nb = 20\nbad_function = lambda est :  new_objective(est=est, a=a, b=b)\n</code></pre> <p>Do: <pre><code>def new_objective(est, a, b)\n    #definition\n\na = 100\nb = 20\ngood_function = lambda est, a=a, b=b : new_objective(est=est, a=a, b=b)\n</code></pre></p>"},{"location":"#tips","title":"Tips","text":"<p>TPOT2 will not check if your data is correctly formatted. It will assume that you have passed in operators that can handle the type of data that was passed in. For instance, if you pass in a pandas dataframe with categorical features and missing data, then you should also include in your configuration operators that can handle those feautures of the data. Alternatively, if you pass in <code>preprocessing = True</code>, TPOT2 will impute missing values, one hot encode categorical features, then standardize the data. (Note that this is currently fitted and transformed on the entire training set before splitting for CV. Later there will be an option to apply per fold, and have the parameters be learnable.)</p> <p>Setting <code>verbose</code> to 5 can be helpful during debugging as it will print out the error generated by failing pipelines. </p>"},{"location":"#contributing-to-tpot2","title":"Contributing to TPOT2","text":"<p>We welcome you to check the existing issues for bugs or enhancements to work on. If you have an idea for an extension to TPOT2, please file a new issue so we can discuss it.</p>"},{"location":"#known-issues","title":"Known issues","text":"<ul> <li>TPOT2 uses the func_timeout package to terminate long running pipelines. The early termination signal may fail on particular estimators and cause TPOT2 to run for longer than intended. If you are using your own custom configuration dictionaries, and are noticing that TPOT2 is running for longer than intended, this may be the issue. We are currently looking into it. Sometimes restarting TPOT2 resolves the issue.</li> <li>Periodic checkpoint folder may not correctly resume if using budget and/or initial_population size.</li> <li>Population class is slow to add new individuals. The Population class needs to be updated to use a dictionary for storage rather than a pandas dataframe.</li> <li>Crossover may sometimes go over the size restrictions.</li> <li>Memory caching with GraphPipeline may miss some nodes where the ordering on inputs happens to be different between two nodes. </li> </ul>"},{"location":"#support-for-tpot2","title":"Support for TPOT2","text":"<p>TPOT2 was developed in the Artificial Intelligence Innovation (A2I) Lab at Cedars-Sinai with funding from the NIH under grants U01 AG066833 and R01 LM010098. We are incredibly grateful for the support of the NIH and the Cedars-Sinai during the development of this project.</p> <p>The TPOT logo was designed by Todd Newmuis, who generously donated his time to the project.</p>"},{"location":"cite/","title":"Citing TPOT2","text":""},{"location":"contribute/","title":"Contributing","text":"<p>We welcome you to check the existing issues for bugs or enhancements to work on. If you have an idea for an extension to TPOT, please file a new issue so we can discuss it.</p>"},{"location":"installation/","title":"Installation","text":"<p>TPOT2 requires a working installation of Python.</p>"},{"location":"installation/#creating-a-conda-environment-optional","title":"Creating a conda environment (optional)","text":"<p>We recommend using conda environments for installing TPOT2, though it would work equally well if manually installed without it.</p> <p>More information on making anaconda environments found here.</p> <pre><code>conda create --name tpot2env python=3.10\nconda activate tpot2env\n</code></pre>"},{"location":"installation/#note-for-m1-mac-or-other-arm-based-cpu-users","title":"Note for M1 Mac or other Arm-based CPU users","text":"<p>You need to install the lightgbm package directly from conda using the following command before installing TPOT2. </p> <p>This is to ensure that you get the version that is compatible with your system.</p> <pre><code>conda install --yes -c conda-forge 'lightgbm&gt;=3.3.3'\n</code></pre>"},{"location":"installation/#developerlatest-branch-installation","title":"Developer/Latest Branch Installation","text":"<pre><code>pip install -e /path/to/tpot2repo\n</code></pre> <p>If you downloaded with git pull, then the repository folder will be named TPOT2. (Note: this folder is the one that includes setup.py inside of it and not the folder of the same name inside it). If you downloaded as a zip, the folder may be called tpot2-main. </p>"},{"location":"related/","title":"Related","text":"<p>Other Automated Machine Learning (AutoML) tools and related projects:</p> Name Language License Description Auto-WEKA Java GPL-v3 Automated model selection and hyper-parameter tuning for Weka models. auto-sklearn Python BSD-3-Clause An automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator. auto_ml Python MIT Automated machine learning for analytics &amp; production. Supports manual feature type declarations. H2O AutoML Java with Python, Scala &amp; R APIs and web GUI Apache 2.0 Automated: data prep, hyperparameter tuning, random grid search and stacked ensembles in a distributed ML platform. devol Python MIT Automated deep neural network design via genetic programming. MLBox Python BSD-3-Clause Accurate hyper-parameter optimization in high-dimensional space with support for distributed computing. Recipe C GPL-v3 Machine-learning pipeline optimization through genetic programming. Uses grammars to define pipeline structure. Xcessiv Python Apache 2.0 A web-based application for quick, scalable, and automated hyper-parameter tuning and stacked ensembling in Python. GAMA Python Apache 2.0 Machine-learning pipeline optimization through asynchronous evaluation based genetic programming.  PyMoo Python Apache 2.0 Multi-objective optimization in Python.  Karoo GP Python MIT A Python based genetic programming application suite with support for symbolic regression and classification.  MABE C++ See here A Python based genetic programming application suite with support for symbolic regression and classification.  SBBFramework Python BSD-2-Clause Python implementation of Symbiotic Bid-Based (SBB) framework for problem decomposition using Genetic Programming (GP).  Tiny GP Python GPL-v3 A minimalistic program implementing Koza-style (tree-based) genetic programming to solve a symbolic regression problem.  Baikal Python BSD-3-Clause A graph-based functional API for building complex scikit-learn pipelines.  skdag Python MIT A more flexible alternative to scikit-learn Pipelines.  d6tflow Python MIT A python library which makes building complex data science workflows easy, fast and intuitive."},{"location":"support/","title":"Support","text":"<p>TPOT2 was developed in the Artificial Intelligence Innovation (A2I) Lab at Cedars-Sinai with funding from the NIH under grants U01 AG066833 and R01 LM010098. We are incredibly grateful for the support of the NIH and the Cedars-Sinai during the development of this project.</p> <p>The TPOT logo was designed by Todd Newmuis, who generously donated his time to the project.</p>"},{"location":"using/","title":"Using TPOT2","text":""},{"location":"Tutorial/1_Estimators_Overview/","title":"1 Estimators Overview","text":"<p>TPOT1 and TPOTSteady use a standard evolutionary algorithm that evaluates exactly population_size individuals each generation. The next generation does not start until the previous is completely finished evaluating. This leads to underutilized CPU time as the cores are waiting for the last individuals to finish training.</p> <p>TPOTEstimatorSteadyState will generate and evaluate the next individual as soon as an individual finishes evaluation. The number of individuals being evaluated is determined by the n_jobs parameter. There is no longer a concept of generations. The population_size parameter now refers to the size of the list of evaluated parents. When an individual is evaluated, the selection method updates the list of parents. Then</p> In\u00a0[2]: Copied! <pre>import tpot2\nimport sklearn\nimport sklearn.datasets\n\nest = tpot2.TPOTEstimatorSteadyState(  population_size=30,\n                                     initial_population_size=30,\n                            scorers=['roc_auc_ovr'], #scorers can be a list of strings or a list of scorers. These get evaluated during cross validation. \n                            scorers_weights=[1],\n\n                            classification=True,\n                            n_jobs=1,\n                                                        #List of other objective functions. All objective functions take in an untrained GraphPipeline and return a score or a list of scores\n                            other_objective_functions= [ ],\n                            \n                            #List of weights for the other objective functions. Must be the same length as other_objective_functions. By default, bigger is better is set to True. \n                            other_objective_functions_weights=[],\n\n                            max_eval_time_seconds=15,\n                            max_time_seconds=30,\n                            verbose=2)\n\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovo')\nX, y = sklearn.datasets.load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\n</pre> import tpot2 import sklearn import sklearn.datasets  est = tpot2.TPOTEstimatorSteadyState(  population_size=30,                                      initial_population_size=30,                             scorers=['roc_auc_ovr'], #scorers can be a list of strings or a list of scorers. These get evaluated during cross validation.                              scorers_weights=[1],                              classification=True,                             n_jobs=1,                                                         #List of other objective functions. All objective functions take in an untrained GraphPipeline and return a score or a list of scores                             other_objective_functions= [ ],                                                          #List of weights for the other objective functions. Must be the same length as other_objective_functions. By default, bigger is better is set to True.                              other_objective_functions_weights=[],                              max_eval_time_seconds=15,                             max_time_seconds=30,                             verbose=2)   scorer = sklearn.metrics.get_scorer('roc_auc_ovo') X, y = sklearn.datasets.load_iris(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25) est.fit(X_train, y_train) print(scorer(est, X_test, y_test))  <pre>Evaluations: : 111it [00:30,  3.64it/s]\n</pre> <pre>1.0\n</pre> In\u00a0[3]: Copied! <pre>fitted_pipeline = est.fitted_pipeline_ # access best pipeline directly\nfitted_pipeline.plot()\n</pre> fitted_pipeline = est.fitted_pipeline_ # access best pipeline directly fitted_pipeline.plot() In\u00a0[4]: Copied! <pre>#view the summary of all evaluated individuals as a pandas dataframe\nest.evaluated_individuals\n</pre> #view the summary of all evaluated individuals as a pandas dataframe est.evaluated_individuals Out[4]: roc_auc_score Parents Variation_Function Individual Submitted Timestamp Completed Timestamp Pareto_Front Instance 0 0.99631 NaN NaN ['LogisticRegression_1'] 1.690848e+09 1.690848e+09 NaN ['LogisticRegression_1'] 1 0.982956 NaN NaN ['DecisionTreeClassifier_1'] 1.690848e+09 1.690848e+09 NaN ['DecisionTreeClassifier_1'] 2 0.953313 NaN NaN ['KNeighborsClassifier_1'] 1.690848e+09 1.690848e+09 NaN ['KNeighborsClassifier_1'] 3 0.5 NaN NaN ['GradientBoostingClassifier_1'] 1.690848e+09 1.690848e+09 NaN ['GradientBoostingClassifier_1'] 4 0.983413 NaN NaN ['ExtraTreesClassifier_1'] 1.690848e+09 1.690848e+09 NaN ['ExtraTreesClassifier_1'] ... ... ... ... ... ... ... ... ... 106 0.989742 (104,) mutate [('MLPClassifier_1', 'SelectPercentile_1')] 1.690848e+09 1.690848e+09 NaN [('MLPClassifier_1', 'SelectPercentile_1')] 107 0.99631 (12,) mutate ['MLPClassifier_1'] 1.690848e+09 1.690848e+09 NaN ['MLPClassifier_1'] 108 0.99754 (97, 93) crossover_then_mutate [('MLPClassifier_1', 'PolynomialFeatures_1'), ... 1.690848e+09 1.690848e+09 NaN [('MLPClassifier_1', 'PolynomialFeatures_1'), ... 109 0.989484 (80, 61) crossover [('MLPClassifier_1', 'FastICA_1')] 1.690848e+09 1.690848e+09 NaN [('MLPClassifier_1', 'FastICA_1')] 110 0.994008 (71,) mutate [('GradientBoostingClassifier_1', 'PolynomialF... 1.690848e+09 1.690848e+09 NaN [('GradientBoostingClassifier_1', 'PolynomialF... <p>111 rows \u00d7 8 columns</p> In\u00a0[5]: Copied! <pre>#view pareto front as a pandas dataframe\nest.pareto_front\n</pre> #view pareto front as a pandas dataframe est.pareto_front Out[5]: roc_auc_score Parents Variation_Function Individual Submitted Timestamp Completed Timestamp Pareto_Front Instance 69 0.99754 (51,) mutate [('LogisticRegression_1', 'PolynomialFeatures_... 1.690848e+09 1.690848e+09 1.0 [('LogisticRegression_1', 'PolynomialFeatures_... <p>TPOTEstimator does a standard evolutionary algorithm. In this version, the next generation doesn't start evaluation until all individuals in the previous generation are finished evaluating.</p> In\u00a0[1]: Copied! <pre>import tpot2\nimport sklearn\nimport sklearn.datasets\n\nest = tpot2.TPOTEstimator(  population_size=30,\n                            generations=5,\n                            scorers=['roc_auc_ovr'], #scorers can be a list of strings or a list of scorers. These get evaluated during cross validation. \n                            scorers_weights=[1],\n                            classification=True,\n                            n_jobs=1, \n                            early_stop=5, #how many generations with no improvement to stop after\n                            \n                            #List of other objective functions. All objective functions take in an untrained GraphPipeline and return a score or a list of scores\n                            other_objective_functions= [ ],\n                            \n                            #List of weights for the other objective functions. Must be the same length as other_objective_functions. By default, bigger is better is set to True. \n                            other_objective_functions_weights=[],\n                            verbose=2)\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovo')\nX, y = sklearn.datasets.load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\n</pre> import tpot2 import sklearn import sklearn.datasets  est = tpot2.TPOTEstimator(  population_size=30,                             generations=5,                             scorers=['roc_auc_ovr'], #scorers can be a list of strings or a list of scorers. These get evaluated during cross validation.                              scorers_weights=[1],                             classification=True,                             n_jobs=1,                              early_stop=5, #how many generations with no improvement to stop after                                                          #List of other objective functions. All objective functions take in an untrained GraphPipeline and return a score or a list of scores                             other_objective_functions= [ ],                                                          #List of weights for the other objective functions. Must be the same length as other_objective_functions. By default, bigger is better is set to True.                              other_objective_functions_weights=[],                             verbose=2)  scorer = sklearn.metrics.get_scorer('roc_auc_ovo') X, y = sklearn.datasets.load_iris(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25) est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) <pre>Generation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:35&lt;00:00,  7.17s/it]\n</pre> <pre>0.9947089947089948\n</pre> <p>The TPOTClassifier and TPOTRegressor are set default parameters for the TPOTEstimator for Classification and Regression. In the future, a metalearner will be used to predict the best values for a given dataset.</p> In\u00a0[5]: Copied! <pre>import tpot2\nimport sklearn\nimport sklearn.metrics\nimport sklearn.datasets\n\nest = tpot2.tpot_estimator.templates.TPOTRegressor(n_jobs=4, max_time_seconds=10)\n\n\nscorer = sklearn.metrics.get_scorer('neg_mean_squared_error')\nX, y = sklearn.datasets.load_diabetes(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\n</pre> import tpot2 import sklearn import sklearn.metrics import sklearn.datasets  est = tpot2.tpot_estimator.templates.TPOTRegressor(n_jobs=4, max_time_seconds=10)   scorer = sklearn.metrics.get_scorer('neg_mean_squared_error') X, y = sklearn.datasets.load_diabetes(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25) est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) <pre>terminating parallel evaluation due to timeout\n-4522.135573056978\n</pre> In\u00a0[6]: Copied! <pre>import tpot2\nimport sklearn\nimport sklearn.datasets\n\nest = tpot2.tpot_estimator.templates.TPOTClassifier(n_jobs=4, max_time_seconds=10)\n\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovo')\nX, y = sklearn.datasets.load_digits(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\n</pre> import tpot2 import sklearn import sklearn.datasets  est = tpot2.tpot_estimator.templates.TPOTClassifier(n_jobs=4, max_time_seconds=10)   scorer = sklearn.metrics.get_scorer('roc_auc_ovo') X, y = sklearn.datasets.load_digits(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25) est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) <pre>terminating parallel evaluation due to timeout\n0.9999490921793659\n</pre>"},{"location":"Tutorial/2_Defining_Search_Space_%28config_dicts%29/","title":"2 Defining Search Space (config dicts)","text":"<p>Everything can be done with the TPOTEstimator class. All other classes (TPOTRegressor, TPOTClassifier, TPOTSymbolicClassifier, TPOTSymbolicRegression, TPOTGeneticFeatureSetSelector, etc.) are actually just different default settings for TPOTEstimator.</p> <p>By Default, TPOT will generate pipelines with a default set of classifiers or regressors as roots (this depends on whether classification is set to true or false). All other nodes are selected from a default list of selectors and transformers. Note: This differs from the TPOT1 behavior where by default classifiers and regressors can appear in locations other than the root. You can modify the the search space for leaves, inner nodes, and roots (final classifiers) separately through built in options or custom configuration dictionaries.</p> <p>In this tutorial we will walk through using the built in configurations, creating custom configurations, and using nested configurations.</p> In\u00a0[1]: Copied! <pre># A Linear pipeline starting with a selector, followed by 0 to 4 transformers, and ending with a classifier.\n\nimport tpot2\nimport sklearn\nimport sklearn.datasets\n\nest = tpot2.TPOTEstimator(  population_size=10,\n                            generations=5,\n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            classification=True,\n                            root_config_dict=\"classifiers\",\n                            inner_config_dict= \"transformers\",\n                            leaf_config_dict=\"selectors\",\n                            linear_pipeline=True,\n                            max_size=6,\n\n                            early_stop=5,\n                            verbose=0)\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovo')\nX, y = sklearn.datasets.load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\nest.fitted_pipeline_.plot()\n</pre> # A Linear pipeline starting with a selector, followed by 0 to 4 transformers, and ending with a classifier.  import tpot2 import sklearn import sklearn.datasets  est = tpot2.TPOTEstimator(  population_size=10,                             generations=5,                             scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             classification=True,                             root_config_dict=\"classifiers\",                             inner_config_dict= \"transformers\",                             leaf_config_dict=\"selectors\",                             linear_pipeline=True,                             max_size=6,                              early_stop=5,                             verbose=0)  scorer = sklearn.metrics.get_scorer('roc_auc_ovo') X, y = sklearn.datasets.load_iris(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25) est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) est.fitted_pipeline_.plot() <pre>1.0\n</pre> In\u00a0[2]: Copied! <pre># A Graph pipeline starting with at least one selector as a leaf, potentially followed by a series\n# of stacking classifiers or transformers, and ending with a classifier. The graph will have at most 15 nodes.\n\nimport tpot2\nimport sklearn\nimport sklearn.datasets\nimport numpy as np\n\nest = tpot2.TPOTEstimator(  population_size=10,\n                            generations=5,\n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            classification=True,\n                            root_config_dict=\"classifiers\",\n                            inner_config_dict= [\"classifiers\",\"transformers\"],\n                            leaf_config_dict=\"selectors\",\n                            max_size=15,\n\n                            early_stop=5,\n                            verbose=0)\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovo')\nX, y = sklearn.datasets.load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\n\nest.fitted_pipeline_.plot()\n</pre> # A Graph pipeline starting with at least one selector as a leaf, potentially followed by a series # of stacking classifiers or transformers, and ending with a classifier. The graph will have at most 15 nodes.  import tpot2 import sklearn import sklearn.datasets import numpy as np  est = tpot2.TPOTEstimator(  population_size=10,                             generations=5,                             scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             classification=True,                             root_config_dict=\"classifiers\",                             inner_config_dict= [\"classifiers\",\"transformers\"],                             leaf_config_dict=\"selectors\",                             max_size=15,                              early_stop=5,                             verbose=0)  scorer = sklearn.metrics.get_scorer('roc_auc_ovo') X, y = sklearn.datasets.load_iris(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25) est.fit(X_train, y_train) print(scorer(est, X_test, y_test))  est.fitted_pipeline_.plot() <pre>0.9941520467836257\n</pre> In\u00a0[3]: Copied! <pre>import tpot2\nimport numpy as np\ndef params_LogisticRegression(trial, name=None):\n    params = {}\n    params['solver'] = trial.suggest_categorical(name=f'solver_{name}',\n                                                 choices=[f'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'])\n    params['dual'] = False\n    params['penalty'] = 'l2'\n    params['C'] = trial.suggest_float(f'C_{name}', 1e-4, 1e4, log=True)\n    params['l1_ratio'] = None\n    if params['solver'] == 'liblinear':\n        params['penalty'] = trial.suggest_categorical(name=f'penalty_{name}', choices=['l1', 'l2'])\n        if params['penalty'] == 'l2':\n            params['dual'] = trial.suggest_categorical(name=f'dual_{name}', choices=[True, False])\n        else:\n            params['penalty'] = 'l1'\n\n    params['class_weight'] = trial.suggest_categorical(name=f'class_weight_{name}', choices=['balanced'])\n    param_grid = {'solver': params['solver'],\n                  'penalty': params['penalty'],\n                  'dual': params['dual'],\n                  'multi_class': 'auto',\n                  'l1_ratio': params['l1_ratio'],\n                  'C': params['C'],\n                  }\n    return param_grid\n</pre> import tpot2 import numpy as np def params_LogisticRegression(trial, name=None):     params = {}     params['solver'] = trial.suggest_categorical(name=f'solver_{name}',                                                  choices=[f'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'])     params['dual'] = False     params['penalty'] = 'l2'     params['C'] = trial.suggest_float(f'C_{name}', 1e-4, 1e4, log=True)     params['l1_ratio'] = None     if params['solver'] == 'liblinear':         params['penalty'] = trial.suggest_categorical(name=f'penalty_{name}', choices=['l1', 'l2'])         if params['penalty'] == 'l2':             params['dual'] = trial.suggest_categorical(name=f'dual_{name}', choices=[True, False])         else:             params['penalty'] = 'l1'      params['class_weight'] = trial.suggest_categorical(name=f'class_weight_{name}', choices=['balanced'])     param_grid = {'solver': params['solver'],                   'penalty': params['penalty'],                   'dual': params['dual'],                   'multi_class': 'auto',                   'l1_ratio': params['l1_ratio'],                   'C': params['C'],                   }     return param_grid In\u00a0[4]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nroot_config_dict = { LogisticRegression : params_LogisticRegression }\n</pre>  from sklearn.linear_model import LogisticRegression root_config_dict = { LogisticRegression : params_LogisticRegression } In\u00a0[5]: Copied! <pre>est = tpot2.TPOTEstimator(population_size=20,generations=10, \n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            classification=True,\n                            inner_config_dict= \"arithmetic_transformer\",\n                            leaf_config_dict=\"feature_set_selector\",\n                            root_config_dict=root_config_dict,\n                            )\n\n#load iris\nscorer = sklearn.metrics.get_scorer('roc_auc_ovo')\nX, y = sklearn.datasets.load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\nest.fitted_pipeline_.plot()\n</pre>   est = tpot2.TPOTEstimator(population_size=20,generations=10,                              scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             classification=True,                             inner_config_dict= \"arithmetic_transformer\",                             leaf_config_dict=\"feature_set_selector\",                             root_config_dict=root_config_dict,                             )  #load iris scorer = sklearn.metrics.get_scorer('roc_auc_ovo') X, y = sklearn.datasets.load_iris(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25) est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) est.fitted_pipeline_.plot() <pre>/home/ribeirop/miniconda3/envs/tpot2env/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n</pre> <pre>1.0\n</pre> In\u00a0[6]: Copied! <pre>transformer_config_dictionary = \"transformers\"\nselector_config_dictionary = \"feature_set_selector\"\nclassifier_config_dictionary = root_config_dict \n\n#Some example search spaces with nested graphs\n\n#pipelines of the shape selector-&gt;transformer\nst_params =  { \n                            'root_config_dict':transformer_config_dictionary,\n                            'leaf_config_dict':selector_config_dictionary,\n                            'inner_config_dict': None,\n                            'max_size' : 2, \n                            'linear_pipeline' : True}\n\n#pipelines of the shape (selector-&gt;transformer) -&gt; classifier. \n# This is equivalent to setting TPOT1 to use the 'Selector-Transformer-Classifier' template\nst_c_params = { \n                            'root_config_dict': classifier_config_dictionary,\n                            'leaf_config_dict': {\"Recursive\" : st_params},\n                            'inner_config_dict': None,\n                            'max_size' : 2, \n                            'linear_pipeline' : True}\n\n#pipelines of the shape ((selector-&gt;transformer) -&gt; classifier)*N) -&gt; classifier\n#This is like having an ensemble of 'Selector-Transformer-Classifier' models with a final meta classifier\nst_c_ensemble_params = { \n                            'root_config_dict': classifier_config_dictionary,\n                            'leaf_config_dict': {\"Recursive\" : st_c_params},\n                            'inner_config_dict': None,\n                            'max_size' : 6, \n                            'linear_pipeline' : True}\n</pre> transformer_config_dictionary = \"transformers\" selector_config_dictionary = \"feature_set_selector\" classifier_config_dictionary = root_config_dict   #Some example search spaces with nested graphs  #pipelines of the shape selector-&gt;transformer st_params =  {                              'root_config_dict':transformer_config_dictionary,                             'leaf_config_dict':selector_config_dictionary,                             'inner_config_dict': None,                             'max_size' : 2,                              'linear_pipeline' : True}  #pipelines of the shape (selector-&gt;transformer) -&gt; classifier.  # This is equivalent to setting TPOT1 to use the 'Selector-Transformer-Classifier' template st_c_params = {                              'root_config_dict': classifier_config_dictionary,                             'leaf_config_dict': {\"Recursive\" : st_params},                             'inner_config_dict': None,                             'max_size' : 2,                              'linear_pipeline' : True}  #pipelines of the shape ((selector-&gt;transformer) -&gt; classifier)*N) -&gt; classifier #This is like having an ensemble of 'Selector-Transformer-Classifier' models with a final meta classifier st_c_ensemble_params = {                              'root_config_dict': classifier_config_dictionary,                             'leaf_config_dict': {\"Recursive\" : st_c_params},                             'inner_config_dict': None,                             'max_size' : 6,                              'linear_pipeline' : True} In\u00a0[7]: Copied! <pre># linear pipelines of the shape selector-&gt;transformer-&gt;classifier\nest = tpot2.TPOTEstimator(population_size=20,generations=10, \n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            classification=True,\n                            **st_c_params,\n                            )\n\n#load iris\nscorer = sklearn.metrics.get_scorer('roc_auc_ovo')\nX, y = sklearn.datasets.load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\nest.fitted_pipeline_.plot()\n</pre> # linear pipelines of the shape selector-&gt;transformer-&gt;classifier est = tpot2.TPOTEstimator(population_size=20,generations=10,                              scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             classification=True,                             **st_c_params,                             )  #load iris scorer = sklearn.metrics.get_scorer('roc_auc_ovo') X, y = sklearn.datasets.load_iris(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25) est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) est.fitted_pipeline_.plot() <pre>0.9880174291938998\n</pre> In\u00a0[8]: Copied! <pre># ensembles of linear pipelines of the shape selector-&gt;transformer-&gt;classifier ensemble pipeline with a final meta classifier\nest = tpot2.TPOTEstimator(population_size=20,generations=10, \n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            classification=True,\n                            **st_c_ensemble_params,\n                            )\n\n#load iris\nscorer = sklearn.metrics.get_scorer('roc_auc_ovo')\nX, y = sklearn.datasets.load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\nest.fitted_pipeline_.plot()\n</pre> # ensembles of linear pipelines of the shape selector-&gt;transformer-&gt;classifier ensemble pipeline with a final meta classifier est = tpot2.TPOTEstimator(population_size=20,generations=10,                              scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             classification=True,                             **st_c_ensemble_params,                             )  #load iris scorer = sklearn.metrics.get_scorer('roc_auc_ovo') X, y = sklearn.datasets.load_iris(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25) est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) est.fitted_pipeline_.plot() <pre>/home/ribeirop/miniconda3/envs/tpot2env/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n</pre> <pre>0.9976851851851851\n</pre>"},{"location":"Tutorial/2_Defining_Search_Space_%28config_dicts%29/#configuration-dictionaries","title":"Configuration Dictionaries\u00b6","text":"<p>The default configuration includes several machine learning estimators from sklearn. Sometimes we may want to change or restrict what is allowed.</p> <p>In TPOT2, we specify three different configuration dictionaries to indicate which modules can go where on the graph</p> <ol> <li>root_config_dict  : Specifies the modules allowed to be placed in the root node. This is the final classifier or regressor. (It can also technically be used as a transformer if the scoring function knows that.). You are guaranteed a root node in every graph pipeline.</li> <li>inner_config_dict : Specifies the modules allowed to be placed all nodes that are not the root node. If leaf_config_dict is set to None, then leaves will be pulled from this list. You are not guaranteed a node from this list however. It is still possible to end up with a graph that contains only a single root, or a root and a leaf even if this is set.</li> <li>leaf_config_dict  : Specifies the modules allowed to be placed as leafs. Unlike inner_config_dict, you are guaranteed to have a leaf node from this list if it is set. The smallest possible graph would thus be [leaf-&gt;root].</li> </ol> <p>Note: TPOT1 internally divided the methods inside the configuration dictionary into selectors/transformers/estimators and treated them differently. TPOT2 does not.</p>"},{"location":"Tutorial/2_Defining_Search_Space_%28config_dicts%29/#built-in-defaults","title":"Built in Defaults\u00b6","text":"<p>Each configuration dictionary parameter has access to the same default parameters. The default parameters can also be grouped into a list to combine their search spaces.</p> <ul> <li>'selectors' : A selection of sklearn Selector methods.</li> <li>'classifiers' : A selection of sklearn Classifier methods.</li> <li>'regressors' : A selection of sklearn Regressor methods.</li> <li>'transformers' : A selection of sklearn Transformer methods.</li> <li>'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.</li> <li>'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.</li> <li>'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.                           Subsets are set with the subsets parameter.</li> <li>list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.</li> </ul>"},{"location":"Tutorial/2_Defining_Search_Space_%28config_dicts%29/#other-search-space-parameters","title":"Other search space parameters\u00b6","text":"<ol> <li>linear_pipeline : If True, pipelines will be linear</li> <li>max_size :  The maximum number of nodes in the pipeline.</li> </ol>"},{"location":"Tutorial/2_Defining_Search_Space_%28config_dicts%29/#defining-configuration-dictionaries","title":"defining configuration dictionaries\u00b6","text":"<p>Configuration dictionaries are python dictionaries where the keys are the method types and the values are optuna-compatible functions that take in a trial and return a hyperparameter dictionary.</p> <p>Configuration dictionaries can also be nested. Meaning that the search space for that node, will be a graph defined by the nested dictionary. More on that later in the tutorial.</p> <p>With these three types of configuration dictionaries plus nesting, one can define very specific search spaces. More on nesting later.</p>"},{"location":"Tutorial/2_Defining_Search_Space_%28config_dicts%29/#custom-configuration-dictionaries","title":"Custom Configuration Dictionaries\u00b6","text":"<p>Next, we will show how to use these features to define a graph pipeline search space similar to symbolic classification.</p> <p>The following defines a pipeline where leafs select a single feature, inner nodes perform arithmetic, and logistic regression is used as a final classifier.</p> <p>The arithmetic transformer and feature set selection of single columns are built in configurations with the \"arithmetic_transformer\" and \"feature_set_selector\" options respectively.</p> <p>There is not a built in configuration for a single logistic regression so we have to manually define one.</p>"},{"location":"Tutorial/2_Defining_Search_Space_%28config_dicts%29/#parameter-function","title":"Parameter function\u00b6","text":"<p>To start, we create a function that takes in a trial object. This object takes in a search space, and outputs a parameter. This is designed to be compatible with the optuna trial class. More information on available functions within trial can be found here: https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html</p> <p>The suggested parameters should be put into a dictionary that has the model parameters as keys with their corresponding values.</p> <p>Note: For optuna optimization to work, it is important to add '_{name}' to each of the names parameters. With large graphs, names of parameters will likely clash. The name parameter here allows TPOT2 to make sure each parameter for each node has a unique label.</p> <p>Note: This will be simplified in a future release.</p>"},{"location":"Tutorial/2_Defining_Search_Space_%28config_dicts%29/#configuration-dictionary","title":"configuration dictionary\u00b6","text":"<p>A configuration dictionary has the python Types for the designed estimator as keys, and the function as values.</p>"},{"location":"Tutorial/2_Defining_Search_Space_%28config_dicts%29/#recursive-configuration-dictionaries-experimental","title":"Recursive Configuration Dictionaries (EXPERIMENTAL)\u00b6","text":"<p>Configuration dictionaries can also be nested. If the string \"Recursive\" is used in place of a type, the node that would go in that place will now represent a graph with those restrictions.</p> <p>All inputs to the recursive node will be merged and input to all the leaves within the recursive graph. The output of the graph will be sent to the outputs of the node that represents it.</p> <p>This is handy for restricting the search space of the model as well as setting specific ensembling templates.</p> <p>(Currently) These are all flattened and merged into a single graph when exported as a graph pipeline. In the future these could be used for ensemble methods such as boosting/stacking/etc.</p> <p>Note that this is not a new instance of the TPOT2 estimator, and it does not independently run GP. Rather this recursive node just sets a search space restriction for that node.</p>"},{"location":"Tutorial/3_Genetic_Feature_Set_Selectors/","title":"3 Genetic Feature Set Selectors","text":"<p>In TPOT1 you had to specify the feature subsets yourself. TPOT2 can identify the subsets using evolutionary algorithms.</p> <p>In the future, FSS will be treated as a special case in the configuration dictionary to make it more efficient when there are large numbers of features.</p> In\u00a0[4]: Copied! <pre>import tpot2\nimport pandas as pd\nimport numpy as np\n#make a dataframe with columns a,b,c,d,e,f\n\n#numpy array where columns are 1,2,3,4,5,6\ndata = np.repeat([np.arange(6)],10,0)\n\ndf = pd.DataFrame(data,columns=['a','b','c','d','e','f'])\nfss = tpot2.builtin_modules.FeatureSetSelector(name='test',sel_subset=['a','b','c'])\n\nprint(\"original DataFrame\")\nprint(df)\nprint(\"Transformed Data\")\nprint(fss.fit_transform(df))\n</pre> import tpot2 import pandas as pd import numpy as np #make a dataframe with columns a,b,c,d,e,f  #numpy array where columns are 1,2,3,4,5,6 data = np.repeat([np.arange(6)],10,0)  df = pd.DataFrame(data,columns=['a','b','c','d','e','f']) fss = tpot2.builtin_modules.FeatureSetSelector(name='test',sel_subset=['a','b','c'])  print(\"original DataFrame\") print(df) print(\"Transformed Data\") print(fss.fit_transform(df)) <pre>original DataFrame\n   a  b  c  d  e  f\n0  0  1  2  3  4  5\n1  0  1  2  3  4  5\n2  0  1  2  3  4  5\n3  0  1  2  3  4  5\n4  0  1  2  3  4  5\n5  0  1  2  3  4  5\n6  0  1  2  3  4  5\n7  0  1  2  3  4  5\n8  0  1  2  3  4  5\n9  0  1  2  3  4  5\nTransformed Data\n[[0 1 2]\n [0 1 2]\n [0 1 2]\n [0 1 2]\n [0 1 2]\n [0 1 2]\n [0 1 2]\n [0 1 2]\n [0 1 2]\n [0 1 2]]\n</pre> In\u00a0[5]: Copied! <pre>import tpot2\nimport sklearn.datasets\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n\ndef params_LogisticRegression(trial, name=None):\n    params = {}\n    params['solver'] = trial.suggest_categorical(name=f'solver_{name}',\n                                                 choices=[f'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'])\n    params['dual'] = False\n    params['penalty'] = 'l2'\n    params['C'] = trial.suggest_float(f'C_{name}', 1e-4, 1e4, log=True)\n    params['l1_ratio'] = None\n    if params['solver'] == 'liblinear':\n        params['penalty'] = trial.suggest_categorical(name=f'penalty_{name}', choices=['l1', 'l2'])\n        if params['penalty'] == 'l2':\n            params['dual'] = trial.suggest_categorical(name=f'dual_{name}', choices=[True, False])\n        else:\n            params['penalty'] = 'l1'\n\n    params['class_weight'] = trial.suggest_categorical(name=f'class_weight_{name}', choices=['balanced'])\n    param_grid = {'solver': params['solver'],\n                  'penalty': params['penalty'],\n                  'dual': params['dual'],\n                  'multi_class': 'auto',\n                  'l1_ratio': params['l1_ratio'],\n                  'C': params['C'],\n                  }\n    return param_grid\n\n\n\nroot_config_dict =  {LogisticRegression: params_LogisticRegression}\n\n\nest = tpot2.TPOTEstimator(population_size=40,generations=100, \n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            n_jobs=32,\n                            classification=True,\n                            leaf_config_dict=\"feature_set_selector\",\n                            root_config_dict=root_config_dict,\n                            inner_config_dict=None,\n                            )\n\n#make some data\nn_features = 100\nX, y = sklearn.datasets.make_classification(n_samples=1000, n_features=n_features, n_informative=10, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovo')\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\nest.fitted_pipeline_.plot()\n</pre> import tpot2 import sklearn.datasets from sklearn.linear_model import LogisticRegression import numpy as np   def params_LogisticRegression(trial, name=None):     params = {}     params['solver'] = trial.suggest_categorical(name=f'solver_{name}',                                                  choices=[f'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'])     params['dual'] = False     params['penalty'] = 'l2'     params['C'] = trial.suggest_float(f'C_{name}', 1e-4, 1e4, log=True)     params['l1_ratio'] = None     if params['solver'] == 'liblinear':         params['penalty'] = trial.suggest_categorical(name=f'penalty_{name}', choices=['l1', 'l2'])         if params['penalty'] == 'l2':             params['dual'] = trial.suggest_categorical(name=f'dual_{name}', choices=[True, False])         else:             params['penalty'] = 'l1'      params['class_weight'] = trial.suggest_categorical(name=f'class_weight_{name}', choices=['balanced'])     param_grid = {'solver': params['solver'],                   'penalty': params['penalty'],                   'dual': params['dual'],                   'multi_class': 'auto',                   'l1_ratio': params['l1_ratio'],                   'C': params['C'],                   }     return param_grid    root_config_dict =  {LogisticRegression: params_LogisticRegression}   est = tpot2.TPOTEstimator(population_size=40,generations=100,                              scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             n_jobs=32,                             classification=True,                             leaf_config_dict=\"feature_set_selector\",                             root_config_dict=root_config_dict,                             inner_config_dict=None,                             )  #make some data n_features = 100 X, y = sklearn.datasets.make_classification(n_samples=1000, n_features=n_features, n_informative=10, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)  scorer = sklearn.metrics.get_scorer('roc_auc_ovo') X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25) est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) est.fitted_pipeline_.plot() <pre>0.7880069190851433\n</pre> In\u00a0[6]: Copied! <pre>import tpot2\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn\n\ndef params_LogisticRegression(trial, name=None):\n    params = {}\n    params['solver'] = trial.suggest_categorical(name=f'solver_{name}',\n                                                 choices=[f'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'])\n    params['dual'] = False\n    params['penalty'] = 'l2'\n    params['C'] = trial.suggest_float(f'C_{name}', 1e-4, 1e4, log=True)\n    params['l1_ratio'] = None\n    if params['solver'] == 'liblinear':\n        params['penalty'] = trial.suggest_categorical(name=f'penalty_{name}', choices=['l1', 'l2'])\n        if params['penalty'] == 'l2':\n            params['dual'] = trial.suggest_categorical(name=f'dual_{name}', choices=[True, False])\n        else:\n            params['penalty'] = 'l1'\n\n    params['class_weight'] = trial.suggest_categorical(name=f'class_weight_{name}', choices=['balanced'])\n    param_grid = {'solver': params['solver'],\n                  'penalty': params['penalty'],\n                  'dual': params['dual'],\n                  'multi_class': 'auto',\n                  'l1_ratio': params['l1_ratio'],\n                  'C': params['C'],\n                  }\n    return param_grid\n\n\nroot_config_dict =  {LogisticRegression: params_LogisticRegression}\n\ndata = (np.random.rand(5000,26)-0.5)*2\n\ndf = pd.DataFrame(data,columns=['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'])\nfss = tpot2.builtin_modules.FeatureSetSelector(name='test',sel_subset=['a','b','c'])\n\nsubsets = { \"one\" :  ['a','b','c'],\n            \"two\" :  ['d','e','f'],\n            \"three\" :  ['g','h','i'],\n            \"four\" :  ['j','k','l'],\n            \"five\" :  ['m','n','o'],\n            \"six\" :  ['p','q','r'],\n            \"seven\" :  ['s','t','u'],\n            \"eight\" :  ['v','w','x'],\n            \"nine\" :  ['y','z']}\n\ny = (df['a'] + df['b'] + df['c']) - (df['d'] + df['e'] + df['f'])\ny = y.to_numpy()\ny = y&gt;np.median(y)\ny.astype(int)\n\n#split test train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.33, random_state=42)\n\n\nest = tpot2.TPOTEstimator(population_size=40,generations=20, \n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            n_jobs=32,\n                            classification=True,\n                            leaf_config_dict=\"feature_set_selector\",\n                            root_config_dict=root_config_dict,\n                            inner_config_dict=\"arithmetic_transformer\",\n                            subsets = subsets,\n                            )\n\n\nest.fit(X_train,y_train)\nprint(sklearn.metrics.get_scorer('roc_auc_ovr')(est, X_test, y_test))\n\nest.fitted_pipeline_.plot()\n</pre> import tpot2 import pandas as pd import numpy as np from sklearn.linear_model import LogisticRegression import sklearn  def params_LogisticRegression(trial, name=None):     params = {}     params['solver'] = trial.suggest_categorical(name=f'solver_{name}',                                                  choices=[f'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'])     params['dual'] = False     params['penalty'] = 'l2'     params['C'] = trial.suggest_float(f'C_{name}', 1e-4, 1e4, log=True)     params['l1_ratio'] = None     if params['solver'] == 'liblinear':         params['penalty'] = trial.suggest_categorical(name=f'penalty_{name}', choices=['l1', 'l2'])         if params['penalty'] == 'l2':             params['dual'] = trial.suggest_categorical(name=f'dual_{name}', choices=[True, False])         else:             params['penalty'] = 'l1'      params['class_weight'] = trial.suggest_categorical(name=f'class_weight_{name}', choices=['balanced'])     param_grid = {'solver': params['solver'],                   'penalty': params['penalty'],                   'dual': params['dual'],                   'multi_class': 'auto',                   'l1_ratio': params['l1_ratio'],                   'C': params['C'],                   }     return param_grid   root_config_dict =  {LogisticRegression: params_LogisticRegression}  data = (np.random.rand(5000,26)-0.5)*2  df = pd.DataFrame(data,columns=['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']) fss = tpot2.builtin_modules.FeatureSetSelector(name='test',sel_subset=['a','b','c'])  subsets = { \"one\" :  ['a','b','c'],             \"two\" :  ['d','e','f'],             \"three\" :  ['g','h','i'],             \"four\" :  ['j','k','l'],             \"five\" :  ['m','n','o'],             \"six\" :  ['p','q','r'],             \"seven\" :  ['s','t','u'],             \"eight\" :  ['v','w','x'],             \"nine\" :  ['y','z']}  y = (df['a'] + df['b'] + df['c']) - (df['d'] + df['e'] + df['f']) y = y.to_numpy() y = y&gt;np.median(y) y.astype(int)  #split test train from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.33, random_state=42)   est = tpot2.TPOTEstimator(population_size=40,generations=20,                              scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             n_jobs=32,                             classification=True,                             leaf_config_dict=\"feature_set_selector\",                             root_config_dict=root_config_dict,                             inner_config_dict=\"arithmetic_transformer\",                             subsets = subsets,                             )   est.fit(X_train,y_train) print(sklearn.metrics.get_scorer('roc_auc_ovr')(est, X_test, y_test))  est.fitted_pipeline_.plot() <pre>0.9999970586029246\n</pre>"},{"location":"Tutorial/4_Symbolic_Regression_and_Classification/","title":"4 Symbolic Regression and Classification","text":"<p>Symbolic Classification</p> <p>This will be turned into a template without needing to specify the configuration files</p> In\u00a0[1]: Copied! <pre>import tpot2\nimport sklearn.datasets\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\ndef params_LogisticRegression(trial, name=None):\n    param_grid = { \n                  'penalty': trial.suggest_categorical(name=f'penalty_{name}', choices=[None]),\n                  'tol': 0.01,\n                  }\n    return param_grid\n\nroot_config_dict =  {LogisticRegression: params_LogisticRegression}\n\n\n\nest = tpot2.TPOTEstimator(population_size=20,generations=10, \n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            classification=True,\n                            inner_config_dict= \"arithmetic_transformer\",\n                            leaf_config_dict=\"feature_set_selector\",\n                            root_config_dict=root_config_dict,\n                            )\n\n#load iris\nscorer = sklearn.metrics.get_scorer('roc_auc_ovo')\nX, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\nest.fitted_pipeline_.plot()\n</pre> import tpot2 import sklearn.datasets from sklearn.linear_model import LogisticRegression import numpy as np def params_LogisticRegression(trial, name=None):     param_grid = {                    'penalty': trial.suggest_categorical(name=f'penalty_{name}', choices=[None]),                   'tol': 0.01,                   }     return param_grid  root_config_dict =  {LogisticRegression: params_LogisticRegression}    est = tpot2.TPOTEstimator(population_size=20,generations=10,                              scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             classification=True,                             inner_config_dict= \"arithmetic_transformer\",                             leaf_config_dict=\"feature_set_selector\",                             root_config_dict=root_config_dict,                             )  #load iris scorer = sklearn.metrics.get_scorer('roc_auc_ovo') X, y = sklearn.datasets.load_breast_cancer(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25) est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) est.fitted_pipeline_.plot() <pre>0.9921946740128559\n</pre> <p>Symbolic Regression</p> In\u00a0[2]: Copied! <pre>import tpot2\nimport sklearn.datasets\nfrom sklearn.linear_model import ElasticNet\nimport numpy as np\ndef params_ElasticNet(trial, name=None):\n    param_grid = { \n                  'alpha': trial.suggest_float(f'alpha_{name}', 0.001, 1.0, log=True),\n                  'tol': 0.01,\n                  }\n    return param_grid\n\nroot_config_dict =  {ElasticNet: params_ElasticNet}\n\n\n\nest = tpot2.TPOTEstimator(population_size=20,generations=5,\n                            scorers=['neg_mean_squared_error'],\n                            scorers_weights=[1],\n                            n_jobs=1,\n                            classification=False,\n                            inner_config_dict= \"arithmetic_transformer\",\n                            leaf_config_dict=\"feature_set_selector\",\n                            root_config_dict=root_config_dict,\n                            )\n\n\nscorer = sklearn.metrics.get_scorer('neg_mean_squared_error')\nX, y = sklearn.datasets.load_diabetes(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\nest.fitted_pipeline_.plot()\n</pre> import tpot2 import sklearn.datasets from sklearn.linear_model import ElasticNet import numpy as np def params_ElasticNet(trial, name=None):     param_grid = {                    'alpha': trial.suggest_float(f'alpha_{name}', 0.001, 1.0, log=True),                   'tol': 0.01,                   }     return param_grid  root_config_dict =  {ElasticNet: params_ElasticNet}    est = tpot2.TPOTEstimator(population_size=20,generations=5,                             scorers=['neg_mean_squared_error'],                             scorers_weights=[1],                             n_jobs=1,                             classification=False,                             inner_config_dict= \"arithmetic_transformer\",                             leaf_config_dict=\"feature_set_selector\",                             root_config_dict=root_config_dict,                             )   scorer = sklearn.metrics.get_scorer('neg_mean_squared_error') X, y = sklearn.datasets.load_diabetes(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25) est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) est.fitted_pipeline_.plot() <pre>-5117.09067144422\n</pre>"},{"location":"Tutorial/5_GraphPipeline/","title":"5 GraphPipeline","text":"<p>GraphPipelines work similarly to the sklearn Pipeline class. Rather than provide a list of steps, in GraphPipeline you provide a graph of steps using networkx. In GraphPipeline, parents get their inputs from their children. Leafs get the raw inputs (X,y).</p> <p>The label of the nodes can be anything, but is unique per instance of an sklearn estimator. Each node has an attribute \"instance\" for the instance of the step.</p> <p>By default, the root of the resulting tree will become the final estimator/classifier/transformer.</p> In\u00a0[1]: Copied! <pre>from sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nimport networkx as nx\nfrom tpot2 import GraphPipeline\nimport sklearn.metrics\n\nX, y = make_classification(random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    random_state=0)\n\n\ng = nx.DiGraph()\n\ng.add_node(\"scaler\", instance=StandardScaler())\ng.add_node(\"svc\", instance=SVC())\ng.add_node(\"LogisticRegression\", instance=LogisticRegression())\ng.add_node(\"LogisticRegression2\", instance=LogisticRegression())\n\ng.add_edge(\"svc\",\"scaler\")\ng.add_edge(\"LogisticRegression\", \"scaler\")\ng.add_edge(\"LogisticRegression2\", \"LogisticRegression\")\ng.add_edge(\"LogisticRegression2\", \"svc\")\n\n\nest = GraphPipeline(g)\nest.plot()\n\nest.fit(X_train, y_train)\nprint(\"score\")\nprint(sklearn.metrics.roc_auc_score(y_test, est.predict_proba(X_test)[:,1]))\n</pre> from sklearn.svm import SVC from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline import networkx as nx from tpot2 import GraphPipeline import sklearn.metrics  X, y = make_classification(random_state=0) X_train, X_test, y_train, y_test = train_test_split(X, y,                                                     random_state=0)   g = nx.DiGraph()  g.add_node(\"scaler\", instance=StandardScaler()) g.add_node(\"svc\", instance=SVC()) g.add_node(\"LogisticRegression\", instance=LogisticRegression()) g.add_node(\"LogisticRegression2\", instance=LogisticRegression())  g.add_edge(\"svc\",\"scaler\") g.add_edge(\"LogisticRegression\", \"scaler\") g.add_edge(\"LogisticRegression2\", \"LogisticRegression\") g.add_edge(\"LogisticRegression2\", \"svc\")   est = GraphPipeline(g) est.plot()  est.fit(X_train, y_train) print(\"score\") print(sklearn.metrics.roc_auc_score(y_test, est.predict_proba(X_test)[:,1])) <pre>score\n0.8974358974358974\n</pre> <p>access nodes through their labels</p> In\u00a0[2]: Copied! <pre>svc = est.graph.nodes[\"svc\"][\"instance\"]\n</pre> svc = est.graph.nodes[\"svc\"][\"instance\"]"},{"location":"Tutorial/6_SH_and_early_termination/","title":"6 SH and early termination","text":"<p>Welcome to this Jupyter Notebook tutorial parameters relating to computational resources. In this tutorial, we will cover the following parameters:</p> <p><code>population_size</code></p> <p><code>initial_population_size</code></p> <p><code>population_scaling</code></p> <p><code>generations_until_end_population</code></p> <p><code>budget_range</code></p> <p><code>generations_until_end_budget</code></p> <p><code>budget_scaling</code></p> <p><code>stepwise_steps</code></p> <p>Population size is the number of individuals evaluated each generation. Budget refers to the proportion of data to sample. By manipulating these parameters, we can control how quickly the budget increases and how population size changes over time. Most often, this will be used to start the algorithm by evaluating a large number of pipelines on small subsets of the data to quickly narrow now best models, before later getting a better estimate with larger samples on fewer datasets. This can reduce overall computational cost by not spending as much time evaluating poor performing pipelines.</p> <p><code>population_size</code> determines the number of individuals to evalaute each generation. Sometimes we may want to evaluate more or fewer individuals in the earlier generations. The <code>initial_population_size</code> parameter specifies the starting size of the population. The population size will gradually move from <code>initial_population_size</code> to <code>population_size</code> over the course of <code>generations_until_end_population</code> generations. <code>population_scaling</code> dictates how fast that scaling takes place. The interpolation over <code>generations_until_end_population</code> is done stepwise with the number of steps specified by <code>stepwise_steps</code>.</p> <p>The same process goes for the budget scaling.</p> <p>The following cell illustrates how the population size and budget change over time with the given settings.</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport tpot2\n\npopulation_size=60\ninitial_population_size=100\npopulation_scaling = .5\ngenerations_until_end_population = 50\n\nbudget_range = [.3,1]\ngenerations_until_end_budget=50\nbudget_scaling = .5\nstepwise_steps = 5\n\n#Population and budget use stepwise\nfig, ax1 = plt.subplots()\nax2 = ax1.twinx()\n\ninterpolated_values_population = tpot2.utils.beta_interpolation(start=initial_population_size, end=population_size, n=generations_until_end_population, n_steps=stepwise_steps, scale=population_scaling)\ninterpolated_values_budget = tpot2.utils.beta_interpolation(start=budget_range[0], end=budget_range[1], n=generations_until_end_budget, n_steps=stepwise_steps, scale=budget_scaling)\nax1.step(list(range(len(interpolated_values_population))), interpolated_values_population, label=f\"population size\")\nax2.step(list(range(len(interpolated_values_budget))), interpolated_values_budget, label=f\"budget\", color='r')\nax1.set_xlabel(\"generation\")\nax1.set_ylabel(\"population size\")\nax2.set_ylabel(\"bugdet\")\n\nax1.legend(loc='center left', bbox_to_anchor=(1.1, 0.4))\nax2.legend(loc='center left', bbox_to_anchor=(1.1, 0.3))\nplt.show()\n</pre> import matplotlib.pyplot as plt import tpot2  population_size=60 initial_population_size=100 population_scaling = .5 generations_until_end_population = 50  budget_range = [.3,1] generations_until_end_budget=50 budget_scaling = .5 stepwise_steps = 5  #Population and budget use stepwise fig, ax1 = plt.subplots() ax2 = ax1.twinx()  interpolated_values_population = tpot2.utils.beta_interpolation(start=initial_population_size, end=population_size, n=generations_until_end_population, n_steps=stepwise_steps, scale=population_scaling) interpolated_values_budget = tpot2.utils.beta_interpolation(start=budget_range[0], end=budget_range[1], n=generations_until_end_budget, n_steps=stepwise_steps, scale=budget_scaling) ax1.step(list(range(len(interpolated_values_population))), interpolated_values_population, label=f\"population size\") ax2.step(list(range(len(interpolated_values_budget))), interpolated_values_budget, label=f\"budget\", color='r') ax1.set_xlabel(\"generation\") ax1.set_ylabel(\"population size\") ax2.set_ylabel(\"bugdet\")  ax1.legend(loc='center left', bbox_to_anchor=(1.1, 0.4)) ax2.legend(loc='center left', bbox_to_anchor=(1.1, 0.3)) plt.show()  In\u00a0[2]: Copied! <pre># A Graph pipeline starting with at least one selector as a leaf, potentially followed by a series\n# of stacking classifiers or transformers, and ending with a classifier. The graph will have at most 15 nodes and a max depth of 6.\n\nimport tpot2\nimport sklearn\nimport sklearn.datasets\nimport numpy as np\nimport time\nimport tpot2\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport sklearn\n\nX, y = sklearn.datasets.load_iris(return_X_y=True)\n\nest = tpot2.TPOTEstimator(  \n                            generations=5,\n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            classification=True,\n                            root_config_dict=\"classifiers\",\n                            inner_config_dict= [\"transformers\"],\n                            leaf_config_dict=\"selectors\",\n                            n_jobs=32,\n                            cv=2,\n                            max_eval_time_seconds=30,\n\n                            population_size=population_size,\n                            initial_population_size=initial_population_size,\n                            population_scaling = population_scaling,\n                            generations_until_end_population = generations_until_end_population,\n                            \n                            budget_range = budget_range,\n                            generations_until_end_budget=generations_until_end_budget,\n                            verbose=0)\n\n\nstart = time.time()\nest.fit(X, y)\nprint(f\"total time: {time.time()-start}\")\n</pre> # A Graph pipeline starting with at least one selector as a leaf, potentially followed by a series # of stacking classifiers or transformers, and ending with a classifier. The graph will have at most 15 nodes and a max depth of 6.  import tpot2 import sklearn import sklearn.datasets import numpy as np import time import tpot2 import pandas as pd import numpy as np from sklearn.linear_model import LogisticRegression import sklearn  X, y = sklearn.datasets.load_iris(return_X_y=True)  est = tpot2.TPOTEstimator(                               generations=5,                             scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             classification=True,                             root_config_dict=\"classifiers\",                             inner_config_dict= [\"transformers\"],                             leaf_config_dict=\"selectors\",                             n_jobs=32,                             cv=2,                             max_eval_time_seconds=30,                              population_size=population_size,                             initial_population_size=initial_population_size,                             population_scaling = population_scaling,                             generations_until_end_population = generations_until_end_population,                                                          budget_range = budget_range,                             generations_until_end_budget=generations_until_end_budget,                             verbose=0)   start = time.time() est.fit(X, y) print(f\"total time: {time.time()-start}\") <pre>2023-06-14 11:49:45,920 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-q6nay1zr', purging\n2023-06-14 11:49:45,921 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wni1q2fv', purging\n2023-06-14 11:49:45,921 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kunoeg91', purging\n2023-06-14 11:49:45,921 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-40sr99dr', purging\n2023-06-14 11:49:45,922 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_b9njy2q', purging\n2023-06-14 11:49:45,922 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qft6b6eq', purging\n2023-06-14 11:49:45,922 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cgnqe8s_', purging\n2023-06-14 11:49:45,922 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mcu4ugbz', purging\n2023-06-14 11:49:45,923 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-za145tll', purging\n2023-06-14 11:49:45,923 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-3qdbpmh_', purging\n2023-06-14 11:49:45,923 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-54ch2nwd', purging\n2023-06-14 11:49:45,923 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0zc92jfw', purging\n2023-06-14 11:49:45,923 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ub9p6598', purging\n2023-06-14 11:49:45,924 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8peu6bbu', purging\n2023-06-14 11:49:45,924 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1qp5dr29', purging\n2023-06-14 11:49:45,924 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hfh3inka', purging\n2023-06-14 11:49:45,924 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-b1yl5oa1', purging\n2023-06-14 11:49:45,924 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-epp_nuw_', purging\n2023-06-14 11:49:45,925 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-q1qdqc8g', purging\n2023-06-14 11:49:45,925 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ek1b28f4', purging\n2023-06-14 11:49:45,925 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1806jovl', purging\n2023-06-14 11:49:45,925 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-p0cuouft', purging\n2023-06-14 11:49:45,925 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wh0g6edf', purging\n2023-06-14 11:49:45,926 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-1o1ws1of', purging\n2023-06-14 11:49:45,926 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_zh96wch', purging\n2023-06-14 11:49:45,926 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-wd9vzw4h', purging\n2023-06-14 11:49:45,926 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-jy7obwb9', purging\n2023-06-14 11:49:45,926 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-f6ildiiw', purging\n2023-06-14 11:49:45,927 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4ddayasf', purging\n2023-06-14 11:49:45,927 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fn6vfz6t', purging\n2023-06-14 11:49:45,927 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-eyc403bk', purging\n2023-06-14 11:49:45,927 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-fr7a5y2z', purging\n2023-06-14 11:49:45,927 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-9kejqh6s', purging\n2023-06-14 11:49:45,927 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_xaoujzg', purging\n2023-06-14 11:49:45,928 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zimc_s51', purging\n2023-06-14 11:49:45,928 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-vtsv2zit', purging\n2023-06-14 11:49:45,928 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-hj0s47vd', purging\n2023-06-14 11:49:45,928 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-dpti5p3r', purging\n2023-06-14 11:49:45,928 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4cplddft', purging\n2023-06-14 11:49:45,929 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-poszaeet', purging\n2023-06-14 11:49:45,929 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-cjx6kkgn', purging\n2023-06-14 11:49:45,929 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-u096a9iq', purging\n2023-06-14 11:49:45,929 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-0k3omqwi', purging\n2023-06-14 11:49:45,929 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-zk0s6ywn', purging\n2023-06-14 11:49:45,930 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-bwn757sx', purging\n2023-06-14 11:49:45,930 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-2nu35fgy', purging\n2023-06-14 11:49:45,930 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-w6b4di6m', purging\n2023-06-14 11:49:45,930 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-asj0iobm', purging\n2023-06-14 11:49:45,930 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-gxxzxsyi', purging\n2023-06-14 11:49:45,931 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-qa8099ky', purging\n2023-06-14 11:49:45,931 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_uypy41h', purging\n2023-06-14 11:49:45,931 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-a4ujlka7', purging\n2023-06-14 11:49:45,931 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-dwhz05x3', purging\n2023-06-14 11:49:45,931 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-kgug_o6d', purging\n2023-06-14 11:49:45,932 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rnbpw5ka', purging\n2023-06-14 11:49:45,932 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-i52qfiid', purging\n2023-06-14 11:49:45,932 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_5el2wab', purging\n2023-06-14 11:49:45,932 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mqhhdxip', purging\n2023-06-14 11:49:45,932 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-i6xplvqh', purging\n2023-06-14 11:49:45,933 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-_dmc4eb5', purging\n2023-06-14 11:49:45,933 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-mok5p0dw', purging\n2023-06-14 11:49:45,933 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-ugwiqoc3', purging\n2023-06-14 11:49:45,933 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-e97he6cf', purging\n2023-06-14 11:49:45,933 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-an5jredd', purging\n</pre> <pre>total time: 17.05474090576172\n</pre> <p>Tutorial on early termination of evaluating CV scores.</p> <p>We can further reduce computational load by terminating the evaluation of individual pipelines early if the first few CV scores are not promising. Note that this is different than early stopping of the full algorithm. In this section we will cover:</p> <p><code>threshold_evaluation_early_stop</code></p> <p><code>threshold_evaluation_scaling</code></p> <p><code>min_history_threshold</code></p> <p><code>selection_evaluation_early_stop</code></p> <p><code>selection_evaluation_scaling</code></p> <p>Threshold early stopping uses previous scores to identify and terminate the cross validation evaluation of poorly performing pipelines. We calculate the percentile scores from the previously evaluated pipelines. A pipeline must reach the given percentile each fold for the next to be evaluated, otherwise the pipeline is discarded.</p> <p>The <code>threshold_evaluation_early_stop</code> parameter is a list that specifies the starting and ending percentiles to use as a threshold for the evaluation early stopping. W The <code>threshold_evaluation_scaling</code> parameter is a float that controls the rate at which the threshold moves from the start to end percentile. The <code>min_history_threshold</code> parameter specifies the minimum number of previous scores needed before using threshold early stopping. This ensures that the algorithm has enough historical data to make an informed decision about when to stop evaluating pipelines.</p> <p>Selection early stopping uses a selection algorithm after each fold to select which algorithms will be evaluated for the next fold. For example, after evaluating 100 individuals on fold 1, we may want to only evaluate the best 50 for the remaining folds.</p> <p>The <code>selection_evaluation_early_stop</code> parameter is a list that specifies the lower and upper percentage of the population size to select each round of CV. This is used to determine which individuals to evaluate in the next generation. The <code>selection_evaluation_scaling</code> parameter is a float that controls the rate at which the selection threshold moves from the start to end percentile.</p> <p>By manipulating these parameters, we can control how the algorithm selects individuals to evaluate in the next generation and when to stop evaluating pipelines that are not performing well.</p> <p>In practice, the values of these parameters will depend on the specific problem and the available computational resources.</p> <p>In the following sections, we will show you how to set and manipulate these parameters using Python code in a Jupyter Notebook. We will also provide examples of how these parameters can affect the performance of the algorithm.</p> In\u00a0[3]: Copied! <pre>import matplotlib.pyplot as plt\nimport tpot2\n\nthreshold_evaluation_early_stop = [30, 90]\nthreshold_evaluation_scaling = .5\ncv = 5\n\n#Population and budget use stepwise\nfig, ax1 = plt.subplots()\n\ninterpolated_values = tpot2.utils.beta_interpolation(start=threshold_evaluation_early_stop[0], end=threshold_evaluation_early_stop[-1], n=cv, n_steps=cv, scale=threshold_evaluation_scaling)\nax1.step(list(range(len(interpolated_values))), interpolated_values, label=f\"threshold\")\nax1.set_xlabel(\"fold\")\nax1.set_ylabel(\"percentile\")\n#ax1.legend(loc='center left', bbox_to_anchor=(1.1, 0.4))\nplt.show()\n</pre> import matplotlib.pyplot as plt import tpot2  threshold_evaluation_early_stop = [30, 90] threshold_evaluation_scaling = .5 cv = 5  #Population and budget use stepwise fig, ax1 = plt.subplots()  interpolated_values = tpot2.utils.beta_interpolation(start=threshold_evaluation_early_stop[0], end=threshold_evaluation_early_stop[-1], n=cv, n_steps=cv, scale=threshold_evaluation_scaling) ax1.step(list(range(len(interpolated_values))), interpolated_values, label=f\"threshold\") ax1.set_xlabel(\"fold\") ax1.set_ylabel(\"percentile\") #ax1.legend(loc='center left', bbox_to_anchor=(1.1, 0.4)) plt.show()  In\u00a0[4]: Copied! <pre>est = tpot2.TPOTEstimator(  \n                            generations=5,\n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            classification=True,\n                            root_config_dict=\"classifiers\",\n                            inner_config_dict= [\"transformers\"],\n                            leaf_config_dict=\"selectors\",\n                            n_jobs=32,\n                            cv=cv,\n                            \n                            # budget_range = [.3,1],\n                            # generations_until_end_budget=4,\n\n                            threshold_evaluation_early_stop = threshold_evaluation_early_stop,\n                            threshold_evaluation_scaling = threshold_evaluation_scaling,\n                            verbose=0)\n\n\nstart = time.time()\nest.fit(X, y)\nprint(f\"total time: {time.time()-start}\")\n</pre> est = tpot2.TPOTEstimator(                               generations=5,                             scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             classification=True,                             root_config_dict=\"classifiers\",                             inner_config_dict= [\"transformers\"],                             leaf_config_dict=\"selectors\",                             n_jobs=32,                             cv=cv,                                                          # budget_range = [.3,1],                             # generations_until_end_budget=4,                              threshold_evaluation_early_stop = threshold_evaluation_early_stop,                             threshold_evaluation_scaling = threshold_evaluation_scaling,                             verbose=0)   start = time.time() est.fit(X, y) print(f\"total time: {time.time()-start}\") <pre>total time: 23.97980833053589\n</pre> In\u00a0[5]: Copied! <pre>import matplotlib.pyplot as plt\nimport tpot2\n\nselection_evaluation_early_stop = [.1, 1]\nselection_evaluation_scaling = .5\ncv = 5\n\n#Population and budget use stepwise\nfig, ax1 = plt.subplots()\n\ninterpolated_values = tpot2.utils.beta_interpolation(start=selection_evaluation_early_stop[0], end=selection_evaluation_early_stop[-1], n=cv, n_steps=cv, scale=selection_evaluation_scaling)\nax1.step(list(range(len(interpolated_values))), interpolated_values, label=f\"threshold\")\nax1.set_xlabel(\"fold\")\nax1.set_ylabel(\"percent to select\")\n#ax1.legend(loc='center left', bbox_to_anchor=(1.1, 0.4))\nplt.show()\n</pre> import matplotlib.pyplot as plt import tpot2  selection_evaluation_early_stop = [.1, 1] selection_evaluation_scaling = .5 cv = 5  #Population and budget use stepwise fig, ax1 = plt.subplots()  interpolated_values = tpot2.utils.beta_interpolation(start=selection_evaluation_early_stop[0], end=selection_evaluation_early_stop[-1], n=cv, n_steps=cv, scale=selection_evaluation_scaling) ax1.step(list(range(len(interpolated_values))), interpolated_values, label=f\"threshold\") ax1.set_xlabel(\"fold\") ax1.set_ylabel(\"percent to select\") #ax1.legend(loc='center left', bbox_to_anchor=(1.1, 0.4)) plt.show()  In\u00a0[6]: Copied! <pre>est = tpot2.TPOTEstimator(  \n                            generations=5,\n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            classification=True,\n                            root_config_dict=\"classifiers\",\n                            inner_config_dict= [\"transformers\"],\n                            leaf_config_dict=\"selectors\",\n                            n_jobs=32,\n                            cv=cv,\n\n                            selection_evaluation_early_stop  = selection_evaluation_early_stop,\n                            selection_evaluation_scaling = selection_evaluation_scaling,\n\n                            verbose=0)\n\n\nstart = time.time()\nest.fit(X, y)\nprint(f\"total time: {time.time()-start}\")\n</pre> est = tpot2.TPOTEstimator(                               generations=5,                             scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             classification=True,                             root_config_dict=\"classifiers\",                             inner_config_dict= [\"transformers\"],                             leaf_config_dict=\"selectors\",                             n_jobs=32,                             cv=cv,                              selection_evaluation_early_stop  = selection_evaluation_early_stop,                             selection_evaluation_scaling = selection_evaluation_scaling,                              verbose=0)   start = time.time() est.fit(X, y) print(f\"total time: {time.time()-start}\") <pre>total time: 23.03678798675537\n</pre> <p>All of the above methods can be used independently or simultaneously as done below:</p> In\u00a0[7]: Copied! <pre>import math\nnp.array([1.2,3.4,1])\n</pre> import math np.array([1.2,3.4,1]) Out[7]: <pre>array([1.2, 3.4, 1. ])</pre> In\u00a0[8]: Copied! <pre>est = tpot2.TPOTEstimator(  \n                            generations=5,\n                            scorers=['roc_auc_ovr'],\n                            scorers_weights=[1],\n                            classification=True,\n                            root_config_dict=\"classifiers\",\n                            inner_config_dict= [\"transformers\"],\n                            leaf_config_dict=\"selectors\",\n                            n_jobs=32,\n                            cv=cv,\n\n                            population_size=population_size,\n                            initial_population_size=initial_population_size,\n                            population_scaling = population_scaling,\n                            generations_until_end_population = generations_until_end_population,\n                            \n                            budget_range = budget_range,\n                            generations_until_end_budget=generations_until_end_budget,\n                            \n                            threshold_evaluation_early_stop = threshold_evaluation_early_stop,\n                            threshold_evaluation_scaling = threshold_evaluation_scaling,\n\n                            selection_evaluation_early_stop  = selection_evaluation_early_stop,\n                            selection_evaluation_scaling = selection_evaluation_scaling,\n\n                            verbose=0)\n\n\nstart = time.time()\nest.fit(X, y)\nprint(f\"total time: {time.time()-start}\")\n</pre> est = tpot2.TPOTEstimator(                               generations=5,                             scorers=['roc_auc_ovr'],                             scorers_weights=[1],                             classification=True,                             root_config_dict=\"classifiers\",                             inner_config_dict= [\"transformers\"],                             leaf_config_dict=\"selectors\",                             n_jobs=32,                             cv=cv,                              population_size=population_size,                             initial_population_size=initial_population_size,                             population_scaling = population_scaling,                             generations_until_end_population = generations_until_end_population,                                                          budget_range = budget_range,                             generations_until_end_budget=generations_until_end_budget,                                                          threshold_evaluation_early_stop = threshold_evaluation_early_stop,                             threshold_evaluation_scaling = threshold_evaluation_scaling,                              selection_evaluation_early_stop  = selection_evaluation_early_stop,                             selection_evaluation_scaling = selection_evaluation_scaling,                              verbose=0)   start = time.time() est.fit(X, y) print(f\"total time: {time.time()-start}\") <pre>/home/ribeirop/miniconda3/envs/tpot2env/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n</pre> <pre>total time: 36.7981653213501\n</pre>"},{"location":"Tutorial/7_dask_parallelization/","title":"Parallelization","text":"In\u00a0[\u00a0]: Copied! <pre>#my_analysis.py\n\nfrom dask.distributed import Client, LocalCluster\nimport tpot2\nimport sklearn\nimport sklearn.datasets\nimport numpy as np\n\nif __name__==\"__main__\":\n    scorer = sklearn.metrics.get_scorer('roc_auc_ovr')\n    X, y = sklearn.datasets.load_digits(return_X_y=True)\n    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n    est = tpot2.TPOTClassifier(population_size= 8, generations=5,)\n    est.fit(X_train, y_train)\n    print(scorer(est, X_test, y_test))\n</pre> #my_analysis.py  from dask.distributed import Client, LocalCluster import tpot2 import sklearn import sklearn.datasets import numpy as np  if __name__==\"__main__\":     scorer = sklearn.metrics.get_scorer('roc_auc_ovr')     X, y = sklearn.datasets.load_digits(return_X_y=True)     X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)     est = tpot2.TPOTClassifier(population_size= 8, generations=5,)     est.fit(X_train, y_train)     print(scorer(est, X_test, y_test)) In\u00a0[\u00a0]: Copied! <pre>import tpot2\nimport sklearn\nimport sklearn.datasets\nimport numpy as np\nscorer = sklearn.metrics.get_scorer('roc_auc_ovr')\nX, y = sklearn.datasets.load_digits(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n\n\nest = tpot2.TPOTClassifier(population_size= 8, generations=5, n_jobs=4, memory_limit=\"4GB\", verbose=1)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\n</pre> import tpot2 import sklearn import sklearn.datasets import numpy as np scorer = sklearn.metrics.get_scorer('roc_auc_ovr') X, y = sklearn.datasets.load_digits(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)   est = tpot2.TPOTClassifier(population_size= 8, generations=5, n_jobs=4, memory_limit=\"4GB\", verbose=1) est.fit(X_train, y_train) print(scorer(est, X_test, y_test)) <p>Initializing a basic dask local cluster</p> In\u00a0[\u00a0]: Copied! <pre>from dask.distributed import Client, LocalCluster\n\nn_jobs = 4\nmemory_limit = \"4GB\"\n\ncluster = LocalCluster(n_workers=n_jobs, #if no client is passed in and no global client exists, create our own\n                        threads_per_worker=1,\n                        memory_limit=memory_limit)\nclient = Client(cluster)\n</pre> from dask.distributed import Client, LocalCluster  n_jobs = 4 memory_limit = \"4GB\"  cluster = LocalCluster(n_workers=n_jobs, #if no client is passed in and no global client exists, create our own                         threads_per_worker=1,                         memory_limit=memory_limit) client = Client(cluster) <p>Get the link to view the dask Dashboard.</p> In\u00a0[\u00a0]: Copied! <pre>client.dashboard_link\n</pre> client.dashboard_link <p>Pass into TPOT to Train. Note that the if a client is passed in manually, TPOT will ignore n_jobs and memory_limit. If there is no client passed in, TPOT will ignore any global/existing client and create its own.</p> In\u00a0[\u00a0]: Copied! <pre>est = tpot2.TPOTClassifier(population_size= 8, generations=5, client=client, verbose=1)\n# this is equivalent to: \n# est = tpot2.TPOTClassifier(population_size= 8, generations=5, n_jobs=4, memory_limit=\"4GB\", verbose=1)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\n\n#It is good to close the client and cluster when you are done with them\nclient.close()\ncluster.close()\n</pre> est = tpot2.TPOTClassifier(population_size= 8, generations=5, client=client, verbose=1) # this is equivalent to:  # est = tpot2.TPOTClassifier(population_size= 8, generations=5, n_jobs=4, memory_limit=\"4GB\", verbose=1) est.fit(X_train, y_train) print(scorer(est, X_test, y_test))  #It is good to close the client and cluster when you are done with them client.close() cluster.close() <p>Option 2</p> <p>You can initialize the cluster and client with a context manager that will automatically close them.</p> In\u00a0[\u00a0]: Copied! <pre>from dask.distributed import Client, LocalCluster\nimport tpot2\nimport sklearn\nimport sklearn.datasets\nimport numpy as np\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovr')\nX, y = sklearn.datasets.load_digits(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n\n\nn_jobs = 4\nmemory_limit = \"4GB\"\n\nwith LocalCluster(  \n    n_workers=n_jobs,\n    threads_per_worker=1,\n    memory_limit='4GB',\n) as cluster, Client(cluster) as client:\n    est = tpot2.TPOTClassifier(population_size= 8, generations=5, client=client, verbose=1)\n    est.fit(X_train, y_train)\n    print(scorer(est, X_test, y_test))\n</pre> from dask.distributed import Client, LocalCluster import tpot2 import sklearn import sklearn.datasets import numpy as np  scorer = sklearn.metrics.get_scorer('roc_auc_ovr') X, y = sklearn.datasets.load_digits(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)   n_jobs = 4 memory_limit = \"4GB\"  with LocalCluster(       n_workers=n_jobs,     threads_per_worker=1,     memory_limit='4GB', ) as cluster, Client(cluster) as client:     est = tpot2.TPOTClassifier(population_size= 8, generations=5, client=client, verbose=1)     est.fit(X_train, y_train)     print(scorer(est, X_test, y_test)) In\u00a0[\u00a0]: Copied! <pre>from dask.distributed import Client, LocalCluster\nimport sklearn\nimport sklearn.datasets\nimport sklearn.metrics\nimport sklearn.model_selection\nimport tpot2\n\nfrom dask_jobqueue import SGECluster # or SLURMCluster, PBSCluster, etc. Replace SGE with your scheduler.\ncluster = SGECluster(\n    queue='all.q',\n    cores=2,\n    memory=\"50 GB\"\n\n)\n\ncluster.adapt(minimum_jobs=10, maximum_jobs=100)  # auto-scale between 10 and 100 jobs\n\nclient = Client(cluster)\n\nscorer = sklearn.metrics.get_scorer('roc_auc_ovr')\nX, y = sklearn.datasets.load_digits(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)\n\nest = tpot2.TPOTClassifier(population_size= 100, generations=5, client=client, verbose=1)\n# this is equivalent to: \n# est = tpot2.TPOTClassifier(population_size= 8, generations=5, n_jobs=4, memory_limit=\"4GB\", verbose=1)\nest.fit(X_train, y_train)\nprint(scorer(est, X_test, y_test))\n\n#It is good to close the client and cluster when you are done with them\nclient.close()\ncluster.close()\n</pre> from dask.distributed import Client, LocalCluster import sklearn import sklearn.datasets import sklearn.metrics import sklearn.model_selection import tpot2  from dask_jobqueue import SGECluster # or SLURMCluster, PBSCluster, etc. Replace SGE with your scheduler. cluster = SGECluster(     queue='all.q',     cores=2,     memory=\"50 GB\"  )  cluster.adapt(minimum_jobs=10, maximum_jobs=100)  # auto-scale between 10 and 100 jobs  client = Client(cluster)  scorer = sklearn.metrics.get_scorer('roc_auc_ovr') X, y = sklearn.datasets.load_digits(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.75, test_size=0.25)  est = tpot2.TPOTClassifier(population_size= 100, generations=5, client=client, verbose=1) # this is equivalent to:  # est = tpot2.TPOTClassifier(population_size= 8, generations=5, n_jobs=4, memory_limit=\"4GB\", verbose=1) est.fit(X_train, y_train) print(scorer(est, X_test, y_test))  #It is good to close the client and cluster when you are done with them client.close() cluster.close()"},{"location":"Tutorial/7_dask_parallelization/#parallelization","title":"Parallelization\u00b6","text":"<p>TPOT2 uses the Dask package for parallelization either locally (dask.destributed.LocalCluster) or multi-node via a job schedule (dask-jobqueue).</p>"},{"location":"Tutorial/7_dask_parallelization/#best-practices","title":"Best Practices\u00b6","text":"<p>When running tpot from an .py script, it is important to protect code with <code>if __name__==\"__main__\":</code></p>"},{"location":"Tutorial/7_dask_parallelization/#local-machine-parallelization","title":"Local Machine Parallelization\u00b6","text":"<p>TPOT2 can be easily parallelized on a local computer by setting the n_jobs and memory_limit parameters.</p> <p><code>n_jobs</code> dictates how many dask workers to launch. In TPOT2 this corresponds to the number of pipelines to evaluate in parallel.</p> <p><code>memory_limit</code> is the amount of RAM to use per worker.</p>"},{"location":"Tutorial/7_dask_parallelization/#manual-dask-clients-and-dashboard","title":"Manual Dask Clients and Dashboard\u00b6","text":"<p>You can also manually initialize a dask client. This can be useful to gain additional control over the parallelization, debugging, as well as viewing a dashboard of the live performance of TPOT2.</p> <p>You can find more details in the official documentation here.</p> <p>Dask Python Tutorial Dask Dashboard</p>"},{"location":"Tutorial/7_dask_parallelization/#dask-multi-node-parallelization","title":"Dask multi node parallelization\u00b6","text":"<p>Dask can parallelize across multiple nodes via job queueing systems. This is done using the dask-jobqueue package. More information can be found in the official documentation here.</p> <p>To parallelize TPOT2 with dask-jobqueue, simply pass in a client based on a jobqueue cluster with desired settings into the client parameter. Each job will evaluate a single pipeline.</p> <p>Note that TPOT will ignore n_jobs and memory_limit as these should be set inside the dask cluster.</p>"},{"location":"Tutorial/8_Genetic_Algorithm_Overview/","title":"8 Genetic Algorithm Overview","text":"<p>Objective functions can optionally take in step, budget, and generations.</p> <p>step - The same objective function will be run for #evaluation_early_stop_steps, the current step will be passed into the function as an interger. (This is useful for getting a single fold of cross validation for example).</p> <p>budget - A parameter that varies over the course of the generations. Gets passed into the objective function as a float between 0 and 1. If the budget of the previous evaluation is less than the current budget, it will get re-evaluated. Useful for using smaller datasets earlier in training.</p> <p>generations - an int corresponding to the current generation number.</p> In\u00a0[3]: Copied! <pre>#knapsack problem\nimport numpy as np\nimport tpot2\nimport random\nimport matplotlib.pyplot as plt\nfrom dask.distributed import Client, LocalCluster\n\nclass SubsetSelector(tpot2.individual_representations.BaseIndividual):\n    def __init__(   self,\n                    values,\n                    initial_set = None,\n                    k=1, #step size for shuffling\n                ):\n\n        if isinstance(values, int):\n            self.values = set(range(0,values))\n        else:\n            self.values = set(values)\n\n\n        if initial_set is None:\n            self.subsets = set(random.choices(values, k=k))\n        else:\n            self.subsets = set(initial_set)\n\n        self.k = k\n\n        self.mutation_list = [self._mutate_add, self._mutate_remove]\n        self.crossover_list = [self._crossover_swap]\n        \n\n    def mutate(self,):\n        mutation_list_copy = self.mutation_list.copy()\n        random.shuffle(mutation_list_copy)\n        for func in mutation_list_copy:\n            if func():\n                return True\n        return False\n\n    def crossover(self, ind2):\n        crossover_list_copy = self.crossover_list.copy()\n        random.shuffle(crossover_list_copy)\n        for func in crossover_list_copy:\n            if func(ind2):\n                return True\n        return False\n\n    def _mutate_add(self,):\n        not_included = list(self.values.difference(self.subsets))\n        if len(not_included) &gt; 1:\n            self.subsets.update(random.sample(not_included, k=min(self.k, len(not_included))))\n            return True\n        else:\n            return False\n\n    def _mutate_remove(self,):\n        if len(self.subsets) &gt; 1:\n            self.subsets = self.subsets - set(random.sample(list(self.subsets), k=min(self.k, len(self.subsets)-1) ))\n\n    def _crossover_swap(self, ss2):\n        diffs = self.subsets.symmetric_difference(ss2.subsets)\n\n        if len(diffs) == 0:\n            return False\n        for v in diffs:\n            self.subsets.discard(v)\n            ss2.subsets.discard(v)\n            random.choice([self.subsets, ss2.subsets]).add(v)\n        \n        return True\n\n    def unique_id(self):\n        return str(tuple(sorted(self.subsets)))\n\ndef individual_generator():\n    while True:\n        yield SubsetSelector(values=np.arange(len(values)))\n\n\nvalues = np.random.randint(200,size=100)\nweights = np.random.random(200)*10\nmax_weight = 50\n\ndef simple_objective(ind, **kwargs):\n    subset = np.array(list(ind.subsets))\n    if len(subset) == 0:\n        return 0, 0\n\n    total_weight = np.sum(weights[subset])\n    total_value = np.sum(values[subset])\n\n    if total_weight &gt; max_weight:\n        total_value = 0\n\n    return total_value, total_weight\n\nobjective_names = [\"Value\", \"Weight\"]\nobjective_function_weights = [1,-1]\n\n\n\nevolver = tpot2.evolvers.BaseEvolver(   individual_generator=individual_generator(), \n                                objective_functions=[simple_objective],\n                                objective_function_weights = objective_function_weights,\n                                bigger_is_better = True,\n                                population_size= 100,\n                                objective_names = objective_names,\n                                generations= 100,\n                                n_jobs=1,\n                                verbose = 1,\n\n)\n\nevolver.optimize()\n</pre> #knapsack problem import numpy as np import tpot2 import random import matplotlib.pyplot as plt from dask.distributed import Client, LocalCluster  class SubsetSelector(tpot2.individual_representations.BaseIndividual):     def __init__(   self,                     values,                     initial_set = None,                     k=1, #step size for shuffling                 ):          if isinstance(values, int):             self.values = set(range(0,values))         else:             self.values = set(values)           if initial_set is None:             self.subsets = set(random.choices(values, k=k))         else:             self.subsets = set(initial_set)          self.k = k          self.mutation_list = [self._mutate_add, self._mutate_remove]         self.crossover_list = [self._crossover_swap]               def mutate(self,):         mutation_list_copy = self.mutation_list.copy()         random.shuffle(mutation_list_copy)         for func in mutation_list_copy:             if func():                 return True         return False      def crossover(self, ind2):         crossover_list_copy = self.crossover_list.copy()         random.shuffle(crossover_list_copy)         for func in crossover_list_copy:             if func(ind2):                 return True         return False      def _mutate_add(self,):         not_included = list(self.values.difference(self.subsets))         if len(not_included) &gt; 1:             self.subsets.update(random.sample(not_included, k=min(self.k, len(not_included))))             return True         else:             return False      def _mutate_remove(self,):         if len(self.subsets) &gt; 1:             self.subsets = self.subsets - set(random.sample(list(self.subsets), k=min(self.k, len(self.subsets)-1) ))      def _crossover_swap(self, ss2):         diffs = self.subsets.symmetric_difference(ss2.subsets)          if len(diffs) == 0:             return False         for v in diffs:             self.subsets.discard(v)             ss2.subsets.discard(v)             random.choice([self.subsets, ss2.subsets]).add(v)                  return True      def unique_id(self):         return str(tuple(sorted(self.subsets)))  def individual_generator():     while True:         yield SubsetSelector(values=np.arange(len(values)))   values = np.random.randint(200,size=100) weights = np.random.random(200)*10 max_weight = 50  def simple_objective(ind, **kwargs):     subset = np.array(list(ind.subsets))     if len(subset) == 0:         return 0, 0      total_weight = np.sum(weights[subset])     total_value = np.sum(values[subset])      if total_weight &gt; max_weight:         total_value = 0      return total_value, total_weight  objective_names = [\"Value\", \"Weight\"] objective_function_weights = [1,-1]    evolver = tpot2.evolvers.BaseEvolver(   individual_generator=individual_generator(),                                  objective_functions=[simple_objective],                                 objective_function_weights = objective_function_weights,                                 bigger_is_better = True,                                 population_size= 100,                                 objective_names = objective_names,                                 generations= 100,                                 n_jobs=1,                                 verbose = 1,  )  evolver.optimize() <pre>Generation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [02:22&lt;00:00,  1.42s/it]\n</pre> In\u00a0[4]: Copied! <pre>final_population_results = evolver.population.evaluated_individuals\nfinal_population_results.reset_index(inplace=True)\nfinal_population_results = final_population_results.rename(columns = {'index':'Selected Index'})\n\nbest_idx = final_population_results[\"Value\"].idxmax()\nbest_individual = final_population_results.loc[best_idx]['Individual']\nprint(\"best subset\", best_individual.subsets)\nprint(\"Best value {0}, weight {1}\".format(final_population_results.loc[best_idx, \"Value\"],final_population_results.loc[best_idx, \"Weight\"]))\nprint()\n\nprint(\"All results\")\nfinal_population_results\n</pre> final_population_results = evolver.population.evaluated_individuals final_population_results.reset_index(inplace=True) final_population_results = final_population_results.rename(columns = {'index':'Selected Index'})  best_idx = final_population_results[\"Value\"].idxmax() best_individual = final_population_results.loc[best_idx]['Individual'] print(\"best subset\", best_individual.subsets) print(\"Best value {0}, weight {1}\".format(final_population_results.loc[best_idx, \"Value\"],final_population_results.loc[best_idx, \"Weight\"])) print()  print(\"All results\") final_population_results <pre>best subset {0, 96, 34, 35, 36, 5, 9, 75, 18, 50, 84, 20, 22, 23, 87, 26, 27, 28}\nBest value 2422.0, weight 49.38389605974704\n\nAll results\n</pre> Out[4]: Selected Index Value Weight Parents Variation_Function Individual Generation Pareto_Front 0 (60,) 11.0 5.586633 NaN NaN &lt;__main__.SubsetSelector object at 0x7fc0b1d2d... 0.0 NaN 1 (66,) 192.0 5.407096 NaN NaN &lt;__main__.SubsetSelector object at 0x7fc0b1d61... 0.0 NaN 2 (2,) 50.0 5.002992 NaN NaN &lt;__main__.SubsetSelector object at 0x7fc0b1d2c... 0.0 NaN 3 (53,) 27.0 4.088630 NaN NaN &lt;__main__.SubsetSelector object at 0x7fc0b1d2c... 0.0 NaN 4 (18,) 165.0 4.886466 NaN NaN &lt;__main__.SubsetSelector object at 0x7fc0b1d2d... 0.0 NaN ... ... ... ... ... ... ... ... ... 9995 (0, 26, 27, 28, 75, 84, 87, 98) 1016.0 18.517959 ((0, 26, 27, 28, 75, 84, 87),) mutate &lt;__main__.SubsetSelector object at 0x7fc09452a... 99.0 NaN 9996 (0, 26, 27, 28, 31, 65, 75, 87) 1058.0 16.186587 ((0, 26, 27, 28, 31, 75, 84, 87),) mutate &lt;__main__.SubsetSelector object at 0x7fc09452a... 99.0 NaN 9997 (0, 26, 27, 28, 31, 55, 75, 84, 87, 96) 1264.0 20.441256 ((0, 26, 27, 28, 31, 75, 84, 85, 87, 96),) mutate &lt;__main__.SubsetSelector object at 0x7fc09452a... 99.0 NaN 9998 (26, 27, 31, 75, 84, 87, 94) 772.0 8.488381 ((0, 26, 27, 31, 75, 84, 87),) mutate &lt;__main__.SubsetSelector object at 0x7fc094529... 99.0 NaN 9999 (0, 27, 29, 31, 75, 84, 87, 93) 1060.0 21.392753 ((0, 27, 28, 31, 75, 84, 85, 87),) mutate &lt;__main__.SubsetSelector object at 0x7fc09452b... 99.0 NaN <p>10000 rows \u00d7 8 columns</p> In\u00a0[5]: Copied! <pre>from scipy.stats import binned_statistic_2d\n\ny = final_population_results[\"Value\"]\nx = final_population_results[\"Weight\"]\nc = final_population_results[\"Generation\"]\n\nx_bins = np.linspace(0, 100, 100)\ny_bins = np.linspace(0, 3000, 100)\n\nret = binned_statistic_2d(x, y, c, statistic=np.mean, bins=[x_bins, y_bins])\n\nfig, ax1 = plt.subplots(1, 1, figsize=(12, 4))\n\nim = ax1.imshow(ret.statistic.T, origin='lower', extent=(0,100,0,3000), vmin=0, vmax=100, aspect=.03)\nax1.set_xlabel(\"Weight\")\nax1.set_ylabel(\"Value\")\nax1.set_title(\"Binned Average Generation\")\n\ncbar = fig.colorbar(im,)\ncbar.set_label('Generation')\nplt.tight_layout()\n</pre> from scipy.stats import binned_statistic_2d  y = final_population_results[\"Value\"] x = final_population_results[\"Weight\"] c = final_population_results[\"Generation\"]  x_bins = np.linspace(0, 100, 100) y_bins = np.linspace(0, 3000, 100)  ret = binned_statistic_2d(x, y, c, statistic=np.mean, bins=[x_bins, y_bins])  fig, ax1 = plt.subplots(1, 1, figsize=(12, 4))  im = ax1.imshow(ret.statistic.T, origin='lower', extent=(0,100,0,3000), vmin=0, vmax=100, aspect=.03) ax1.set_xlabel(\"Weight\") ax1.set_ylabel(\"Value\") ax1.set_title(\"Binned Average Generation\")  cbar = fig.colorbar(im,) cbar.set_label('Generation') plt.tight_layout()"},{"location":"documentation/tpot2/_version/","title":"version","text":""},{"location":"documentation/tpot2/graphsklearn/","title":"Graphsklearn","text":""},{"location":"documentation/tpot2/graphsklearn/#tpot2.graphsklearn.GraphPipeline","title":"<code>GraphPipeline</code>","text":"<p>         Bases: <code>_BaseComposition</code></p> Source code in <code>tpot2/graphsklearn.py</code> <pre><code>class GraphPipeline(_BaseComposition):\ndef __init__(\nself,\ngraph,\ncross_val_predict_cv=0, #signature function(estimator, X, y=none)\nmethod='auto',\nmemory=None, #TODO memory caching like sklearn.pipeline\nsubset_column = None,\ndrop_subset_column = True,\n**kwargs,\n):\nsuper().__init__(**kwargs)\nself.graph = graph\nself.cross_val_predict_cv = cross_val_predict_cv\nself.method = method\nself.memory = memory\nself.subset_column = subset_column\nself.drop_subset_column = drop_subset_column\nsetup_ordered_successors(graph)\nself.topo_sorted_nodes = list(nx.topological_sort(self.graph))\nself.topo_sorted_nodes.reverse()\nself.root = self.topo_sorted_nodes[-1]\n#TODO clean this up\ntry:\nnx.find_cycle(self.G)\nraise BaseException \nexcept: \npass\ndef __str__(self):\nif len(self.graph.edges) &gt; 0:\nreturn str(self.graph.edges)\nelse:\nreturn str(self.graph.nodes)\ndef fit(self, X, y, subset_col = None):\n# if self.subset_column is not None and self.subset_values is not None:\n#     if isinstance(X, pd.DataFrame):\n#         indeces_to_keep = X[self.subset_column].isin(self._subset_values)\n#         X = X[indeces_to_keep]\n#         y = y[indeces_to_keep]\n#     else:\n#         indeces_to_keep = np.isin(X[:,self.subset_column], self._subset_values)\n#         X = X[indeces_to_keep]\n#         y = y[indeces_to_keep]\nif self.subset_column is not None:\nsubset_col = X[:,self.subset_column]\nif self.drop_subset_column:\nX = np.delete(X, self.subset_column, axis=1)\nfit_sklearn_digraph(   graph=self.graph,\nX=X,\ny=y,\nmethod=self.method,\ncross_val_predict_cv = self.cross_val_predict_cv,\nmemory = self.memory,\ntopo_sort = self.topo_sorted_nodes,\nsubset_col = subset_col,\n)\nreturn self\ndef plot(self, ):\nplot(graph = self.graph)\ndef __sklearn_is_fitted__(self):\n'''Indicate whether pipeline has been fit.'''\ntry:\n# check if the last step of the pipeline is fitted\n# we only check the last step since if the last step is fit, it\n# means the previous steps should also be fit. This is faster than\n# checking if every step of the pipeline is fit.\nsklearn.utils.validation.check_is_fitted(self.graph.nodes[self.root][\"instance\"])\nreturn True\nexcept sklearn.exceptions.NotFittedError:\nreturn False\n@available_if(_estimator_has('predict'))\ndef predict(self, X, **predict_params):\nif self.subset_column is not None:\nsubset_col = X[:,self.subset_column]\nif self.drop_subset_column:\nX = np.delete(X, self.subset_column, axis=1)\nthis_X = get_inputs_to_node(self.graph,\nX, \nself.root,\nmethod = self.method,\ntopo_sort = self.topo_sorted_nodes,\n)\nreturn self.graph.nodes[self.root][\"instance\"].predict(this_X, **predict_params)\n@available_if(_estimator_has('predict_proba'))\ndef predict_proba(self, X, **predict_params):\nif self.subset_column is not None:\nif self.drop_subset_column:\nX = np.delete(X, self.subset_column, axis=1)\nthis_X = get_inputs_to_node(self.graph,\nX, \nself.root,\nmethod = self.method,\ntopo_sort = self.topo_sorted_nodes,\n)\nreturn self.graph.nodes[self.root][\"instance\"].predict_proba(this_X, **predict_params)\n@available_if(_estimator_has('decision_function'))\ndef decision_function(self, X, **predict_params):\nif self.subset_column is not None:\nif self.drop_subset_column:\nX = np.delete(X, self.subset_column, axis=1)\nthis_X = get_inputs_to_node(self.graph,\nX, \nself.root,\nmethod = self.method,\ntopo_sort = self.topo_sorted_nodes,\n)\nreturn self.graph.nodes[self.root][\"instance\"].decision_function(this_X, **predict_params)\n@available_if(_estimator_has('transform'))\ndef transform(self, X, **predict_params):\nif self.subset_column is not None:\nif self.drop_subset_column:\nX = np.delete(X, self.subset_column, axis=1)\nthis_X = get_inputs_to_node(self.graph,\nX, \nself.root,\nmethod = self.method,\ntopo_sort = self.topo_sorted_nodes,\n)\nreturn self.graph.nodes[self.root][\"instance\"].transform(this_X, **predict_params)\n@property\ndef classes_(self):\n\"\"\"The classes labels. Only exist if the last step is a classifier.\"\"\"\nreturn self.graph.nodes[self.root][\"instance\"].classes_\n</code></pre>"},{"location":"documentation/tpot2/graphsklearn/#tpot2.graphsklearn.GraphPipeline.classes_","title":"<code>classes_</code>  <code>property</code>","text":"<p>The classes labels. Only exist if the last step is a classifier.</p>"},{"location":"documentation/tpot2/graphsklearn/#tpot2.graphsklearn.GraphPipeline.__sklearn_is_fitted__","title":"<code>__sklearn_is_fitted__()</code>","text":"<p>Indicate whether pipeline has been fit.</p> Source code in <code>tpot2/graphsklearn.py</code> <pre><code>def __sklearn_is_fitted__(self):\n'''Indicate whether pipeline has been fit.'''\ntry:\n# check if the last step of the pipeline is fitted\n# we only check the last step since if the last step is fit, it\n# means the previous steps should also be fit. This is faster than\n# checking if every step of the pipeline is fit.\nsklearn.utils.validation.check_is_fitted(self.graph.nodes[self.root][\"instance\"])\nreturn True\nexcept sklearn.exceptions.NotFittedError:\nreturn False\n</code></pre>"},{"location":"documentation/tpot2/logbook/","title":"Logbook","text":""},{"location":"documentation/tpot2/population/","title":"Population","text":""},{"location":"documentation/tpot2/population/#tpot2.population.Population","title":"<code>Population</code>","text":"<p>Primary usage is to keep track of evaluated individuals</p> <p>Parameters:</p> Name Type Description Default <code>initial_population</code> <p>Initial population to start with. If None, start with an empty population.</p> <code>list of BaseIndividuals</code> <code>use_unique_id</code> <p>If True, individuals are treated as unique if they have the same unique_id(). If False, all new individuals are treated as unique.</p> <code>Bool</code> <code>callback</code> <p>NOT YET IMPLEMENTED A function to call after each generation. The function should take a Population object as its only argument.</p> <code>function</code> <p>Attributes:</p> Name Type Description <code>population</code> <code>{list of BaseIndividuals}</code> <p>The current population of individuals. Contains the live instances of BaseIndividuals.</p> <code>evaluated_individuals</code> <code>{dict}</code> <p>A dictionary of dictionaries. The keys are the unique_id() or self of each BaseIndividual. Can be thought of as a table with the unique_id() as the row index and the inner dictionary keys as the columns.</p> Source code in <code>tpot2/population.py</code> <pre><code>class Population():\n'''\n    Primary usage is to keep track of evaluated individuals\n    Parameters\n    ----------\n    initial_population : {list of BaseIndividuals}, default=None\n        Initial population to start with. If None, start with an empty population.\n    use_unique_id : {Bool}, default=True\n        If True, individuals are treated as unique if they have the same unique_id().\n        If False, all new individuals are treated as unique.\n    callback : {function}, default=None\n        NOT YET IMPLEMENTED\n        A function to call after each generation. The function should take a Population object as its only argument.\n    Attributes\n    ----------\n    population : {list of BaseIndividuals}\n        The current population of individuals. Contains the live instances of BaseIndividuals.\n    evaluated_individuals : {dict}\n        A dictionary of dictionaries. The keys are the unique_id() or self of each BaseIndividual.\n        Can be thought of as a table with the unique_id() as the row index and the inner dictionary keys as the columns.\n    '''\ndef __init__(   self,\ncolumn_names: typing.List[str] = None,\nn_jobs: int = 1,\ncallback=None,\n) -&gt; None:\nif column_names is not None:\ncolumn_names = column_names+[\"Parents\", \"Variation_Function\"]\nelse:\ncolumn_names = [\"Parents\", \"Variation_Function\"]\nself.evaluated_individuals = pd.DataFrame(columns=column_names)\nself.evaluated_individuals[\"Parents\"] = self.evaluated_individuals[\"Parents\"].astype('object')\nself.use_unique_id = True #Todo clean this up. perhaps pull unique_id() out of baseestimator and have it be supplied as a function\nself.n_jobs = n_jobs\nself.callback=callback\nself.population = []\n#remove individuals that either do not have a column_name value or a nan in that value\n#TODO take into account when the value is not a list/tuple?\n#TODO make invalid a global variable?\ndef remove_invalid_from_population(self, column_names, invalid_value = \"INVALID\"):\n'''\n        Remove individuals from the live population if either do not have a value in the column_name column or if the value contains np.nan.\n        Parameters\n        ----------\n        column_name : {str}\n            The name of the column to check for np.nan values.\n        Returns\n        -------\n        None\n        '''\nif isinstance(column_names, str): #TODO check this\ncolumn_names = [column_names]\nnew_pop = []\nis_valid = lambda ind: ind.unique_id() not in self.evaluated_individuals.index or invalid_value not in self.evaluated_individuals.loc[ind.unique_id(),column_names].to_list()\nself.population = [ind for ind in self.population if is_valid(ind)]\n# takes the list of individuals and adds it to the live population list. \n# if keep_repeats is False, repeated individuals are not added to the population\n# returns a list of individuals added to the live population  \n#TODO make keep repeats allow for previously evaluated individuals,\n#but make sure that the live population only includes one of each, no repeats\ndef add_to_population(self, individuals: typing.List[BaseIndividual], keep_repeats=False, mutate_until_unique=True):\n'''\n        Add individuals to the live population. Add individuals to the evaluated_individuals if they are not already there.\n        Parameters:\n        -----------\n        individuals : {list of BaseIndividuals}\n            The individuals to add to the live population.\n        keep_repeats : {bool}, default=False\n            If True, allow the population to have repeated individuals.\n            If False, only add individuals that have not yet been added to geneology.\n        '''\nif not isinstance(individuals, collections.abc.Iterable):\nindividuals = [individuals]\nnew_individuals = []\n#TODO check for proper inputs\nfor individual in individuals:\nkey = individual.unique_id()\nif key not in self.evaluated_individuals.index: #If its new, we always add it\nself.evaluated_individuals.loc[key] = np.nan\nself.evaluated_individuals.loc[key,\"Individual\"] = copy.deepcopy(individual)\nself.population.append(individual)\nnew_individuals.append(individual)\nelse:#If its old\nif keep_repeats: #If we want to keep repeats, we add it\nself.population.append(individual)\nnew_individuals.append(individual)\nelif mutate_until_unique: #If its old and we don't want repeats, we can optionally mutate it until it is unique\nfor _ in range(20):\nindividual = copy.deepcopy(individual)\nindividual.mutate()\nkey = individual.unique_id()\nif key not in self.evaluated_individuals.index:\nself.evaluated_individuals.loc[key] = np.nan\nself.evaluated_individuals.loc[key,\"Individual\"] = copy.deepcopy(individual)\nself.population.append(individual)\nnew_individuals.append(individual)\nbreak\nreturn new_individuals\ndef update_column(self, individual, column_names, data):\n'''\n        Update the column_name column in the evaluated_individuals with the data.\n        If the data is a list, it must be the same length as the evaluated_individuals.\n        If the data is a single value, it will be applied to all individuals in the evaluated_individuals.\n        '''\nif isinstance(individual, collections.abc.Iterable):\nif self.use_unique_id:\nkey = [ind.unique_id() for ind in individual]\nelse:\nkey = individual\nelse:\nif self.use_unique_id:\nkey = individual.unique_id()\nelse:\nkey = individual\nself.evaluated_individuals.loc[key,column_names] = data\ndef get_column(self, individual, column_names=None, to_numpy=True):\n'''\n        Update the column_name column in the evaluated_individuals with the data.\n        If the data is a list, it must be the same length as the evaluated_individuals.\n        If the data is a single value, it will be applied to all individuals in the evaluated_individuals.\n        '''\nif isinstance(individual, collections.abc.Iterable):\nif self.use_unique_id:\nkey = [ind.unique_id() for ind in individual]\nelse:\nkey = individual\nelse:\nif self.use_unique_id:\nkey = individual.unique_id()\nelse:\nkey = individual\nif column_names is not None:\nslice = self.evaluated_individuals.loc[key,column_names]\nelse:\nslice = self.evaluated_individuals.loc[key]\nif to_numpy:\nslice.reset_index(drop=True, inplace=True)\nreturn slice.to_numpy()\nelse:\nreturn slice\n#returns the individuals without a 'column' as a key in geneology\n#TODO make sure not to get repeats in this list even if repeats are in the \"live\" population\ndef get_unevaluated_individuals(self, column_names, individual_list=None):\nif individual_list is None:\nindividual_list = self.population\nif self.use_unique_id:\nunevaluated_filter = lambda individual: individual.unique_id() not in self.evaluated_individuals.index or any(self.evaluated_individuals.loc[individual.unique_id(), column_names].isna())\nelse:\nunevaluated_filter = lambda individual: individual not in self.evaluated_individuals.index or any(self.evaluated_individuals.loc[individual.unique_id(), column_names].isna())\nreturn [individual for individual in individual_list if unevaluated_filter(individual)]    \n# def get_valid_evaluated_individuals_df(self, column_names_to_check, invalid_values=[\"TIMEOUT\",\"INVALID\"]):\n#     '''\n#     Returns a dataframe of the evaluated individuals that do no have invalid_values in column_names_to_check.\n#     '''\n#     return self.evaluated_individuals[~self.evaluated_individuals[column_names_to_check].isin(invalid_values).any(axis=1)]\n#the live population empied and is set to new_population\ndef set_population(self,  new_population, keep_repeats=True):\n'''\n        sets population to new population\n        for selection?\n        '''\nself.population = []\nself.add_to_population(new_population, keep_repeats=keep_repeats)\n#TODO should we just generate one offspring per crossover? \ndef create_offspring(self, parents_list, var_op_list, add_to_population=True, keep_repeats=False, mutate_until_unique=True, n_jobs=1):\n'''\n        parents_list: a list of lists of parents. \n        var_op_list: a list of var_ops to apply to each list of parents. Should be the same length as parents_list.\n        for example:\n        parents_list = [[parent1, parent2], [parent3]]\n        var_op_list = [\"crossover\", \"mutate\"]\n        This will apply crossover to parent1 and parent2 and mutate to parent3.\n        Creates offspring from parents using the var_op_list.\n        If string, will use a built in method \n            - \"crossover\" : crossover\n            - \"mutate\" : mutate\n            - \"mutate_and_crossover\" : mutate_and_crossover\n            - \"cross_and_mutate\" : cross_and_mutate\n        '''\nnew_offspring = []\nall_offspring = parallel_create_offspring(parents_list, var_op_list, n_jobs=n_jobs)\nfor parents, offspring, var_op in zip(parents_list, all_offspring, var_op_list):\n# if var_op in built_in_var_ops_dict:\n#     var_op = built_in_var_ops_dict[var_op]\n# offspring = copy.deepcopy(parents)\n# offspring = var_op(offspring)\n# if isinstance(offspring, collections.abc.Iterable):\n#     offspring = offspring[0] \nif add_to_population:\nadded = self.add_to_population(offspring, keep_repeats=keep_repeats, mutate_until_unique=mutate_until_unique)\nif len(added) &gt; 0:\nfor new_child in added:\nparent_keys = [parent.unique_id() for parent in parents]\nif not pd.api.types.is_object_dtype(self.evaluated_individuals[\"Parents\"]): #TODO Is there a cleaner way of doing this? Not required for some python environments?\nself.evaluated_individuals[\"Parents\"] = self.evaluated_individuals[\"Parents\"].astype('object')\nself.evaluated_individuals.at[new_child.unique_id(),\"Parents\"] = tuple(parent_keys)\n#if var_op is a function\nif hasattr(var_op, '__call__'):\nself.evaluated_individuals.at[new_child.unique_id(),\"Variation_Function\"] = var_op.__name__\nelse:\nself.evaluated_individuals.at[new_child.unique_id(),\"Variation_Function\"] = var_op\nnew_offspring.append(new_child)\nelse:\nnew_offspring.append(offspring)\nreturn new_offspring\n</code></pre>"},{"location":"documentation/tpot2/population/#tpot2.population.Population.add_to_population","title":"<code>add_to_population(individuals, keep_repeats=False, mutate_until_unique=True)</code>","text":"<p>Add individuals to the live population. Add individuals to the evaluated_individuals if they are not already there.</p>"},{"location":"documentation/tpot2/population/#tpot2.population.Population.add_to_population--parameters","title":"Parameters:","text":"<p>individuals : {list of BaseIndividuals}     The individuals to add to the live population. keep_repeats : {bool}, default=False     If True, allow the population to have repeated individuals.     If False, only add individuals that have not yet been added to geneology.</p> Source code in <code>tpot2/population.py</code> <pre><code>def add_to_population(self, individuals: typing.List[BaseIndividual], keep_repeats=False, mutate_until_unique=True):\n'''\n    Add individuals to the live population. Add individuals to the evaluated_individuals if they are not already there.\n    Parameters:\n    -----------\n    individuals : {list of BaseIndividuals}\n        The individuals to add to the live population.\n    keep_repeats : {bool}, default=False\n        If True, allow the population to have repeated individuals.\n        If False, only add individuals that have not yet been added to geneology.\n    '''\nif not isinstance(individuals, collections.abc.Iterable):\nindividuals = [individuals]\nnew_individuals = []\n#TODO check for proper inputs\nfor individual in individuals:\nkey = individual.unique_id()\nif key not in self.evaluated_individuals.index: #If its new, we always add it\nself.evaluated_individuals.loc[key] = np.nan\nself.evaluated_individuals.loc[key,\"Individual\"] = copy.deepcopy(individual)\nself.population.append(individual)\nnew_individuals.append(individual)\nelse:#If its old\nif keep_repeats: #If we want to keep repeats, we add it\nself.population.append(individual)\nnew_individuals.append(individual)\nelif mutate_until_unique: #If its old and we don't want repeats, we can optionally mutate it until it is unique\nfor _ in range(20):\nindividual = copy.deepcopy(individual)\nindividual.mutate()\nkey = individual.unique_id()\nif key not in self.evaluated_individuals.index:\nself.evaluated_individuals.loc[key] = np.nan\nself.evaluated_individuals.loc[key,\"Individual\"] = copy.deepcopy(individual)\nself.population.append(individual)\nnew_individuals.append(individual)\nbreak\nreturn new_individuals\n</code></pre>"},{"location":"documentation/tpot2/population/#tpot2.population.Population.create_offspring","title":"<code>create_offspring(parents_list, var_op_list, add_to_population=True, keep_repeats=False, mutate_until_unique=True, n_jobs=1)</code>","text":"<p>parents_list: a list of lists of parents.  var_op_list: a list of var_ops to apply to each list of parents. Should be the same length as parents_list.</p> <p>for example: parents_list = [[parent1, parent2], [parent3]] var_op_list = [\"crossover\", \"mutate\"]</p> <p>This will apply crossover to parent1 and parent2 and mutate to parent3.</p> <p>Creates offspring from parents using the var_op_list. If string, will use a built in method      - \"crossover\" : crossover     - \"mutate\" : mutate     - \"mutate_and_crossover\" : mutate_and_crossover     - \"cross_and_mutate\" : cross_and_mutate</p> Source code in <code>tpot2/population.py</code> <pre><code>def create_offspring(self, parents_list, var_op_list, add_to_population=True, keep_repeats=False, mutate_until_unique=True, n_jobs=1):\n'''\n    parents_list: a list of lists of parents. \n    var_op_list: a list of var_ops to apply to each list of parents. Should be the same length as parents_list.\n    for example:\n    parents_list = [[parent1, parent2], [parent3]]\n    var_op_list = [\"crossover\", \"mutate\"]\n    This will apply crossover to parent1 and parent2 and mutate to parent3.\n    Creates offspring from parents using the var_op_list.\n    If string, will use a built in method \n        - \"crossover\" : crossover\n        - \"mutate\" : mutate\n        - \"mutate_and_crossover\" : mutate_and_crossover\n        - \"cross_and_mutate\" : cross_and_mutate\n    '''\nnew_offspring = []\nall_offspring = parallel_create_offspring(parents_list, var_op_list, n_jobs=n_jobs)\nfor parents, offspring, var_op in zip(parents_list, all_offspring, var_op_list):\n# if var_op in built_in_var_ops_dict:\n#     var_op = built_in_var_ops_dict[var_op]\n# offspring = copy.deepcopy(parents)\n# offspring = var_op(offspring)\n# if isinstance(offspring, collections.abc.Iterable):\n#     offspring = offspring[0] \nif add_to_population:\nadded = self.add_to_population(offspring, keep_repeats=keep_repeats, mutate_until_unique=mutate_until_unique)\nif len(added) &gt; 0:\nfor new_child in added:\nparent_keys = [parent.unique_id() for parent in parents]\nif not pd.api.types.is_object_dtype(self.evaluated_individuals[\"Parents\"]): #TODO Is there a cleaner way of doing this? Not required for some python environments?\nself.evaluated_individuals[\"Parents\"] = self.evaluated_individuals[\"Parents\"].astype('object')\nself.evaluated_individuals.at[new_child.unique_id(),\"Parents\"] = tuple(parent_keys)\n#if var_op is a function\nif hasattr(var_op, '__call__'):\nself.evaluated_individuals.at[new_child.unique_id(),\"Variation_Function\"] = var_op.__name__\nelse:\nself.evaluated_individuals.at[new_child.unique_id(),\"Variation_Function\"] = var_op\nnew_offspring.append(new_child)\nelse:\nnew_offspring.append(offspring)\nreturn new_offspring\n</code></pre>"},{"location":"documentation/tpot2/population/#tpot2.population.Population.get_column","title":"<code>get_column(individual, column_names=None, to_numpy=True)</code>","text":"<p>Update the column_name column in the evaluated_individuals with the data. If the data is a list, it must be the same length as the evaluated_individuals. If the data is a single value, it will be applied to all individuals in the evaluated_individuals.</p> Source code in <code>tpot2/population.py</code> <pre><code>def get_column(self, individual, column_names=None, to_numpy=True):\n'''\n    Update the column_name column in the evaluated_individuals with the data.\n    If the data is a list, it must be the same length as the evaluated_individuals.\n    If the data is a single value, it will be applied to all individuals in the evaluated_individuals.\n    '''\nif isinstance(individual, collections.abc.Iterable):\nif self.use_unique_id:\nkey = [ind.unique_id() for ind in individual]\nelse:\nkey = individual\nelse:\nif self.use_unique_id:\nkey = individual.unique_id()\nelse:\nkey = individual\nif column_names is not None:\nslice = self.evaluated_individuals.loc[key,column_names]\nelse:\nslice = self.evaluated_individuals.loc[key]\nif to_numpy:\nslice.reset_index(drop=True, inplace=True)\nreturn slice.to_numpy()\nelse:\nreturn slice\n</code></pre>"},{"location":"documentation/tpot2/population/#tpot2.population.Population.remove_invalid_from_population","title":"<code>remove_invalid_from_population(column_names, invalid_value='INVALID')</code>","text":"<p>Remove individuals from the live population if either do not have a value in the column_name column or if the value contains np.nan.</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <p>The name of the column to check for np.nan values.</p> <code>str</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot2/population.py</code> <pre><code>def remove_invalid_from_population(self, column_names, invalid_value = \"INVALID\"):\n'''\n    Remove individuals from the live population if either do not have a value in the column_name column or if the value contains np.nan.\n    Parameters\n    ----------\n    column_name : {str}\n        The name of the column to check for np.nan values.\n    Returns\n    -------\n    None\n    '''\nif isinstance(column_names, str): #TODO check this\ncolumn_names = [column_names]\nnew_pop = []\nis_valid = lambda ind: ind.unique_id() not in self.evaluated_individuals.index or invalid_value not in self.evaluated_individuals.loc[ind.unique_id(),column_names].to_list()\nself.population = [ind for ind in self.population if is_valid(ind)]\n</code></pre>"},{"location":"documentation/tpot2/population/#tpot2.population.Population.set_population","title":"<code>set_population(new_population, keep_repeats=True)</code>","text":"<p>sets population to new population for selection?</p> Source code in <code>tpot2/population.py</code> <pre><code>def set_population(self,  new_population, keep_repeats=True):\n'''\n    sets population to new population\n    for selection?\n    '''\nself.population = []\nself.add_to_population(new_population, keep_repeats=keep_repeats)\n</code></pre>"},{"location":"documentation/tpot2/population/#tpot2.population.Population.update_column","title":"<code>update_column(individual, column_names, data)</code>","text":"<p>Update the column_name column in the evaluated_individuals with the data. If the data is a list, it must be the same length as the evaluated_individuals. If the data is a single value, it will be applied to all individuals in the evaluated_individuals.</p> Source code in <code>tpot2/population.py</code> <pre><code>def update_column(self, individual, column_names, data):\n'''\n    Update the column_name column in the evaluated_individuals with the data.\n    If the data is a list, it must be the same length as the evaluated_individuals.\n    If the data is a single value, it will be applied to all individuals in the evaluated_individuals.\n    '''\nif isinstance(individual, collections.abc.Iterable):\nif self.use_unique_id:\nkey = [ind.unique_id() for ind in individual]\nelse:\nkey = individual\nelse:\nif self.use_unique_id:\nkey = individual.unique_id()\nelse:\nkey = individual\nself.evaluated_individuals.loc[key,column_names] = data\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/arithmetictransformer/","title":"Arithmetictransformer","text":""},{"location":"documentation/tpot2/builtin_modules/column_one_hot_encoder/","title":"Column one hot encoder","text":""},{"location":"documentation/tpot2/builtin_modules/column_one_hot_encoder/#tpot2.builtin_modules.column_one_hot_encoder.ColumnOneHotEncoder","title":"<code>ColumnOneHotEncoder</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot2/builtin_modules/column_one_hot_encoder.py</code> <pre><code>class ColumnOneHotEncoder(BaseEstimator, TransformerMixin):\ndef __init__(self, columns='auto', drop=None, handle_unknown='error', sparse_output=False, min_frequency=None,max_categories=None):\nself.columns = columns\nself.drop = drop\nself.handle_unknown = handle_unknown\nself.sparse_output = sparse_output\nself.min_frequency = min_frequency\nself.max_categories = max_categories\ndef fit(self, X, y=None):\n\"\"\"Fit OneHotEncoder to X, then transform X.\n        Equivalent to self.fit(X).transform(X), but more convenient and more\n        efficient. See fit for the parameters, transform for the return value.\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Dense array or sparse matrix.\n        y: array-like {n_samples,} (Optional, ignored)\n            Feature labels\n        \"\"\"\nif (self.columns == \"categorical\" or self.columns == \"numeric\") and not isinstance(X, pd.DataFrame):\nraise ValueError(f\"Invalid value for columns: {self.columns}. \"\n\"Only 'all' or &lt;list&gt; is supported for np arrays\")\nif self.columns == \"categorical\":\nself.columns_ = list(X.select_dtypes(exclude='number').columns)\nelif self.columns == \"numeric\":\nself.columns_ =  [col for col in X.columns if is_numeric_dtype(X[col])]\nelif self.columns == \"all\":\nif isinstance(X, pd.DataFrame):\nself.columns_ = X.columns\nelse:\nself.columns_ = list(range(X.shape[1]))\nelif isinstance(self.columns, list):\nself.columns_ = self.columns\nelse:\nraise ValueError(f\"Invalid value for columns: {self.columns}\")\nif len(self.columns_) == 0:\nreturn self\nself.enc = sklearn.preprocessing.OneHotEncoder( categories='auto',   \ndrop = self.drop,\nhandle_unknown = self.handle_unknown,\nsparse_output = self.sparse_output,\nmin_frequency = self.min_frequency,\nmax_categories = self.max_categories)\n#TODO make this more consistent with sklearn baseimputer/baseencoder\nif isinstance(X, pd.DataFrame):\nself.enc.set_output(transform=\"pandas\")\nfor col in X.columns:\n# check if the column name is not a string\nif not isinstance(col, str):\n# if it's not a string, rename the column with \"X\" prefix\nX.rename(columns={col: f\"X{col}\"}, inplace=True)\nif len(self.columns_) == X.shape[1]:\nX_sel = self.enc.fit(X)\nelse:\nX_sel, X_not_sel = _X_selected(X, self.columns_)\nX_sel = self.enc.fit(X_sel)\nreturn self\ndef transform(self, X):\n\"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Dense array or sparse matrix.\n        Returns\n        -------\n        X_out : sparse matrix if sparse=True else a 2-d array, dtype=int\n            Transformed input.\n        \"\"\"\nif len(self.columns_) == 0:\nreturn X\n#TODO make this more consistent with sklearn baseimputer/baseencoder\nif isinstance(X, pd.DataFrame):\nfor col in X.columns:\n# check if the column name is not a string\nif not isinstance(col, str):\n# if it's not a string, rename the column with \"X\" prefix\nX.rename(columns={col: f\"X{col}\"}, inplace=True)\nif len(self.columns_) == X.shape[1]:\nreturn self.enc.transform(X)\nelse:\nX_sel, X_not_sel= _X_selected(X, self.columns_)\nX_sel = self.enc.transform(X_sel)\n#If X is dataframe\nif isinstance(X, pd.DataFrame):\nX_sel = pd.DataFrame(X_sel, columns=self.enc.get_feature_names_out())\nreturn pd.concat([X_not_sel.reset_index(drop=True), X_sel.reset_index(drop=True)], axis=1)\nelse:\nreturn np.hstack((X_not_sel, X_sel))\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/column_one_hot_encoder/#tpot2.builtin_modules.column_one_hot_encoder.ColumnOneHotEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit OneHotEncoder to X, then transform X.</p> <p>Equivalent to self.fit(X).transform(X), but more convenient and more efficient. See fit for the parameters, transform for the return value.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix, shape=(n_samples, n_features)</code> <p>Dense array or sparse matrix.</p> required <code>y</code> <p>Feature labels</p> <code>None</code> Source code in <code>tpot2/builtin_modules/column_one_hot_encoder.py</code> <pre><code>def fit(self, X, y=None):\n\"\"\"Fit OneHotEncoder to X, then transform X.\n    Equivalent to self.fit(X).transform(X), but more convenient and more\n    efficient. See fit for the parameters, transform for the return value.\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Dense array or sparse matrix.\n    y: array-like {n_samples,} (Optional, ignored)\n        Feature labels\n    \"\"\"\nif (self.columns == \"categorical\" or self.columns == \"numeric\") and not isinstance(X, pd.DataFrame):\nraise ValueError(f\"Invalid value for columns: {self.columns}. \"\n\"Only 'all' or &lt;list&gt; is supported for np arrays\")\nif self.columns == \"categorical\":\nself.columns_ = list(X.select_dtypes(exclude='number').columns)\nelif self.columns == \"numeric\":\nself.columns_ =  [col for col in X.columns if is_numeric_dtype(X[col])]\nelif self.columns == \"all\":\nif isinstance(X, pd.DataFrame):\nself.columns_ = X.columns\nelse:\nself.columns_ = list(range(X.shape[1]))\nelif isinstance(self.columns, list):\nself.columns_ = self.columns\nelse:\nraise ValueError(f\"Invalid value for columns: {self.columns}\")\nif len(self.columns_) == 0:\nreturn self\nself.enc = sklearn.preprocessing.OneHotEncoder( categories='auto',   \ndrop = self.drop,\nhandle_unknown = self.handle_unknown,\nsparse_output = self.sparse_output,\nmin_frequency = self.min_frequency,\nmax_categories = self.max_categories)\n#TODO make this more consistent with sklearn baseimputer/baseencoder\nif isinstance(X, pd.DataFrame):\nself.enc.set_output(transform=\"pandas\")\nfor col in X.columns:\n# check if the column name is not a string\nif not isinstance(col, str):\n# if it's not a string, rename the column with \"X\" prefix\nX.rename(columns={col: f\"X{col}\"}, inplace=True)\nif len(self.columns_) == X.shape[1]:\nX_sel = self.enc.fit(X)\nelse:\nX_sel, X_not_sel = _X_selected(X, self.columns_)\nX_sel = self.enc.fit(X_sel)\nreturn self\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/column_one_hot_encoder/#tpot2.builtin_modules.column_one_hot_encoder.ColumnOneHotEncoder.transform","title":"<code>transform(X)</code>","text":"<p>Transform X using one-hot encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix, shape=(n_samples, n_features)</code> <p>Dense array or sparse matrix.</p> required <p>Returns:</p> Name Type Description <code>X_out</code> <code>sparse matrix if sparse=True else a 2-d array, dtype=int</code> <p>Transformed input.</p> Source code in <code>tpot2/builtin_modules/column_one_hot_encoder.py</code> <pre><code>def transform(self, X):\n\"\"\"Transform X using one-hot encoding.\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Dense array or sparse matrix.\n    Returns\n    -------\n    X_out : sparse matrix if sparse=True else a 2-d array, dtype=int\n        Transformed input.\n    \"\"\"\nif len(self.columns_) == 0:\nreturn X\n#TODO make this more consistent with sklearn baseimputer/baseencoder\nif isinstance(X, pd.DataFrame):\nfor col in X.columns:\n# check if the column name is not a string\nif not isinstance(col, str):\n# if it's not a string, rename the column with \"X\" prefix\nX.rename(columns={col: f\"X{col}\"}, inplace=True)\nif len(self.columns_) == X.shape[1]:\nreturn self.enc.transform(X)\nelse:\nX_sel, X_not_sel= _X_selected(X, self.columns_)\nX_sel = self.enc.transform(X_sel)\n#If X is dataframe\nif isinstance(X, pd.DataFrame):\nX_sel = pd.DataFrame(X_sel, columns=self.enc.get_feature_names_out())\nreturn pd.concat([X_not_sel.reset_index(drop=True), X_sel.reset_index(drop=True)], axis=1)\nelse:\nreturn np.hstack((X_not_sel, X_sel))\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/feature_encoding_frequency_selector/","title":"Feature encoding frequency selector","text":""},{"location":"documentation/tpot2/builtin_modules/feature_encoding_frequency_selector/#tpot2.builtin_modules.feature_encoding_frequency_selector.FeatureEncodingFrequencySelector","title":"<code>FeatureEncodingFrequencySelector</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>SelectorMixin</code></p> <p>Feature selector based on Encoding Frequency. Encoding frequency is the frequency of each unique element(0/1/2/3) present in a feature set.  Features are selected on the basis of a threshold assigned for encoding frequency. If frequency of any unique element is less than or equal to threshold, the feature is removed.</p> Source code in <code>tpot2/builtin_modules/feature_encoding_frequency_selector.py</code> <pre><code>class FeatureEncodingFrequencySelector(BaseEstimator, SelectorMixin):\n\"\"\"Feature selector based on Encoding Frequency. Encoding frequency is the frequency of each unique element(0/1/2/3) present in a feature set. \n     Features are selected on the basis of a threshold assigned for encoding frequency. If frequency of any unique element is less than or equal to threshold, the feature is removed.  \"\"\"\n@property\ndef __name__(self):\n\"\"\"Instance name is the same as the class name. \"\"\"\nreturn self.__class__.__name__\ndef __init__(self, threshold):\n\"\"\"Create a FeatureEncodingFrequencySelector object.\n        Parameters\n        ----------\n        threshold : float, required\n            Threshold value for allele frequency. If frequency of A or frequency of a is less than the threshold value then the feature is dropped.\n        Returns\n        -------\n        None\n        \"\"\"\nself.threshold = threshold\n\"\"\"def fit(self, X, y=None):\n        Fit FeatureAlleleFrequencySelector for feature selection\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_features}\n            The training input samples.\n        y : numpy array {n_samples,}\n            The training target values.\n        Returns\n        -------\n        self : object\n            Returns a copy of the estimator\n        self.selected_feature_indexes = []\n        self.no_of_features = X.shape[1]\n        # Finding the no of alleles in each feature column\n        for i in range(0, X.shape[1]):\n            no_of_AA_featurewise = np.count_nonzero(X[:,i]==0)\n            no_of_Aa_featurewise = np.count_nonzero(X[:,i]==1)\n            no_of_aa_featurewise = np.count_nonzero(X[:,i]==2)\n            frequency_A_featurewise = (2*no_of_AA_featurewise + no_of_Aa_featurewise) / (2*no_of_AA_featurewise + \n            2*no_of_Aa_featurewise + 2*no_of_aa_featurewise)\n            frequency_a_featurewise = 1 - frequency_A_featurewise\n            if(not(frequency_A_featurewise &lt;= self.threshold) and not(frequency_a_featurewise &lt;= self.threshold)):\n                self.selected_feature_indexes.append(i)\n        return self\"\"\"\n\"\"\"def transform(self, X):\n        Make subset after fit\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_features}\n            New data, where n_samples is the number of samples and n_features is the number of features.\n        Returns\n        -------\n        X_transformed : numpy ndarray, {n_samples, n_features}\n            The transformed feature set.\n        X_transformed = X[:, self.selected_feature_indexes]\n        return X_transformed\"\"\"\ndef fit(self, X, y=None) :\n\"\"\"Fit FeatureEncodingFrequencySelector for feature selection. This function gets the appropriate features. \"\"\"\nself.selected_feature_indexes = []\nself.no_of_original_features = X.shape[1]\n# Finding the frequency of all the unique elements present featurewise in the input variable X\nfor i in range(0, X.shape[1]):\nunique, counts = np.unique(X[:,i], return_counts=True)\nelement_count_dict_featurewise = dict(zip(unique, counts))\nelement_frequency_dict_featurewise = {}\nfeature_column_selected = True\nfor x in unique:\nx_frequency_featurewise = element_count_dict_featurewise[x] / sum(counts)\nelement_frequency_dict_featurewise[x] = x_frequency_featurewise\nfor frequency in element_frequency_dict_featurewise.values():\nif frequency &lt;= self.threshold :\nfeature_column_selected = False\nbreak\nif feature_column_selected == True :\nself.selected_feature_indexes.append(i)\nif not len(self.selected_feature_indexes):\n\"\"\"msg = \"No feature in X meets the encoding frequency threshold {0:.5f}\"\n            raise ValueError(msg.format(self.threshold))\"\"\"\nfor i in range(0, X.shape[1]):\nself.selected_feature_indexes.append(i)\nreturn self\ndef transform(self, X):\n\"\"\" Make subset after fit. This function returns a transformed version of X.  \"\"\"\nX_transformed = X[:, self.selected_feature_indexes]\nreturn X_transformed\ndef _get_support_mask(self):\n\"\"\"\n        Get the boolean mask indicating which features are selected\n        It is the abstractmethod\n        Returns\n        -------\n        support : boolean array of shape [# input features]\n            An element is True iff its corresponding feature is selected for retention.\n            \"\"\"\nn_features = self.no_of_original_features\nmask = np.zeros(n_features, dtype=bool)\nmask[np.asarray(self.selected_feature_indexes)] = True\nreturn mask\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/feature_encoding_frequency_selector/#tpot2.builtin_modules.feature_encoding_frequency_selector.FeatureEncodingFrequencySelector.__name__","title":"<code>__name__</code>  <code>property</code>","text":"<p>Instance name is the same as the class name.</p>"},{"location":"documentation/tpot2/builtin_modules/feature_encoding_frequency_selector/#tpot2.builtin_modules.feature_encoding_frequency_selector.FeatureEncodingFrequencySelector.__init__","title":"<code>__init__(threshold)</code>","text":"<p>Create a FeatureEncodingFrequencySelector object.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>(float, required)</code> <p>Threshold value for allele frequency. If frequency of A or frequency of a is less than the threshold value then the feature is dropped.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot2/builtin_modules/feature_encoding_frequency_selector.py</code> <pre><code>def __init__(self, threshold):\n\"\"\"Create a FeatureEncodingFrequencySelector object.\n    Parameters\n    ----------\n    threshold : float, required\n        Threshold value for allele frequency. If frequency of A or frequency of a is less than the threshold value then the feature is dropped.\n    Returns\n    -------\n    None\n    \"\"\"\nself.threshold = threshold\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/feature_encoding_frequency_selector/#tpot2.builtin_modules.feature_encoding_frequency_selector.FeatureEncodingFrequencySelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit FeatureEncodingFrequencySelector for feature selection. This function gets the appropriate features.</p> Source code in <code>tpot2/builtin_modules/feature_encoding_frequency_selector.py</code> <pre><code>def fit(self, X, y=None) :\n\"\"\"Fit FeatureEncodingFrequencySelector for feature selection. This function gets the appropriate features. \"\"\"\nself.selected_feature_indexes = []\nself.no_of_original_features = X.shape[1]\n# Finding the frequency of all the unique elements present featurewise in the input variable X\nfor i in range(0, X.shape[1]):\nunique, counts = np.unique(X[:,i], return_counts=True)\nelement_count_dict_featurewise = dict(zip(unique, counts))\nelement_frequency_dict_featurewise = {}\nfeature_column_selected = True\nfor x in unique:\nx_frequency_featurewise = element_count_dict_featurewise[x] / sum(counts)\nelement_frequency_dict_featurewise[x] = x_frequency_featurewise\nfor frequency in element_frequency_dict_featurewise.values():\nif frequency &lt;= self.threshold :\nfeature_column_selected = False\nbreak\nif feature_column_selected == True :\nself.selected_feature_indexes.append(i)\nif not len(self.selected_feature_indexes):\n\"\"\"msg = \"No feature in X meets the encoding frequency threshold {0:.5f}\"\n        raise ValueError(msg.format(self.threshold))\"\"\"\nfor i in range(0, X.shape[1]):\nself.selected_feature_indexes.append(i)\nreturn self\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/feature_encoding_frequency_selector/#tpot2.builtin_modules.feature_encoding_frequency_selector.FeatureEncodingFrequencySelector.transform","title":"<code>transform(X)</code>","text":"<p>Make subset after fit. This function returns a transformed version of X.</p> Source code in <code>tpot2/builtin_modules/feature_encoding_frequency_selector.py</code> <pre><code>def transform(self, X):\n\"\"\" Make subset after fit. This function returns a transformed version of X.  \"\"\"\nX_transformed = X[:, self.selected_feature_indexes]\nreturn X_transformed\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/feature_set_selector/","title":"Feature set selector","text":"<p>This file is part of the TPOT library.</p> <p>TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - and many more generous open source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot2/builtin_modules/feature_set_selector/#tpot2.builtin_modules.feature_set_selector.FeatureSetSelector","title":"<code>FeatureSetSelector</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>SelectorMixin</code></p> <p>Select predefined feature subsets.</p> Source code in <code>tpot2/builtin_modules/feature_set_selector.py</code> <pre><code>class FeatureSetSelector(BaseEstimator, SelectorMixin):\n\"\"\"Select predefined feature subsets.\"\"\"\ndef __init__(self, sel_subset=None, name=None):\n\"\"\"Create a FeatureSetSelector object.\n        Parameters\n        ----------\n        sel_subset: list or int\n            If X is a dataframe, items in sel_subset must correspond to column names\n            If X is a numpy array, items in sel_subset must correspond to column indexes\n            int: index of a single column\n        Returns\n        -------\n        None\n        \"\"\"\nself.name = name\nself.sel_subset = sel_subset\ndef fit(self, X, y=None):\n\"\"\"Fit FeatureSetSelector for feature selection\n        Parameters\n        ----------\n        X: array-like of shape (n_samples, n_features)\n            The training input samples.\n        y: array-like, shape (n_samples,)\n            The target values (integers that correspond to classes in classification, real numbers in regression).\n        Returns\n        -------\n        self: object\n            Returns a copy of the estimator\n        \"\"\"\nif isinstance(self.sel_subset, int) or isinstance(self.sel_subset, str):\nself.sel_subset = [self.sel_subset]\n#generate  self.feat_list_idx\nif isinstance(X, pd.DataFrame):\nself.feature_names_in_ = X.columns.tolist()\nself.feat_list_idx = sorted([self.feature_names_in_.index(feat) for feat in self.sel_subset])\nelif isinstance(X, np.ndarray):\nself.feature_names_in_ = None#list(range(X.shape[1]))\nself.feat_list_idx = sorted(self.sel_subset)\nn_features = X.shape[1]\nself.mask = np.zeros(n_features, dtype=bool)\nself.mask[np.asarray(self.feat_list_idx)] = True\nreturn self\n#TODO keep returned as dataframe if input is dataframe? may not be consistent with sklearn\n# def transform(self, X):\ndef _get_support_mask(self):\n\"\"\"\n        Get the boolean mask indicating which features are selected\n        Returns\n        -------\n        support : boolean array of shape [# input features]\n            An element is True iff its corresponding feature is selected for\n            retention.\n        \"\"\"\nreturn self.mask\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/feature_set_selector/#tpot2.builtin_modules.feature_set_selector.FeatureSetSelector.__init__","title":"<code>__init__(sel_subset=None, name=None)</code>","text":"<p>Create a FeatureSetSelector object.</p> <p>Parameters:</p> Name Type Description Default <code>sel_subset</code> <p>If X is a dataframe, items in sel_subset must correspond to column names If X is a numpy array, items in sel_subset must correspond to column indexes int: index of a single column</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>tpot2/builtin_modules/feature_set_selector.py</code> <pre><code>def __init__(self, sel_subset=None, name=None):\n\"\"\"Create a FeatureSetSelector object.\n    Parameters\n    ----------\n    sel_subset: list or int\n        If X is a dataframe, items in sel_subset must correspond to column names\n        If X is a numpy array, items in sel_subset must correspond to column indexes\n        int: index of a single column\n    Returns\n    -------\n    None\n    \"\"\"\nself.name = name\nself.sel_subset = sel_subset\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/feature_set_selector/#tpot2.builtin_modules.feature_set_selector.FeatureSetSelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit FeatureSetSelector for feature selection</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>The training input samples.</p> required <code>y</code> <p>The target values (integers that correspond to classes in classification, real numbers in regression).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Returns a copy of the estimator</p> Source code in <code>tpot2/builtin_modules/feature_set_selector.py</code> <pre><code>def fit(self, X, y=None):\n\"\"\"Fit FeatureSetSelector for feature selection\n    Parameters\n    ----------\n    X: array-like of shape (n_samples, n_features)\n        The training input samples.\n    y: array-like, shape (n_samples,)\n        The target values (integers that correspond to classes in classification, real numbers in regression).\n    Returns\n    -------\n    self: object\n        Returns a copy of the estimator\n    \"\"\"\nif isinstance(self.sel_subset, int) or isinstance(self.sel_subset, str):\nself.sel_subset = [self.sel_subset]\n#generate  self.feat_list_idx\nif isinstance(X, pd.DataFrame):\nself.feature_names_in_ = X.columns.tolist()\nself.feat_list_idx = sorted([self.feature_names_in_.index(feat) for feat in self.sel_subset])\nelif isinstance(X, np.ndarray):\nself.feature_names_in_ = None#list(range(X.shape[1]))\nself.feat_list_idx = sorted(self.sel_subset)\nn_features = X.shape[1]\nself.mask = np.zeros(n_features, dtype=bool)\nself.mask[np.asarray(self.feat_list_idx)] = True\nreturn self\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/feature_transformers/","title":"Feature transformers","text":"<p>Copyright 2015-Present Randal S. Olson.</p> <p>This file is part of the TPOT library.</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot2/builtin_modules/feature_transformers/#tpot2.builtin_modules.feature_transformers.CategoricalSelector","title":"<code>CategoricalSelector</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Meta-transformer for selecting categorical features and transform them using OneHotEncoder.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>int</code> <p>Maximum number of unique values per feature to consider the feature to be categorical.</p> <code>10</code> <p>minimum_fraction: float, default=None     Minimum fraction of unique values in a feature to consider the feature     to be categorical.</p> Source code in <code>tpot2/builtin_modules/feature_transformers.py</code> <pre><code>class CategoricalSelector(BaseEstimator, TransformerMixin):\n\"\"\"Meta-transformer for selecting categorical features and transform them using OneHotEncoder.\n    Parameters\n    ----------\n    threshold : int, default=10\n        Maximum number of unique values per feature to consider the feature\n        to be categorical.\n    minimum_fraction: float, default=None\n        Minimum fraction of unique values in a feature to consider the feature\n        to be categorical.\n    \"\"\"\ndef __init__(self, threshold=10, minimum_fraction=None):\n\"\"\"Create a CategoricalSelector object.\"\"\"\nself.threshold = threshold\nself.minimum_fraction = minimum_fraction\ndef fit(self, X, y=None):\n\"\"\"Do nothing and return the estimator unchanged\n        This method is just there to implement the usual API and hence\n        work in pipelines.\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\nX = check_array(X, accept_sparse='csr')\nreturn self\ndef transform(self, X):\n\"\"\"Select categorical features and transform them using OneHotEncoder.\n        Parameters\n        ----------\n        X: numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples and n_components is the number of components.\n        Returns\n        -------\n        array-like, {n_samples, n_components}\n        \"\"\"\nselected = auto_select_categorical_features(X, threshold=self.threshold)\nX_sel, _, n_selected, _ = _X_selected(X, selected)\nif n_selected == 0:\n# No features selected.\nraise ValueError('No categorical feature was found!')\nelse:\nohe = OneHotEncoder(categorical_features='all', sparse=False, minimum_fraction=self.minimum_fraction)\nreturn ohe.fit_transform(X_sel)\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/feature_transformers/#tpot2.builtin_modules.feature_transformers.CategoricalSelector.__init__","title":"<code>__init__(threshold=10, minimum_fraction=None)</code>","text":"<p>Create a CategoricalSelector object.</p> Source code in <code>tpot2/builtin_modules/feature_transformers.py</code> <pre><code>def __init__(self, threshold=10, minimum_fraction=None):\n\"\"\"Create a CategoricalSelector object.\"\"\"\nself.threshold = threshold\nself.minimum_fraction = minimum_fraction\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/feature_transformers/#tpot2.builtin_modules.feature_transformers.CategoricalSelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged This method is just there to implement the usual API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot2/builtin_modules/feature_transformers.py</code> <pre><code>def fit(self, X, y=None):\n\"\"\"Do nothing and return the estimator unchanged\n    This method is just there to implement the usual API and hence\n    work in pipelines.\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\nX = check_array(X, accept_sparse='csr')\nreturn self\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/feature_transformers/#tpot2.builtin_modules.feature_transformers.CategoricalSelector.transform","title":"<code>transform(X)</code>","text":"<p>Select categorical features and transform them using OneHotEncoder.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>New data, where n_samples is the number of samples and n_components is the number of components.</p> required <p>Returns:</p> Type Description <code>(array - like, {n_samples, n_components})</code> Source code in <code>tpot2/builtin_modules/feature_transformers.py</code> <pre><code>def transform(self, X):\n\"\"\"Select categorical features and transform them using OneHotEncoder.\n    Parameters\n    ----------\n    X: numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples and n_components is the number of components.\n    Returns\n    -------\n    array-like, {n_samples, n_components}\n    \"\"\"\nselected = auto_select_categorical_features(X, threshold=self.threshold)\nX_sel, _, n_selected, _ = _X_selected(X, selected)\nif n_selected == 0:\n# No features selected.\nraise ValueError('No categorical feature was found!')\nelse:\nohe = OneHotEncoder(categorical_features='all', sparse=False, minimum_fraction=self.minimum_fraction)\nreturn ohe.fit_transform(X_sel)\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/feature_transformers/#tpot2.builtin_modules.feature_transformers.ContinuousSelector","title":"<code>ContinuousSelector</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Meta-transformer for selecting continuous features and transform them using PCA.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>int</code> <p>Maximum number of unique values per feature to consider the feature to be categorical.</p> <code>10</code> <p>svd_solver : string {'auto', 'full', 'arpack', 'randomized'}     auto :         the solver is selected by a default policy based on <code>X.shape</code> and         <code>n_components</code>: if the input data is larger than 500x500 and the         number of components to extract is lower than 80% of the smallest         dimension of the data, then the more efficient 'randomized'         method is enabled. Otherwise the exact full SVD is computed and         optionally truncated afterwards.     full :         run exact full SVD calling the standard LAPACK solver via         <code>scipy.linalg.svd</code> and select the components by postprocessing     arpack :         run SVD truncated to n_components calling ARPACK solver via         <code>scipy.sparse.linalg.svds</code>. It requires strictly         0 &lt; n_components &lt; X.shape[1]     randomized :         run randomized SVD by the method of Halko et al.</p> <p>iterated_power : int &gt;= 0, or 'auto', (default 'auto')     Number of iterations for the power method computed by     svd_solver == 'randomized'.</p> Source code in <code>tpot2/builtin_modules/feature_transformers.py</code> <pre><code>class ContinuousSelector(BaseEstimator, TransformerMixin):\n\"\"\"Meta-transformer for selecting continuous features and transform them using PCA.\n    Parameters\n    ----------\n    threshold : int, default=10\n        Maximum number of unique values per feature to consider the feature\n        to be categorical.\n    svd_solver : string {'auto', 'full', 'arpack', 'randomized'}\n        auto :\n            the solver is selected by a default policy based on `X.shape` and\n            `n_components`: if the input data is larger than 500x500 and the\n            number of components to extract is lower than 80% of the smallest\n            dimension of the data, then the more efficient 'randomized'\n            method is enabled. Otherwise the exact full SVD is computed and\n            optionally truncated afterwards.\n        full :\n            run exact full SVD calling the standard LAPACK solver via\n            `scipy.linalg.svd` and select the components by postprocessing\n        arpack :\n            run SVD truncated to n_components calling ARPACK solver via\n            `scipy.sparse.linalg.svds`. It requires strictly\n            0 &lt; n_components &lt; X.shape[1]\n        randomized :\n            run randomized SVD by the method of Halko et al.\n    iterated_power : int &gt;= 0, or 'auto', (default 'auto')\n        Number of iterations for the power method computed by\n        svd_solver == 'randomized'.\n    \"\"\"\ndef __init__(self, threshold=10, svd_solver='randomized' ,iterated_power='auto', random_state=42):\n\"\"\"Create a ContinuousSelector object.\"\"\"\nself.threshold = threshold\nself.svd_solver = svd_solver\nself.iterated_power = iterated_power\nself.random_state = random_state\ndef fit(self, X, y=None):\n\"\"\"Do nothing and return the estimator unchanged\n        This method is just there to implement the usual API and hence\n        work in pipelines.\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\nX = check_array(X)\nreturn self\ndef transform(self, X):\n\"\"\"Select continuous features and transform them using PCA.\n        Parameters\n        ----------\n        X: numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples and n_components is the number of components.\n        Returns\n        -------\n        array-like, {n_samples, n_components}\n        \"\"\"\nselected = auto_select_categorical_features(X, threshold=self.threshold)\n_, X_sel, n_selected, _ = _X_selected(X, selected)\nif n_selected == 0:\n# No features selected.\nraise ValueError('No continuous feature was found!')\nelse:\npca = PCA(svd_solver=self.svd_solver, iterated_power=self.iterated_power, random_state=self.random_state)\nreturn pca.fit_transform(X_sel)\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/feature_transformers/#tpot2.builtin_modules.feature_transformers.ContinuousSelector.__init__","title":"<code>__init__(threshold=10, svd_solver='randomized', iterated_power='auto', random_state=42)</code>","text":"<p>Create a ContinuousSelector object.</p> Source code in <code>tpot2/builtin_modules/feature_transformers.py</code> <pre><code>def __init__(self, threshold=10, svd_solver='randomized' ,iterated_power='auto', random_state=42):\n\"\"\"Create a ContinuousSelector object.\"\"\"\nself.threshold = threshold\nself.svd_solver = svd_solver\nself.iterated_power = iterated_power\nself.random_state = random_state\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/feature_transformers/#tpot2.builtin_modules.feature_transformers.ContinuousSelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged This method is just there to implement the usual API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot2/builtin_modules/feature_transformers.py</code> <pre><code>def fit(self, X, y=None):\n\"\"\"Do nothing and return the estimator unchanged\n    This method is just there to implement the usual API and hence\n    work in pipelines.\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\nX = check_array(X)\nreturn self\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/feature_transformers/#tpot2.builtin_modules.feature_transformers.ContinuousSelector.transform","title":"<code>transform(X)</code>","text":"<p>Select continuous features and transform them using PCA.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>New data, where n_samples is the number of samples and n_components is the number of components.</p> required <p>Returns:</p> Type Description <code>(array - like, {n_samples, n_components})</code> Source code in <code>tpot2/builtin_modules/feature_transformers.py</code> <pre><code>def transform(self, X):\n\"\"\"Select continuous features and transform them using PCA.\n    Parameters\n    ----------\n    X: numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples and n_components is the number of components.\n    Returns\n    -------\n    array-like, {n_samples, n_components}\n    \"\"\"\nselected = auto_select_categorical_features(X, threshold=self.threshold)\n_, X_sel, n_selected, _ = _X_selected(X, selected)\nif n_selected == 0:\n# No features selected.\nraise ValueError('No continuous feature was found!')\nelse:\npca = PCA(svd_solver=self.svd_solver, iterated_power=self.iterated_power, random_state=self.random_state)\nreturn pca.fit_transform(X_sel)\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/genetic_encoders/","title":"Genetic encoders","text":"<p>This file contains the class definition for all the genetic encoders. All the genetic encoder classes inherit the Scikit learn BaseEstimator and TransformerMixin classes to follow the Scikit learn paradigm.</p>"},{"location":"documentation/tpot2/builtin_modules/genetic_encoders/#tpot2.builtin_modules.genetic_encoders.DominantEncoder","title":"<code>DominantEncoder</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>This class contains the function definition for encoding the input features as a Dominant genetic model. The encoding used is AA(0)-&gt;1, Aa(1)-&gt;1, aa(2)-&gt;0.</p> Source code in <code>tpot2/builtin_modules/genetic_encoders.py</code> <pre><code>class DominantEncoder(BaseEstimator, TransformerMixin):\n\"\"\"This class contains the function definition for encoding the input features as a Dominant genetic model.\n    The encoding used is AA(0)-&gt;1, Aa(1)-&gt;1, aa(2)-&gt;0. \"\"\"\ndef fit(self, X, y=None):\n\"\"\"Do nothing and return the estimator unchanged.\n        Dummy function to fit in with the sklearn API and hence work in pipelines.\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\nreturn self\ndef transform(self, X, y=None):\n\"\"\"Transform the data by applying the Dominant encoding.\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples (number of individuals)\n            and n_components is the number of components (number of features).\n        y : None\n            Unused\n        Returns\n        -------\n        X_transformed: numpy ndarray, {n_samples, n_components}\n            The encoded feature set\n        \"\"\"\nX = check_array(X)\nmap = {0: 1, 1: 1, 2: 0}\nmapping_function = np.vectorize(lambda i: map[i] if i in map else i)\nX_transformed = mapping_function(X)\nreturn X_transformed\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/genetic_encoders/#tpot2.builtin_modules.genetic_encoders.DominantEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged. Dummy function to fit in with the sklearn API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot2/builtin_modules/genetic_encoders.py</code> <pre><code>def fit(self, X, y=None):\n\"\"\"Do nothing and return the estimator unchanged.\n    Dummy function to fit in with the sklearn API and hence work in pipelines.\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\nreturn self\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/genetic_encoders/#tpot2.builtin_modules.genetic_encoders.DominantEncoder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the data by applying the Dominant encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>New data, where n_samples is the number of samples (number of individuals) and n_components is the number of components (number of features).</p> required <code>y</code> <code>None</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>The encoded feature set</p> Source code in <code>tpot2/builtin_modules/genetic_encoders.py</code> <pre><code>def transform(self, X, y=None):\n\"\"\"Transform the data by applying the Dominant encoding.\n    Parameters\n    ----------\n    X : numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples (number of individuals)\n        and n_components is the number of components (number of features).\n    y : None\n        Unused\n    Returns\n    -------\n    X_transformed: numpy ndarray, {n_samples, n_components}\n        The encoded feature set\n    \"\"\"\nX = check_array(X)\nmap = {0: 1, 1: 1, 2: 0}\nmapping_function = np.vectorize(lambda i: map[i] if i in map else i)\nX_transformed = mapping_function(X)\nreturn X_transformed\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/genetic_encoders/#tpot2.builtin_modules.genetic_encoders.HeterosisEncoder","title":"<code>HeterosisEncoder</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>This class contains the function definition for encoding the input features as a Heterozygote Advantage genetic model. The encoding used is AA(0)-&gt;0, Aa(1)-&gt;1, aa(2)-&gt;0.</p> Source code in <code>tpot2/builtin_modules/genetic_encoders.py</code> <pre><code>class HeterosisEncoder(BaseEstimator, TransformerMixin):\n\"\"\"This class contains the function definition for encoding the input features as a Heterozygote Advantage genetic model.\n    The encoding used is AA(0)-&gt;0, Aa(1)-&gt;1, aa(2)-&gt;0. \"\"\"\ndef fit(self, X, y=None):\n\"\"\"Do nothing and return the estimator unchanged.\n        Dummy function to fit in with the sklearn API and hence work in pipelines.\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\nreturn self\ndef transform(self, X, y=None):\n\"\"\"Transform the data by applying the Heterosis encoding.\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples (number of individuals)\n            and n_components is the number of components (number of features).\n        y : None\n            Unused\n        Returns\n        -------\n        X_transformed: numpy ndarray, {n_samples, n_components}\n            The encoded feature set\n        \"\"\"\nX = check_array(X)\nmap = {0: 0, 1: 1, 2: 0}\nmapping_function = np.vectorize(lambda i: map[i] if i in map else i)\nX_transformed = mapping_function(X)\nreturn X_transformed\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/genetic_encoders/#tpot2.builtin_modules.genetic_encoders.HeterosisEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged. Dummy function to fit in with the sklearn API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot2/builtin_modules/genetic_encoders.py</code> <pre><code>def fit(self, X, y=None):\n\"\"\"Do nothing and return the estimator unchanged.\n    Dummy function to fit in with the sklearn API and hence work in pipelines.\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\nreturn self\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/genetic_encoders/#tpot2.builtin_modules.genetic_encoders.HeterosisEncoder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the data by applying the Heterosis encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>New data, where n_samples is the number of samples (number of individuals) and n_components is the number of components (number of features).</p> required <code>y</code> <code>None</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>The encoded feature set</p> Source code in <code>tpot2/builtin_modules/genetic_encoders.py</code> <pre><code>def transform(self, X, y=None):\n\"\"\"Transform the data by applying the Heterosis encoding.\n    Parameters\n    ----------\n    X : numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples (number of individuals)\n        and n_components is the number of components (number of features).\n    y : None\n        Unused\n    Returns\n    -------\n    X_transformed: numpy ndarray, {n_samples, n_components}\n        The encoded feature set\n    \"\"\"\nX = check_array(X)\nmap = {0: 0, 1: 1, 2: 0}\nmapping_function = np.vectorize(lambda i: map[i] if i in map else i)\nX_transformed = mapping_function(X)\nreturn X_transformed\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/genetic_encoders/#tpot2.builtin_modules.genetic_encoders.OverDominanceEncoder","title":"<code>OverDominanceEncoder</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>This class contains the function definition for encoding the input features as a Over Dominance genetic model. The encoding used is AA(0)-&gt;1, Aa(1)-&gt;2, aa(2)-&gt;0.</p> Source code in <code>tpot2/builtin_modules/genetic_encoders.py</code> <pre><code>class OverDominanceEncoder(BaseEstimator, TransformerMixin):\n\"\"\"This class contains the function definition for encoding the input features as a Over Dominance genetic model.\n    The encoding used is AA(0)-&gt;1, Aa(1)-&gt;2, aa(2)-&gt;0. \"\"\"\ndef fit(self, X, y=None):\n\"\"\"Do nothing and return the estimator unchanged.\n        Dummy function to fit in with the sklearn API and hence work in pipelines.\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\nreturn self\ndef transform(self, X, y=None):\n\"\"\"Transform the data by applying the Heterosis encoding.\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples (number of individuals)\n            and n_components is the number of components (number of features).\n        y : None\n            Unused\n        Returns\n        -------\n        X_transformed: numpy ndarray, {n_samples, n_components}\n            The encoded feature set\n        \"\"\"\nX = check_array(X)\nmap = {0: 1, 1: 2, 2: 0}\nmapping_function = np.vectorize(lambda i: map[i] if i in map else i)\nX_transformed = mapping_function(X)\nreturn X_transformed\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/genetic_encoders/#tpot2.builtin_modules.genetic_encoders.OverDominanceEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged. Dummy function to fit in with the sklearn API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot2/builtin_modules/genetic_encoders.py</code> <pre><code>def fit(self, X, y=None):\n\"\"\"Do nothing and return the estimator unchanged.\n    Dummy function to fit in with the sklearn API and hence work in pipelines.\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\nreturn self\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/genetic_encoders/#tpot2.builtin_modules.genetic_encoders.OverDominanceEncoder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the data by applying the Heterosis encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>New data, where n_samples is the number of samples (number of individuals) and n_components is the number of components (number of features).</p> required <code>y</code> <code>None</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>The encoded feature set</p> Source code in <code>tpot2/builtin_modules/genetic_encoders.py</code> <pre><code>def transform(self, X, y=None):\n\"\"\"Transform the data by applying the Heterosis encoding.\n    Parameters\n    ----------\n    X : numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples (number of individuals)\n        and n_components is the number of components (number of features).\n    y : None\n        Unused\n    Returns\n    -------\n    X_transformed: numpy ndarray, {n_samples, n_components}\n        The encoded feature set\n    \"\"\"\nX = check_array(X)\nmap = {0: 1, 1: 2, 2: 0}\nmapping_function = np.vectorize(lambda i: map[i] if i in map else i)\nX_transformed = mapping_function(X)\nreturn X_transformed\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/genetic_encoders/#tpot2.builtin_modules.genetic_encoders.RecessiveEncoder","title":"<code>RecessiveEncoder</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>This class contains the function definition for encoding the input features as a Recessive genetic model. The encoding used is AA(0)-&gt;0, Aa(1)-&gt;1, aa(2)-&gt;1.</p> Source code in <code>tpot2/builtin_modules/genetic_encoders.py</code> <pre><code>class RecessiveEncoder(BaseEstimator, TransformerMixin):\n\"\"\"This class contains the function definition for encoding the input features as a Recessive genetic model.\n    The encoding used is AA(0)-&gt;0, Aa(1)-&gt;1, aa(2)-&gt;1. \"\"\"\ndef fit(self, X, y=None):\n\"\"\"Do nothing and return the estimator unchanged.\n        Dummy function to fit in with the sklearn API and hence work in pipelines.\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\nreturn self\ndef transform(self, X, y=None):\n\"\"\"Transform the data by applying the Recessive encoding.\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples (number of individuals)\n            and n_components is the number of components (number of features).\n        y : None\n            Unused\n        Returns\n        -------\n        X_transformed: numpy ndarray, {n_samples, n_components}\n            The encoded feature set\n        \"\"\"\nX = check_array(X)\nmap = {0: 0, 1: 1, 2: 1}\nmapping_function = np.vectorize(lambda i: map[i] if i in map else i)\nX_transformed = mapping_function(X)\nreturn X_transformed\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/genetic_encoders/#tpot2.builtin_modules.genetic_encoders.RecessiveEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged. Dummy function to fit in with the sklearn API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot2/builtin_modules/genetic_encoders.py</code> <pre><code>def fit(self, X, y=None):\n\"\"\"Do nothing and return the estimator unchanged.\n    Dummy function to fit in with the sklearn API and hence work in pipelines.\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\nreturn self\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/genetic_encoders/#tpot2.builtin_modules.genetic_encoders.RecessiveEncoder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the data by applying the Recessive encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>New data, where n_samples is the number of samples (number of individuals) and n_components is the number of components (number of features).</p> required <code>y</code> <code>None</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>The encoded feature set</p> Source code in <code>tpot2/builtin_modules/genetic_encoders.py</code> <pre><code>def transform(self, X, y=None):\n\"\"\"Transform the data by applying the Recessive encoding.\n    Parameters\n    ----------\n    X : numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples (number of individuals)\n        and n_components is the number of components (number of features).\n    y : None\n        Unused\n    Returns\n    -------\n    X_transformed: numpy ndarray, {n_samples, n_components}\n        The encoded feature set\n    \"\"\"\nX = check_array(X)\nmap = {0: 0, 1: 1, 2: 1}\nmapping_function = np.vectorize(lambda i: map[i] if i in map else i)\nX_transformed = mapping_function(X)\nreturn X_transformed\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/genetic_encoders/#tpot2.builtin_modules.genetic_encoders.UnderDominanceEncoder","title":"<code>UnderDominanceEncoder</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>This class contains the function definition for encoding the input features as a Under Dominance genetic model. The encoding used is AA(0)-&gt;2, Aa(1)-&gt;0, aa(2)-&gt;1.</p> Source code in <code>tpot2/builtin_modules/genetic_encoders.py</code> <pre><code>class UnderDominanceEncoder(BaseEstimator, TransformerMixin):\n\"\"\"This class contains the function definition for encoding the input features as a Under Dominance genetic model.\n    The encoding used is AA(0)-&gt;2, Aa(1)-&gt;0, aa(2)-&gt;1. \"\"\"\ndef fit(self, X, y=None):\n\"\"\"Do nothing and return the estimator unchanged.\n        Dummy function to fit in with the sklearn API and hence work in pipelines.\n        Parameters\n        ----------\n        X : array-like\n        \"\"\"\nreturn self\ndef transform(self, X, y=None):\n\"\"\"Transform the data by applying the Heterosis encoding.\n        Parameters\n        ----------\n        X : numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples (number of individuals)\n            and n_components is the number of components (number of features).\n        y : None\n            Unused\n        Returns\n        -------\n        X_transformed: numpy ndarray, {n_samples, n_components}\n            The encoded feature set\n        \"\"\"\nX = check_array(X)\nmap = {0: 2, 1: 0, 2: 1}\nmapping_function = np.vectorize(lambda i: map[i] if i in map else i)\nX_transformed = mapping_function(X)\nreturn X_transformed\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/genetic_encoders/#tpot2.builtin_modules.genetic_encoders.UnderDominanceEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Do nothing and return the estimator unchanged. Dummy function to fit in with the sklearn API and hence work in pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> required Source code in <code>tpot2/builtin_modules/genetic_encoders.py</code> <pre><code>def fit(self, X, y=None):\n\"\"\"Do nothing and return the estimator unchanged.\n    Dummy function to fit in with the sklearn API and hence work in pipelines.\n    Parameters\n    ----------\n    X : array-like\n    \"\"\"\nreturn self\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/genetic_encoders/#tpot2.builtin_modules.genetic_encoders.UnderDominanceEncoder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the data by applying the Heterosis encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>New data, where n_samples is the number of samples (number of individuals) and n_components is the number of components (number of features).</p> required <code>y</code> <code>None</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>numpy ndarray, {n_samples, n_components}</code> <p>The encoded feature set</p> Source code in <code>tpot2/builtin_modules/genetic_encoders.py</code> <pre><code>def transform(self, X, y=None):\n\"\"\"Transform the data by applying the Heterosis encoding.\n    Parameters\n    ----------\n    X : numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples (number of individuals)\n        and n_components is the number of components (number of features).\n    y : None\n        Unused\n    Returns\n    -------\n    X_transformed: numpy ndarray, {n_samples, n_components}\n        The encoded feature set\n    \"\"\"\nX = check_array(X)\nmap = {0: 2, 1: 0, 2: 1}\nmapping_function = np.vectorize(lambda i: map[i] if i in map else i)\nX_transformed = mapping_function(X)\nreturn X_transformed\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/imputer/","title":"Imputer","text":"<p>Copyright (c) 2015 The auto-sklearn developers. All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <p>a. Redistributions of source code must retain the above copyright notice,      this list of conditions and the following disclaimer.   b. Redistributions in binary form must reproduce the above copyright      notice, this list of conditions and the following disclaimer in the      documentation and/or other materials provided with the distribution.   c. Neither the name of the auto-sklearn Developers  nor the names of      its contributors may be used to endorse or promote products      derived from this software without specific prior written      permission.</p> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"documentation/tpot2/builtin_modules/imputer/#tpot2.builtin_modules.imputer.ColumnSimpleImputer","title":"<code>ColumnSimpleImputer</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>tpot2/builtin_modules/imputer.py</code> <pre><code>class ColumnSimpleImputer(BaseEstimator, TransformerMixin):\ndef __init__(self,  columns=\"all\",         \nmissing_values=np.nan,\nstrategy=\"mean\",\nfill_value=None,\ncopy=True,\nadd_indicator=False,\nkeep_empty_features=False,):\nself.columns = columns\nself.missing_values = missing_values\nself.strategy = strategy\nself.fill_value = fill_value\nself.copy = copy\nself.add_indicator = add_indicator\nself.keep_empty_features = keep_empty_features\ndef fit(self, X, y=None):\n\"\"\"Fit OneHotEncoder to X, then transform X.\n        Equivalent to self.fit(X).transform(X), but more convenient and more\n        efficient. See fit for the parameters, transform for the return value.\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Dense array or sparse matrix.\n        y: array-like {n_samples,} (Optional, ignored)\n            Feature labels\n        \"\"\"\nif (self.columns == \"categorical\" or self.columns == \"numeric\") and not isinstance(X, pd.DataFrame):\nraise ValueError(f\"Invalid value for columns: {self.columns}. \"\n\"Only 'all' or &lt;list&gt; is supported for np arrays\")\nif self.columns == \"categorical\":\nself.columns_ = list(X.select_dtypes(exclude='number').columns)\nelif self.columns == \"numeric\":\nself.columns_ =  [col for col in X.columns if is_numeric_dtype(X[col])]\nelif self.columns == \"all\":\nif isinstance(X, pd.DataFrame):\nself.columns_ = X.columns\nelse:\nself.columns_ = list(range(X.shape[1]))\nelif isinstance(self.columns, list):\nself.columns_ = self.columns\nelse:\nraise ValueError(f\"Invalid value for columns: {self.columns}\")\nif len(self.columns_) == 0:\nreturn self\nself.imputer = sklearn.impute.SimpleImputer(missing_values=self.missing_values,\nstrategy=self.strategy,\nfill_value=self.fill_value,\ncopy=self.copy,\nadd_indicator=self.add_indicator,\nkeep_empty_features=self.keep_empty_features)\nif isinstance(X, pd.DataFrame):\nself.imputer.set_output(transform=\"pandas\")\nif isinstance(X, pd.DataFrame):\nself.imputer.fit(X[self.columns_], y)\nelse:\nself.imputer.fit(X[:, self.columns_], y)\nreturn self\ndef transform(self, X):\n\"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Dense array or sparse matrix.\n        Returns\n        -------\n        X_out : sparse matrix if sparse=True else a 2-d array, dtype=int\n            Transformed input.\n        \"\"\"\nif len(self.columns_) == 0:\nreturn X\nif isinstance(X, pd.DataFrame):\nX = X.copy()\nX[self.columns_] = self.imputer.transform(X[self.columns_])\nreturn X\nelse:\nX = np.copy(X)\nX[:, self.columns_] = self.imputer.transform(X[:, self.columns_])\nreturn X\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/imputer/#tpot2.builtin_modules.imputer.ColumnSimpleImputer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit OneHotEncoder to X, then transform X.</p> <p>Equivalent to self.fit(X).transform(X), but more convenient and more efficient. See fit for the parameters, transform for the return value.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix, shape=(n_samples, n_features)</code> <p>Dense array or sparse matrix.</p> required <code>y</code> <p>Feature labels</p> <code>None</code> Source code in <code>tpot2/builtin_modules/imputer.py</code> <pre><code>def fit(self, X, y=None):\n\"\"\"Fit OneHotEncoder to X, then transform X.\n    Equivalent to self.fit(X).transform(X), but more convenient and more\n    efficient. See fit for the parameters, transform for the return value.\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Dense array or sparse matrix.\n    y: array-like {n_samples,} (Optional, ignored)\n        Feature labels\n    \"\"\"\nif (self.columns == \"categorical\" or self.columns == \"numeric\") and not isinstance(X, pd.DataFrame):\nraise ValueError(f\"Invalid value for columns: {self.columns}. \"\n\"Only 'all' or &lt;list&gt; is supported for np arrays\")\nif self.columns == \"categorical\":\nself.columns_ = list(X.select_dtypes(exclude='number').columns)\nelif self.columns == \"numeric\":\nself.columns_ =  [col for col in X.columns if is_numeric_dtype(X[col])]\nelif self.columns == \"all\":\nif isinstance(X, pd.DataFrame):\nself.columns_ = X.columns\nelse:\nself.columns_ = list(range(X.shape[1]))\nelif isinstance(self.columns, list):\nself.columns_ = self.columns\nelse:\nraise ValueError(f\"Invalid value for columns: {self.columns}\")\nif len(self.columns_) == 0:\nreturn self\nself.imputer = sklearn.impute.SimpleImputer(missing_values=self.missing_values,\nstrategy=self.strategy,\nfill_value=self.fill_value,\ncopy=self.copy,\nadd_indicator=self.add_indicator,\nkeep_empty_features=self.keep_empty_features)\nif isinstance(X, pd.DataFrame):\nself.imputer.set_output(transform=\"pandas\")\nif isinstance(X, pd.DataFrame):\nself.imputer.fit(X[self.columns_], y)\nelse:\nself.imputer.fit(X[:, self.columns_], y)\nreturn self\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/imputer/#tpot2.builtin_modules.imputer.ColumnSimpleImputer.transform","title":"<code>transform(X)</code>","text":"<p>Transform X using one-hot encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix, shape=(n_samples, n_features)</code> <p>Dense array or sparse matrix.</p> required <p>Returns:</p> Name Type Description <code>X_out</code> <code>sparse matrix if sparse=True else a 2-d array, dtype=int</code> <p>Transformed input.</p> Source code in <code>tpot2/builtin_modules/imputer.py</code> <pre><code>def transform(self, X):\n\"\"\"Transform X using one-hot encoding.\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Dense array or sparse matrix.\n    Returns\n    -------\n    X_out : sparse matrix if sparse=True else a 2-d array, dtype=int\n        Transformed input.\n    \"\"\"\nif len(self.columns_) == 0:\nreturn X\nif isinstance(X, pd.DataFrame):\nX = X.copy()\nX[self.columns_] = self.imputer.transform(X[self.columns_])\nreturn X\nelse:\nX = np.copy(X)\nX[:, self.columns_] = self.imputer.transform(X[:, self.columns_])\nreturn X\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/one_hot_encoder/","title":"One hot encoder","text":"<p>Copyright (c) 2015 The auto-sklearn developers. All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <p>a. Redistributions of source code must retain the above copyright notice,      this list of conditions and the following disclaimer.   b. Redistributions in binary form must reproduce the above copyright      notice, this list of conditions and the following disclaimer in the      documentation and/or other materials provided with the distribution.   c. Neither the name of the auto-sklearn Developers  nor the names of      its contributors may be used to endorse or promote products      derived from this software without specific prior written      permission.</p> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"documentation/tpot2/builtin_modules/one_hot_encoder/#tpot2.builtin_modules.one_hot_encoder.OneHotEncoder","title":"<code>OneHotEncoder</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Encode categorical integer features using a one-hot aka one-of-K scheme.</p> <p>The input to this transformer should be a matrix of integers, denoting the values taken on by categorical (discrete) features. The output will be a sparse matrix were each column corresponds to one possible value of one feature. It is assumed that input features take on values in the range [0, n_values).</p> <p>This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.</p> <p>Parameters:</p> Name Type Description Default <code>categorical_features</code> <p>Specify what features are treated as categorical.</p> <ul> <li>'all': All features are treated as categorical.</li> <li>'auto' (default): Select only features that have less than 10 unique values.</li> <li>array of indices: Array of categorical feature indices.</li> <li>mask: Array of length n_features and with dtype=bool.</li> </ul> <p>Non-categorical features are always stacked to the right of the matrix.</p> <code>'auto'</code> <p>dtype : number type, default=np.float     Desired dtype of output.</p> <p>sparse : boolean, default=True     Will return sparse matrix if set True else will return an array.</p> <p>threshold : int, default=10     Maximum number of unique values per feature to consider the feature     to be categorical when categorical_features is 'auto'.</p> <p>minimum_fraction : float, default=None     Minimum fraction of unique values in a feature to consider the feature     to be categorical.</p> <p>Attributes:</p> Name Type Description <code>`active_features_`</code> <code>array</code> <p>Indices for active features, meaning values that actually occur in the training set. Only available when n_values is <code>'auto'</code>.</p> <p><code>feature_indices_</code> : array of shape (n_features,)     Indices to feature ranges.     Feature <code>i</code> in the original data is mapped to features     from <code>feature_indices_[i]</code> to <code>feature_indices_[i+1]</code>     (and then potentially masked by <code>active_features_</code> afterwards)</p> <p><code>n_values_</code> : array of shape (n_features,)     Maximum number of values per feature.</p> <p>Examples:</p> <p>Given a dataset with three features and two samples, we let the encoder find the maximum value per feature and transform the data to a binary one-hot encoding.</p> <pre><code>&gt;&gt;&gt; from sklearn.preprocessing import OneHotEncoder\n&gt;&gt;&gt; enc = OneHotEncoder()\n&gt;&gt;&gt; enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\nOneHotEncoder(categorical_features='all', dtype=&lt;... 'float'&gt;,\n       sparse=True, minimum_fraction=None)\n&gt;&gt;&gt; enc.n_values_\narray([2, 3, 4])\n&gt;&gt;&gt; enc.feature_indices_\narray([0, 2, 5, 9])\n&gt;&gt;&gt; enc.transform([[0, 1, 1]]).toarray()\narray([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.]])\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/one_hot_encoder/#tpot2.builtin_modules.one_hot_encoder.OneHotEncoder--see-also","title":"See also","text":"<p>sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of   dictionary items (also handles string-valued features). sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot   encoding of dictionary items or strings.</p> Source code in <code>tpot2/builtin_modules/one_hot_encoder.py</code> <pre><code>class OneHotEncoder(BaseEstimator, TransformerMixin):\n\"\"\"Encode categorical integer features using a one-hot aka one-of-K scheme.\n    The input to this transformer should be a matrix of integers, denoting\n    the values taken on by categorical (discrete) features. The output will be\n    a sparse matrix were each column corresponds to one possible value of one\n    feature. It is assumed that input features take on values in the range\n    [0, n_values).\n    This encoding is needed for feeding categorical data to many scikit-learn\n    estimators, notably linear models and SVMs with the standard kernels.\n    Parameters\n    ----------\n    categorical_features: \"all\" or array of indices or mask\n        Specify what features are treated as categorical.\n        - 'all': All features are treated as categorical.\n        - 'auto' (default): Select only features that have less than 10 unique values.\n        - array of indices: Array of categorical feature indices.\n        - mask: Array of length n_features and with dtype=bool.\n        Non-categorical features are always stacked to the right of the matrix.\n    dtype : number type, default=np.float\n        Desired dtype of output.\n    sparse : boolean, default=True\n        Will return sparse matrix if set True else will return an array.\n    threshold : int, default=10\n        Maximum number of unique values per feature to consider the feature\n        to be categorical when categorical_features is 'auto'.\n    minimum_fraction : float, default=None\n        Minimum fraction of unique values in a feature to consider the feature\n        to be categorical.\n    Attributes\n    ----------\n    `active_features_` : array\n        Indices for active features, meaning values that actually occur\n        in the training set. Only available when n_values is ``'auto'``.\n    `feature_indices_` : array of shape (n_features,)\n        Indices to feature ranges.\n        Feature ``i`` in the original data is mapped to features\n        from ``feature_indices_[i]`` to ``feature_indices_[i+1]``\n        (and then potentially masked by `active_features_` afterwards)\n    `n_values_` : array of shape (n_features,)\n        Maximum number of values per feature.\n    Examples\n    --------\n    Given a dataset with three features and two samples, we let the encoder\n    find the maximum value per feature and transform the data to a binary\n    one-hot encoding.\n    &gt;&gt;&gt; from sklearn.preprocessing import OneHotEncoder\n    &gt;&gt;&gt; enc = OneHotEncoder()\n    &gt;&gt;&gt; enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])  # doctest: +ELLIPSIS\n    OneHotEncoder(categorical_features='all', dtype=&lt;... 'float'&gt;,\n           sparse=True, minimum_fraction=None)\n    &gt;&gt;&gt; enc.n_values_\n    array([2, 3, 4])\n    &gt;&gt;&gt; enc.feature_indices_\n    array([0, 2, 5, 9])\n    &gt;&gt;&gt; enc.transform([[0, 1, 1]]).toarray()\n    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.]])\n    See also\n    --------\n    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n      dictionary items (also handles string-valued features).\n    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n      encoding of dictionary items or strings.\n    \"\"\"\ndef __init__(self, categorical_features='auto', dtype=np.float64,\nsparse=True, minimum_fraction=None, threshold=10):\nself.categorical_features = categorical_features\nself.dtype = dtype\nself.sparse = sparse\nself.minimum_fraction = minimum_fraction\nself.threshold = threshold\ndef fit(self, X, y=None):\n\"\"\"Fit OneHotEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape=(n_samples, n_feature)\n            Input array of type int.\n        Returns\n        -------\n        self\n        \"\"\"\nself.fit_transform(X)\nreturn self\ndef _matrix_adjust(self, X):\n\"\"\"Adjust all values in X to encode for NaNs and infinities in the data.\n        Parameters\n        ----------\n        X : array-like, shape=(n_samples, n_feature)\n            Input array of type int.\n        Returns\n        -------\n        X : array-like, shape=(n_samples, n_feature)\n            Input array without any NaNs or infinities.\n        \"\"\"\ndata_matrix = X.data if sparse.issparse(X) else X\n# Shift all values to specially encode for NAN/infinity/OTHER and 0\n#   Old value       New Value\n#   ---------       ---------\n#   N (0..int_max)  N + 3\n#   np.NaN          2\n#   infinity        2\n#   *other*         1\n#\n# A value of 0 is reserved, as that is specially handled in sparse\n# matrices.\ndata_matrix += len(SPARSE_ENCODINGS) + 1\ndata_matrix[~np.isfinite(data_matrix)] = SPARSE_ENCODINGS['NAN']\nreturn X\ndef _fit_transform(self, X):\n\"\"\"Assume X contains only categorical features.\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Dense array or sparse matrix.\n        \"\"\"\nX = self._matrix_adjust(X)\nX = check_array(\nX,\naccept_sparse='csc',\nforce_all_finite=False,\ndtype=int\n)\nif X.min() &lt; 0:\nraise ValueError(\"X needs to contain only non-negative integers.\")\nn_samples, n_features = X.shape\n# Remember which values should not be replaced by the value 'other'\nif self.minimum_fraction is not None:\ndo_not_replace_by_other = []\nfor column in range(X.shape[1]):\ndo_not_replace_by_other.append(list())\nif sparse.issparse(X):\nindptr_start = X.indptr[column]\nindptr_end = X.indptr[column + 1]\nunique = np.unique(X.data[indptr_start:indptr_end])\ncolsize = indptr_end - indptr_start\nelse:\nunique = np.unique(X[:, column])\ncolsize = X.shape[0]\nfor unique_value in unique:\nif np.isfinite(unique_value):\nif sparse.issparse(X):\nindptr_start = X.indptr[column]\nindptr_end = X.indptr[column + 1]\ncount = np.nansum(unique_value ==\nX.data[indptr_start:indptr_end])\nelse:\ncount = np.nansum(unique_value == X[:, column])\nelse:\nif sparse.issparse(X):\nindptr_start = X.indptr[column]\nindptr_end = X.indptr[column + 1]\ncount = np.nansum(~np.isfinite(\nX.data[indptr_start:indptr_end]))\nelse:\ncount = np.nansum(~np.isfinite(X[:, column]))\nfraction = float(count) / colsize\nif fraction &gt;= self.minimum_fraction:\ndo_not_replace_by_other[-1].append(unique_value)\nfor unique_value in unique:\nif unique_value not in do_not_replace_by_other[-1]:\nif sparse.issparse(X):\nindptr_start = X.indptr[column]\nindptr_end = X.indptr[column + 1]\nX.data[indptr_start:indptr_end][\nX.data[indptr_start:indptr_end] ==\nunique_value] = SPARSE_ENCODINGS['OTHER']\nelse:\nX[:, column][X[:, column] == unique_value] = SPARSE_ENCODINGS['OTHER']\nself.do_not_replace_by_other_ = do_not_replace_by_other\nif sparse.issparse(X):\nn_values = X.max(axis=0).toarray().flatten() + len(SPARSE_ENCODINGS)\nelse:\nn_values = np.max(X, axis=0) + len(SPARSE_ENCODINGS)\nself.n_values_ = n_values\nn_values = np.hstack([[0], n_values])\nindices = np.cumsum(n_values)\nself.feature_indices_ = indices\nif sparse.issparse(X):\nrow_indices = X.indices\ncolumn_indices = []\nfor i in range(len(X.indptr) - 1):\nnbr = X.indptr[i+1] - X.indptr[i]\ncolumn_indices_ = [indices[i]] * nbr\ncolumn_indices_ += X.data[X.indptr[i]:X.indptr[i+1]]\ncolumn_indices.extend(column_indices_)\ndata = np.ones(X.data.size)\nelse:\ncolumn_indices = (X + indices[:-1]).ravel()\nrow_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\nn_features)\ndata = np.ones(n_samples * n_features)\nout = sparse.coo_matrix((data, (row_indices, column_indices)),\nshape=(n_samples, indices[-1]),\ndtype=self.dtype).tocsc()\nmask = np.array(out.sum(axis=0)).ravel() != 0\nactive_features = np.where(mask)[0]\nout = out[:, active_features]\nself.active_features_ = active_features\nreturn out.tocsr() if self.sparse else out.toarray()\ndef fit_transform(self, X, y=None):\n\"\"\"Fit OneHotEncoder to X, then transform X.\n        Equivalent to self.fit(X).transform(X), but more convenient and more\n        efficient. See fit for the parameters, transform for the return value.\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Dense array or sparse matrix.\n        y: array-like {n_samples,} (Optional, ignored)\n            Feature labels\n        \"\"\"\nif self.categorical_features == \"auto\":\nself.categorical_features_ = auto_select_categorical_features(X, threshold=self.threshold)\nelse:\nself.categorical_features_ = self.categorical_features\nreturn _transform_selected(\nX,\nself._fit_transform,\nself.categorical_features_,\ncopy=True\n)\ndef _transform(self, X):\n\"\"\"Asssume X contains only categorical features.\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Dense array or sparse matrix.\n        \"\"\"\nX = self._matrix_adjust(X)\nX = check_array(X, accept_sparse='csc', force_all_finite=False,\ndtype=int)\nif X.min() &lt; 0:\nraise ValueError(\"X needs to contain only non-negative integers.\")\nn_samples, n_features = X.shape\nindices = self.feature_indices_\nif n_features != indices.shape[0] - 1:\nraise ValueError(\"X has different shape than during fitting.\"\n\" Expected %d, got %d.\"\n% (indices.shape[0] - 1, n_features))\n# Replace all indicators which were below `minimum_fraction` in the\n# training set by 'other'\nif self.minimum_fraction is not None:\nfor column in range(X.shape[1]):\nif sparse.issparse(X):\nindptr_start = X.indptr[column]\nindptr_end = X.indptr[column + 1]\nunique = np.unique(X.data[indptr_start:indptr_end])\nelse:\nunique = np.unique(X[:, column])\nfor unique_value in unique:\nif unique_value not in self.do_not_replace_by_other_[column]:\nif sparse.issparse(X):\nindptr_start = X.indptr[column]\nindptr_end = X.indptr[column + 1]\nX.data[indptr_start:indptr_end][\nX.data[indptr_start:indptr_end] ==\nunique_value] = SPARSE_ENCODINGS['OTHER']\nelse:\nX[:, column][X[:, column] == unique_value] = SPARSE_ENCODINGS['OTHER']\nif sparse.issparse(X):\nn_values_check = X.max(axis=0).toarray().flatten() + 1\nelse:\nn_values_check = np.max(X, axis=0) + 1\n# Replace all indicators which are out of bounds by 'other' (index 0)\nif (n_values_check &gt; self.n_values_).any():\n# raise ValueError(\"Feature out of bounds. Try setting n_values.\")\nfor i, n_value_check in enumerate(n_values_check):\nif (n_value_check - 1) &gt;= self.n_values_[i]:\nif sparse.issparse(X):\nindptr_start = X.indptr[i]\nindptr_end = X.indptr[i+1]\nX.data[indptr_start:indptr_end][X.data[indptr_start:indptr_end] &gt;= self.n_values_[i]] = 0\nelse:\nX[:, i][X[:, i] &gt;= self.n_values_[i]] = 0\nif sparse.issparse(X):\nrow_indices = X.indices\ncolumn_indices = []\nfor i in range(len(X.indptr) - 1):\nnbr = X.indptr[i + 1] - X.indptr[i]\ncolumn_indices_ = [indices[i]] * nbr\ncolumn_indices_ += X.data[X.indptr[i]:X.indptr[i + 1]]\ncolumn_indices.extend(column_indices_)\ndata = np.ones(X.data.size)\nelse:\ncolumn_indices = (X + indices[:-1]).ravel()\nrow_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\nn_features)\ndata = np.ones(n_samples * n_features)\nout = sparse.coo_matrix((data, (row_indices, column_indices)),\nshape=(n_samples, indices[-1]),\ndtype=self.dtype).tocsc()\nout = out[:, self.active_features_]\nreturn out.tocsr() if self.sparse else out.toarray()\ndef transform(self, X):\n\"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Dense array or sparse matrix.\n        Returns\n        -------\n        X_out : sparse matrix if sparse=True else a 2-d array, dtype=int\n            Transformed input.\n        \"\"\"\nreturn _transform_selected(\nX, self._transform,\nself.categorical_features_,\ncopy=True\n)\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/one_hot_encoder/#tpot2.builtin_modules.one_hot_encoder.OneHotEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit OneHotEncoder to X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like, shape=(n_samples, n_feature)</code> <p>Input array of type int.</p> required <p>Returns:</p> Type Description <code>self</code> Source code in <code>tpot2/builtin_modules/one_hot_encoder.py</code> <pre><code>def fit(self, X, y=None):\n\"\"\"Fit OneHotEncoder to X.\n    Parameters\n    ----------\n    X : array-like, shape=(n_samples, n_feature)\n        Input array of type int.\n    Returns\n    -------\n    self\n    \"\"\"\nself.fit_transform(X)\nreturn self\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/one_hot_encoder/#tpot2.builtin_modules.one_hot_encoder.OneHotEncoder.fit_transform","title":"<code>fit_transform(X, y=None)</code>","text":"<p>Fit OneHotEncoder to X, then transform X.</p> <p>Equivalent to self.fit(X).transform(X), but more convenient and more efficient. See fit for the parameters, transform for the return value.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix, shape=(n_samples, n_features)</code> <p>Dense array or sparse matrix.</p> required <code>y</code> <p>Feature labels</p> <code>None</code> Source code in <code>tpot2/builtin_modules/one_hot_encoder.py</code> <pre><code>def fit_transform(self, X, y=None):\n\"\"\"Fit OneHotEncoder to X, then transform X.\n    Equivalent to self.fit(X).transform(X), but more convenient and more\n    efficient. See fit for the parameters, transform for the return value.\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Dense array or sparse matrix.\n    y: array-like {n_samples,} (Optional, ignored)\n        Feature labels\n    \"\"\"\nif self.categorical_features == \"auto\":\nself.categorical_features_ = auto_select_categorical_features(X, threshold=self.threshold)\nelse:\nself.categorical_features_ = self.categorical_features\nreturn _transform_selected(\nX,\nself._fit_transform,\nself.categorical_features_,\ncopy=True\n)\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/one_hot_encoder/#tpot2.builtin_modules.one_hot_encoder.OneHotEncoder.transform","title":"<code>transform(X)</code>","text":"<p>Transform X using one-hot encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix, shape=(n_samples, n_features)</code> <p>Dense array or sparse matrix.</p> required <p>Returns:</p> Name Type Description <code>X_out</code> <code>sparse matrix if sparse=True else a 2-d array, dtype=int</code> <p>Transformed input.</p> Source code in <code>tpot2/builtin_modules/one_hot_encoder.py</code> <pre><code>def transform(self, X):\n\"\"\"Transform X using one-hot encoding.\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Dense array or sparse matrix.\n    Returns\n    -------\n    X_out : sparse matrix if sparse=True else a 2-d array, dtype=int\n        Transformed input.\n    \"\"\"\nreturn _transform_selected(\nX, self._transform,\nself.categorical_features_,\ncopy=True\n)\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/one_hot_encoder/#tpot2.builtin_modules.one_hot_encoder.auto_select_categorical_features","title":"<code>auto_select_categorical_features(X, threshold=10)</code>","text":"<p>Make a feature mask of categorical features in X.</p> <p>Features with less than 10 unique values are considered categorical.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or sparse matrix, shape=(n_samples, n_features)</code> <p>Dense array or sparse matrix.</p> required <p>threshold : int     Maximum number of unique values per feature to consider the feature     to be categorical.</p> <p>Returns:</p> Name Type Description <code>feature_mask</code> <code>array of booleans of size {n_features, }</code> Source code in <code>tpot2/builtin_modules/one_hot_encoder.py</code> <pre><code>def auto_select_categorical_features(X, threshold=10):\n\"\"\"Make a feature mask of categorical features in X.\n    Features with less than 10 unique values are considered categorical.\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Dense array or sparse matrix.\n    threshold : int\n        Maximum number of unique values per feature to consider the feature\n        to be categorical.\n    Returns\n    -------\n    feature_mask : array of booleans of size {n_features, }\n    \"\"\"\nfeature_mask = []\nfor column in range(X.shape[1]):\nif sparse.issparse(X):\nindptr_start = X.indptr[column]\nindptr_end = X.indptr[column + 1]\nunique = np.unique(X.data[indptr_start:indptr_end])\nelse:\nunique = np.unique(X[:, column])\nfeature_mask.append(len(unique) &lt;= threshold)\nreturn feature_mask\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/passthrough/","title":"Passthrough","text":""},{"location":"documentation/tpot2/builtin_modules/selector_wrappers/","title":"Selector wrappers","text":""},{"location":"documentation/tpot2/builtin_modules/zero_count/","title":"Zero count","text":"<p>This file is part of the TPOT library.</p> <p>TPOT was primarily developed at the University of Pennsylvania by:     - Randal S. Olson (rso@randalolson.com)     - Weixuan Fu (weixuanf@upenn.edu)     - Daniel Angell (dpa34@drexel.edu)     - and many more generous open source contributors</p> <p>TPOT is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p> <p>TPOT is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</p> <p>You should have received a copy of the GNU Lesser General Public License along with TPOT. If not, see http://www.gnu.org/licenses/.</p>"},{"location":"documentation/tpot2/builtin_modules/zero_count/#tpot2.builtin_modules.zero_count.ZeroCount","title":"<code>ZeroCount</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Adds the count of zeros and count of non-zeros per sample as features.</p> Source code in <code>tpot2/builtin_modules/zero_count.py</code> <pre><code>class ZeroCount(BaseEstimator, TransformerMixin):\n\"\"\"Adds the count of zeros and count of non-zeros per sample as features.\"\"\"\ndef fit(self, X, y=None):\n\"\"\"Dummy function to fit in with the sklearn API.\"\"\"\nreturn self\ndef transform(self, X, y=None):\n\"\"\"Transform data by adding two virtual features.\n        Parameters\n        ----------\n        X: numpy ndarray, {n_samples, n_components}\n            New data, where n_samples is the number of samples and n_components\n            is the number of components.\n        y: None\n            Unused\n        Returns\n        -------\n        X_transformed: array-like, shape (n_samples, n_features)\n            The transformed feature set\n        \"\"\"\nX = check_array(X)\nn_features = X.shape[1]\nX_transformed = np.copy(X)\nnon_zero_vector = np.count_nonzero(X_transformed, axis=1)\nnon_zero = np.reshape(non_zero_vector, (-1, 1))\nzero_col = np.reshape(n_features - non_zero_vector, (-1, 1))\nX_transformed = np.hstack((non_zero, X_transformed))\nX_transformed = np.hstack((zero_col, X_transformed))\nreturn X_transformed\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/zero_count/#tpot2.builtin_modules.zero_count.ZeroCount.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Dummy function to fit in with the sklearn API.</p> Source code in <code>tpot2/builtin_modules/zero_count.py</code> <pre><code>def fit(self, X, y=None):\n\"\"\"Dummy function to fit in with the sklearn API.\"\"\"\nreturn self\n</code></pre>"},{"location":"documentation/tpot2/builtin_modules/zero_count/#tpot2.builtin_modules.zero_count.ZeroCount.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform data by adding two virtual features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>New data, where n_samples is the number of samples and n_components is the number of components.</p> required <code>y</code> <p>Unused</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_transformed</code> <code>(array - like, shape(n_samples, n_features))</code> <p>The transformed feature set</p> Source code in <code>tpot2/builtin_modules/zero_count.py</code> <pre><code>def transform(self, X, y=None):\n\"\"\"Transform data by adding two virtual features.\n    Parameters\n    ----------\n    X: numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples and n_components\n        is the number of components.\n    y: None\n        Unused\n    Returns\n    -------\n    X_transformed: array-like, shape (n_samples, n_features)\n        The transformed feature set\n    \"\"\"\nX = check_array(X)\nn_features = X.shape[1]\nX_transformed = np.copy(X)\nnon_zero_vector = np.count_nonzero(X_transformed, axis=1)\nnon_zero = np.reshape(non_zero_vector, (-1, 1))\nzero_col = np.reshape(n_features - non_zero_vector, (-1, 1))\nX_transformed = np.hstack((non_zero, X_transformed))\nX_transformed = np.hstack((zero_col, X_transformed))\nreturn X_transformed\n</code></pre>"},{"location":"documentation/tpot2/config/all_single_modules/","title":"All single modules","text":""},{"location":"documentation/tpot2/config/autoqtl_builtins/","title":"Autoqtl builtins","text":""},{"location":"documentation/tpot2/config/classifiers/","title":"Classifiers","text":""},{"location":"documentation/tpot2/config/hyperparametersuggestor/","title":"Hyperparametersuggestor","text":""},{"location":"documentation/tpot2/config/mdr_configs/","title":"Mdr configs","text":""},{"location":"documentation/tpot2/config/regressors/","title":"Regressors","text":""},{"location":"documentation/tpot2/config/selectors/","title":"Selectors","text":""},{"location":"documentation/tpot2/config/special_configs/","title":"Special configs","text":""},{"location":"documentation/tpot2/config/special_configs/#tpot2.config.special_configs.make_FSS_config_dictionary","title":"<code>make_FSS_config_dictionary(subsets=None, n_features=None, feature_names=None)</code>","text":"<p>Create the search space of parameters for FeatureSetSelector.</p> <p>Parameters:</p> Name Type Description Default <code>subsets</code> <ul> <li>str : If a string, it is assumed to be a path to a csv file with the subsets.      The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.</li> <li>list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets.</li> </ul> <code>None</code> <p>n_features: int the number of features in the dataset.     If subsets is None, each column will be treated as a subset. One column will be selected per subset.</p> Source code in <code>tpot2/config/special_configs.py</code> <pre><code>def make_FSS_config_dictionary(subsets=None, n_features=None, feature_names=None):\n\"\"\"Create the search space of parameters for FeatureSetSelector.\n    Parameters\n    ----------\n    subsets: Sets the subsets to select from.\n        - str : If a string, it is assumed to be a path to a csv file with the subsets. \n            The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n        - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets.\n    n_features: int the number of features in the dataset.\n        If subsets is None, each column will be treated as a subset. One column will be selected per subset.\n    \"\"\"\n#require at least of of the parameters\nif subsets is None and n_features is None:\nraise ValueError('At least one of the parameters must be provided')\nif isinstance(subsets, str):\ndf = pd.read_csv(subsets,header=None,index_col=0)\ndf.set_index(0,inplace=True)\ndf['features'] = df.apply(lambda x: list([x[c] for c in df.columns]),axis=1) \nsubset_dict = {}\nfor row in df.index:\nsubset_dict[row] = df.loc[row]['features']\nelif isinstance(subsets, dict):\nsubset_dict = subsets\nelif isinstance(subsets, list) or isinstance(subsets, np.ndarray):\nsubset_dict = {str(i):subsets[i] for i in range(len(subsets))}\nelse:\nif feature_names is None:\nsubset_dict = {str(i):i for i in range(n_features)}\nelse:\nsubset_dict = {str(i):feature_names[i] for i in range(len(feature_names))}\nnames_list = list(subset_dict.keys())\nreturn {FeatureSetSelector: partial(params_feature_set_selector, names_list = names_list, subset_dict=subset_dict)}\n</code></pre>"},{"location":"documentation/tpot2/config/special_configs/#tpot2.config.special_configs.params_feature_set_selector","title":"<code>params_feature_set_selector(trial, name=None, names_list=None, subset_dict=None)</code>","text":"<p>Create a dictionary of parameters for FeatureSetSelector.</p> <p>Parameters:</p> Name Type Description Default <code>trial</code> <p>A trial corresponds to the evaluation of a objective function.</p> required <code>name</code> <p>Used for compatibility in when calling multiple optuna of multiple parameters at once.</p> <code>None</code> <code>names_list</code> <p>List of names of the feature set selector. To more easily keep track of what the subsets represent. Included to prevent repeat calls to list(subset_dict.keys()) which may be slow and/or have different orderings</p> <code>None</code> <code>subset_dict</code> <p>A dictionary of subsets. The keys are the names of the subsets and the values are the subsets.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>params</code> <code>dictionary</code> <p>A dictionary of parameters for FeatureSetSelector.</p> Source code in <code>tpot2/config/special_configs.py</code> <pre><code>def params_feature_set_selector(trial, name=None, names_list = None, subset_dict=None):\n\"\"\"Create a dictionary of parameters for FeatureSetSelector.\n    Parameters\n    ----------\n    trial: optuna.trial.Trial\n        A trial corresponds to the evaluation of a objective function.\n    name: string\n        Used for compatibility in when calling multiple optuna of multiple parameters at once.\n    names_list: list of string\n        List of names of the feature set selector. To more easily keep track of what the subsets represent.\n        Included to prevent repeat calls to list(subset_dict.keys()) which may be slow and/or have different orderings\n    subset_dict: dictionary\n        A dictionary of subsets. The keys are the names of the subsets and the values are the subsets.\n    Returns\n    -------\n    params: dictionary\n        A dictionary of parameters for FeatureSetSelector.\n    \"\"\"\nsubset_name = trial.suggest_categorical(f'subset_name_{name}', names_list)\nparams =    {'name': subset_name,\n'sel_subset': subset_dict[subset_name],\n}\nreturn params\n</code></pre>"},{"location":"documentation/tpot2/config/transformers/","title":"Transformers","text":""},{"location":"documentation/tpot2/evolvers/base_evolver/","title":"Base evolver","text":""},{"location":"documentation/tpot2/evolvers/base_evolver/#tpot2.evolvers.base_evolver.BaseEvolver","title":"<code>BaseEvolver</code>","text":"Source code in <code>tpot2/evolvers/base_evolver.py</code> <pre><code>class BaseEvolver():\ndef __init__(   self, \nindividual_generator ,\nobjective_functions,\nobjective_function_weights,\nobjective_names = None,\nobjective_kwargs = None,\nbigger_is_better = True,\npopulation_size = 50,\ninitial_population_size = None,\npopulation_scaling = .5, \ngenerations_until_end_population = 1, \ngenerations = 50, \nearly_stop = None,\nearly_stop_tol = 0.001,\nmax_time_seconds=float(\"inf\"), \nmax_eval_time_seconds=60*5,\nn_jobs=1,\nmemory_limit=\"4GB\",\nclient=None,\nsurvival_percentage = 1,\ncrossover_probability=.2,\nmutate_probability=.7,\nmutate_then_crossover_probability=.05,\ncrossover_then_mutate_probability=.05,\nn_parents=2,\nsurvival_selector = survival_select_NSGA2,\nparent_selector = tournament_selection_dominated,\nbudget_range = None, \nbudget_scaling = .5, \ngenerations_until_end_budget = 1,                    \nstepwise_steps = 5,\nthreshold_evaluation_early_stop = None, \nthreshold_evaluation_scaling = .5,\nmin_history_threshold = 20,\nselection_evaluation_early_stop = None,\nselection_evaluation_scaling = .5,\nevaluation_early_stop_steps = None, \nfinal_score_strategy = \"mean\",\nverbose = 0, \nperiodic_checkpoint_folder = None,\ncallback = None,\n) -&gt; None:\n\"\"\"\n        Uses mutation, crossover, and optimization functions to evolve a population of individuals towards the given objective functions.\n        Parameters\n        ----------\n        individual_generator : generator\n            Generator that yields new base individuals. Used to generate initial population.\n        objective_functions : list of callables\n            list of functions that get applied to the individual and return a float or list of floats\n            If an objective function returns multiple values, they are all concatenated in order \n            with respect to objective_function_weights and early_stop_tol.\n        objective_function_weights : list of floats\n            list of weights for each objective function. Sign flips whether bigger is better or not\n        objective_names : list of strings, default=None\n            Names of the objectives. If None, objective0, objective1, etc. will be used\n        objective_kwargs : dict, default=None\n            Dictionary of keyword arguments to pass to the objective function\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n        population_size : int, default=50\n            Size of the population\n        initial_population_size : int, default=None\n            Size of the initial population. If None, population_size will be used.\n        population_scaling : int, default=0.5\n            Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        generations_until_end_population : int, default=1  \n            Number of generations until the population size reaches population_size            \n        generations : int, default=50\n            Number of generations to run\n        early_stop : int, default=None\n            Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered.\n        early_stop_tol : float, list of floats, or None, default=0.001\n            -list of floats\n                list of tolerances for each objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int \n                If an int is given, it will be used as the tolerance for all objectives\n        max_time_seconds : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n        max_eval_time_seconds : float, default=60*5\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n        memory_limit : str, default=\"4GB\"\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit. \n        survival_percentage : float, default=1\n            Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive. \n            For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.\n        crossover_probability : float, default=.2\n            Probability of generating a new individual by crossover between two individuals.\n        mutate_probability : float, default=.7\n            Probability of generating a new individual by crossover between one individuals.\n        mutate_then_crossover_probability : float, default=.05\n            Probability of generating a new individual by mutating two individuals followed by crossover.\n        crossover_then_mutate_probability : float, default=.05\n            Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n        n_parents : int, default=2\n            Number of parents to use for crossover. Must be greater than 1.\n        survival_selector : function, default=survival_select_NSGA2\n            Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n            Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n        parent_selector : function, default=parent_select_NSGA2\n            Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n        budget_range : list [start, end], default=None\n            A starting and ending budget to use for the budget scaling.\n        budget_scaling float : [0,1], default=0.5\n            A scaling factor to use when determining how fast we move the budget from the start to end budget.\n        generations_until_end_budget : int, default=1\n            The number of generations to run before reaching the max budget.\n        stepwise_steps : int, default=1\n            The number of staircase steps to take when scaling the budget and population size.\n        threshold_evaluation_early_stop : list [start, end], default=None\n            starting and ending percentile to use as a threshold for the evaluation early stopping.\n            Values between 0 and 100.\n        threshold_evaluation_scaling : float [0,inf), default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n        min_history_threshold : int, default=0\n            The minimum number of previous scores needed before using threshold early stopping.\n        selection_evaluation_early_stop : list, default=None\n            A lower and upper percent of the population size to select each round of CV.\n            Values between 0 and 1.\n        selection_evaluation_scaling : float, default=0.5 \n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n        evaluation_early_stop_steps : int, default=1\n            The number of steps that will be taken from the objective function. (e.g., the number of CV folds to evaluate)\n        final_score_strategy : str, default=\"mean\" \n            The strategy to use when determining the final score for an individual.\n            \"mean\": The mean of all objective scores\n            \"last\": The score returned by the last call. Currently each objective is evaluated with a clone of the individual.\n        verbose : int, default=0\n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n            2. evaluations progress bar\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n        callback : tpot2.CallBackInterface, default=None\n            Callback object. Not implemented\n        \"\"\"\nif threshold_evaluation_early_stop is not None or selection_evaluation_early_stop is not None:\nif evaluation_early_stop_steps is None:\nraise ValueError(\"evaluation_early_stop_steps must be set when using threshold_evaluation_early_stop or selection_evaluation_early_stop\")\nself.individual_generator = individual_generator \nself.population_size = population_size \nself.objective_functions = objective_functions \nself.objective_function_weights = np.array(objective_function_weights)\nself.bigger_is_better = bigger_is_better\nif not bigger_is_better:\nself.objective_function_weights = np.array(self.objective_function_weights)*-1\nself.initial_population_size = initial_population_size\nif self.initial_population_size is None:\nself.cur_population_size = population_size\nelse:\nself.cur_population_size = initial_population_size\nself.population_scaling = population_scaling\nself.generations_until_end_population = generations_until_end_population\nself.population_size_list = None\nself.periodic_checkpoint_folder = periodic_checkpoint_folder\nself.verbose  = verbose  \nself.callback = callback \nself.generations = generations \nself.n_jobs = n_jobs\nif max_time_seconds is None:\nself.max_time_seconds = float(\"inf\")\nelse:\nself.max_time_seconds = max_time_seconds  \n#functools requires none for infinite time, doesn't support inf\nif max_eval_time_seconds is not None and math.isinf(max_eval_time_seconds ):\nself.max_eval_time_seconds = None\nelse:\nself.max_eval_time_seconds = max_eval_time_seconds\nself.generation = 0\nself.threshold_evaluation_early_stop =threshold_evaluation_early_stop\nself.threshold_evaluation_scaling =  max(0.00001,threshold_evaluation_scaling ) \nself.min_history_threshold = min_history_threshold\nself.selection_evaluation_early_stop = selection_evaluation_early_stop\nself.selection_evaluation_scaling =  max(0.00001,selection_evaluation_scaling )\nself.evaluation_early_stop_steps = evaluation_early_stop_steps\nself.final_score_strategy = final_score_strategy\nself.budget_range = budget_range\nself.budget_scaling = budget_scaling\nself.generations_until_end_budget = generations_until_end_budget\nself.stepwise_steps = stepwise_steps\nself.memory_limit = memory_limit\nself.client = client\nself.survival_selector=survival_selector\nself.parent_selector=parent_selector\nself.survival_percentage = survival_percentage\ntotal_var_p = crossover_probability + mutate_probability + mutate_then_crossover_probability + crossover_then_mutate_probability\nself.crossover_probability = crossover_probability / total_var_p\nself.mutate_probability = mutate_probability  / total_var_p\nself.mutate_then_crossover_probability= mutate_then_crossover_probability / total_var_p\nself.crossover_then_mutate_probability= crossover_then_mutate_probability / total_var_p\nself.n_parents = n_parents\nif objective_kwargs is None:\nself.objective_kwargs = {}\nelse:\nself.objective_kwargs = objective_kwargs\n# if objective_kwargs is None:\n#     self.objective_kwargs = [{}] * len(self.objective_functions)\n# elif isinstance(objective_kwargs, dict):\n#     self.objective_kwargs = [objective_kwargs] * len(self.objective_functions)\n# else:\n#     self.objective_kwargs = objective_kwargs\n###########\nif self.initial_population_size != self.population_size:\nself.population_size_list = beta_interpolation(start=self.cur_population_size, end=self.population_size, scale=self.population_scaling, n=generations_until_end_population, n_steps=self.stepwise_steps)\nself.population_size_list = np.round(self.population_size_list).astype(int)\nif self.budget_range is None:\nself.budget_list = None\nelse:\nself.budget_list = beta_interpolation(start=self.budget_range[0], end=self.budget_range[1], n=self.generations_until_end_budget, scale=self.budget_scaling, n_steps=self.stepwise_steps)\nif objective_names is None:\nself.objective_names = [\"objective\"+str(i) for i in range(len(objective_function_weights))]\nelse:\nself.objective_names = objective_names\nif self.budget_list is not None:\nif len(self.budget_list) &lt;= self.generation:\nself.budget = self.budget_list[-1]\nelse:\nself.budget = self.budget_list[self.generation]\nelse:\nself.budget = None\nself.early_stop_tol = early_stop_tol\nself.early_stop = early_stop\nif isinstance(self.early_stop_tol, float):\nself.early_stop_tol = [self.early_stop_tol for _ in range(len(self.objective_names))]\nself.early_stop_tol = [np.inf if tol is None else tol for tol in self.early_stop_tol]\nself.population = None\nself.population_file = None\nif self.periodic_checkpoint_folder is not None:\nself.population_file = os.path.join(self.periodic_checkpoint_folder, \"population.pkl\")\nif not os.path.exists(self.periodic_checkpoint_folder):\nos.makedirs(self.periodic_checkpoint_folder)\nif os.path.exists(self.population_file):\nself.population = pickle.load(open(self.population_file, \"rb\"))\nif len(self.population.evaluated_individuals)&gt;0 and \"Generation\" in self.population.evaluated_individuals.columns:      \nself.generation = self.population.evaluated_individuals['Generation'].max() + 1 #TODO check if this is empty?\ninit_names = self.objective_names\nif self.budget_range is not None:\ninit_names = init_names + [\"Budget\"]\nif self.population is None:\nself.population = tpot2.Population(column_names=init_names)\ninitial_population = [next(self.individual_generator) for _ in range(self.cur_population_size)]\nself.population.add_to_population(initial_population)\nself.population.update_column(self.population.population, column_names=\"Generation\", data=self.generation)\ndef optimize(self, generations=None):\nif self.client is not None: #If user passed in a client manually\nself._client = self.client\nelse:\nif self.verbose &gt;= 4:\nsilence_logs = 30\nelif self.verbose &gt;=5:\nsilence_logs = 40\nelse:\nsilence_logs = 50\nself._cluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\nthreads_per_worker=1,\nsilence_logs=silence_logs,\nprocesses=True,\nmemory_limit=self.memory_limit)\nself._client = Client(self._cluster)\nif generations is None:\ngenerations = self.generations\nstart_time = time.time() \ngenerations_without_improvement = np.array([0 for _ in range(len(self.objective_function_weights))])\nbest_scores = [-np.inf for _ in range(len(self.objective_function_weights))]\nself.scheduled_timeout_time = time.time() + self.max_time_seconds\ntry: \n#for gen in tnrange(generations,desc=\"Generation\", disable=self.verbose&lt;1):\ndone = False\ngen = 0\nif self.verbose &gt;= 1:\nif generations is None or np.isinf(generations):\npbar = tqdm.tqdm(total=0)\nelse:\npbar = tqdm.tqdm(total=generations)\npbar.set_description(\"Generation\")\nwhile not done:\n# Generation 0 is the initial population\nif self.generation == 0:\nif self.population_file is not None:\npickle.dump(self.population, open(self.population_file, \"wb\"))\nself.evaluate_population()\nif self.population_file is not None:\npickle.dump(self.population, open(self.population_file, \"wb\"))\nattempts = 2\nwhile len(self.population.population) == 0 and attempts &gt; 0:\nnew_initial_population = [next(self.individual_generator) for _ in range(self.cur_population_size)]\nself.population.add_to_population(new_initial_population)\nattempts -= 1\nself.evaluate_population()\nif len(self.population.population) == 0:\nraise Exception(\"No individuals could be evaluated in the initial population\")\nself.generation += 1\n# Generation 1 is the first generation after the initial population\nelse:\nif time.time() - start_time &gt; self.max_time_seconds:\nbreak\nself.step()\nif self.verbose &gt;= 3:  \nsign = np.sign(self.objective_function_weights)\nvalid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[self.objective_names].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\ncur_best_scores = valid_df.max(axis=0)*sign\ncur_best_scores = cur_best_scores.to_numpy()\nprint(\"Generation: \", self.generation)\nfor i, obj in enumerate(self.objective_names):\nprint(f\"Best {obj} score: {cur_best_scores[i]}\")\nif self.early_stop:\nif self.budget is None or self.budget&gt;=self.budget_range[-1]: #self.budget&gt;=1:\n#get sign of objective_function_weights\nsign = np.sign(self.objective_function_weights)\n#get best score for each objective\nvalid_df = self.population.evaluated_individuals[~self.population.evaluated_individuals[self.objective_names].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names]*sign\ncur_best_scores = valid_df.max(axis=0)\ncur_best_scores = cur_best_scores.to_numpy()\n#cur_best_scores =  self.population.get_column(self.population.population, column_names=self.objective_names).max(axis=0)*sign #TODO this assumes the current population is the best\nimproved = ( np.array(cur_best_scores) - np.array(best_scores) &gt;= np.array(self.early_stop_tol) )\nnot_improved = np.logical_not(improved)\ngenerations_without_improvement = generations_without_improvement * not_improved + not_improved #set to zero if not improved, else increment\npass\n#update best score\nbest_scores = [max(best_scores[i], cur_best_scores[i]) for i in range(len(self.objective_names))]\nif all(generations_without_improvement&gt;self.early_stop):\nif self.verbose &gt;= 3:\nprint(\"Early stop\")\nbreak\n#save population\nif self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\npickle.dump(self.population, open(self.population_file, \"wb\"))\ngen += 1\nif self.verbose &gt;= 1:\npbar.update(1)\nif generations is not None and gen &gt;= generations:\ndone = True\nexcept KeyboardInterrupt:\nif self.verbose &gt;= 3:\nprint(\"KeyboardInterrupt\")\nself.population.remove_invalid_from_population(column_names=self.objective_names, invalid_value=\"INVALID\")\nself.population.remove_invalid_from_population(column_names=self.objective_names, invalid_value=\"TIMEOUT\")\nif self.population_file is not None:\npickle.dump(self.population, open(self.population_file, \"wb\"))\nif self.client is None: #If we created our own client, close it\nself._client.close()\nself._cluster.close()\ntpot2.utils.get_pareto_frontier(self.population.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights, invalid_values=[\"TIMEOUT\",\"INVALID\"])\ndef step(self,):\nif self.population_size_list is not None:\nif self.generation &lt; len(self.population_size_list):\nself.cur_population_size = self.population_size_list[self.generation]\nelse:\nself.cur_population_size = self.population_size\nif self.budget_list is not None:\nif len(self.budget_list) &lt;= self.generation:\nself.budget = self.budget_range[-1]\nelse:\nself.budget = self.budget_list[self.generation]\nelse:\nself.budget = None\nself.one_generation_step()\nself.generation += 1\ndef one_generation_step(self, ): #your EA Algorithm goes here\nif self.survival_selector is not None:\nn_survivors = max(1,int(self.cur_population_size*self.survival_percentage)) #always keep at least one individual\n#Get survivors from current population\nweighted_scores = self.population.get_column(self.population.population, column_names=self.objective_names) * self.objective_function_weights\nnew_population_index = np.ravel(self.survival_selector(weighted_scores, k=n_survivors)) #TODO make it clear that we are concatenating scores...\nself.population.set_population(np.array(self.population.population)[new_population_index])\nweighted_scores = self.population.get_column(self.population.population, column_names=self.objective_names) * self.objective_function_weights\n#number of crossover pairs and mutation only parent to generate\nn_crossover = int(self.cur_population_size*self.crossover_probability)\nn_crossover_then_mutate = int(self.cur_population_size*self.crossover_then_mutate_probability)\nn_mutate_then_crossover = int(self.cur_population_size*self.mutate_then_crossover_probability)\nn_total_crossover_pairs = n_crossover + n_crossover_then_mutate + n_mutate_then_crossover\nn_mutate_parents = self.cur_population_size - n_total_crossover_pairs\n#get crossover pairs\nif n_total_crossover_pairs &gt; 0:\ncx_parents_index = self.parent_selector(weighted_scores, k=n_total_crossover_pairs, n_parents=self.n_parents,   ) #TODO make it clear that we are concatenating scores...\ncx_var_ops = np.concatenate([ np.repeat(\"crossover\",n_crossover),\nnp.repeat(\"mutate_then_crossover\",n_mutate_then_crossover),\nnp.repeat(\"crossover_then_mutate\",n_crossover_then_mutate),\n])\nelse:\ncx_parents_index = []\ncx_var_ops = []\n#get mutation only parents\nif n_mutate_parents &gt; 0:\nm_parents_index = self.parent_selector(weighted_scores, k=n_mutate_parents, n_parents=1,  ) #TODO make it clear that we are concatenating scores...\nm_var_ops = np.repeat(\"mutate\",len(m_parents_index))\nelse:\nm_parents_index = []\nm_var_ops = []\ncx_parents = np.array(self.population.population)[cx_parents_index]\nm_parents = np.array(self.population.population)[m_parents_index]\nparents = list(cx_parents) + list(m_parents)\nvar_ops = np.concatenate([cx_var_ops, m_var_ops])\noffspring = self.population.create_offspring(parents, var_ops, n_jobs=1) \nself.population.update_column(offspring, column_names=\"Generation\", data=self.generation, )\n#print(\"done making offspring\")\n#print(\"evaluating\")\nself.evaluate_population()\n#print(\"done evaluating\")\n# Gets a list of unevaluated individuals in the livepopulation, evaluates them, and removes failed attempts\n# TODO This could probably be an independent function?\ndef evaluate_population(self,):\n#Update the sliding scales and thresholds \n# Save population, TODO remove some of these\nif self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\npickle.dump(self.population, open(self.population_file, \"wb\"))\nlast_save_time = time.time()\n#Get the current thresholds per step\nself.thresholds = None\nif self.threshold_evaluation_early_stop is not None:\nold_data = self.population.evaluated_individuals[self.objective_names]\nold_data = old_data[old_data[self.objective_names].notnull().all(axis=1)]\nif len(old_data) &gt;= self.min_history_threshold:\nself.thresholds = np.array([get_thresholds(old_data[obj_name],\nstart=self.threshold_evaluation_early_stop[0], \nend=self.threshold_evaluation_early_stop[1], \nscale=self.threshold_evaluation_scaling,\nn=self.evaluation_early_stop_steps)\nfor obj_name in self.objective_names]).T\n#Get the selectors survival rates per step\nif self.selection_evaluation_early_stop is not None:\nlower = self.cur_population_size*self.selection_evaluation_early_stop[0]\nupper = self.cur_population_size*self.selection_evaluation_early_stop[1]\n#survival_counts = self.cur_population_size*(scipy.special.betainc(1,self.selection_evaluation_scaling,np.linspace(0,1,self.evaluation_early_stop_steps))*(upper-lower)+lower)\nsurvival_counts = np.array(beta_interpolation(start=lower, end=upper, scale=self.selection_evaluation_scaling, n=self.evaluation_early_stop_steps, n_steps=self.evaluation_early_stop_steps))\nself.survival_counts = survival_counts.astype(int)\nelse:\nself.survival_counts = None\nif self.evaluation_early_stop_steps is not None:\nif self.survival_counts is None:\n#TODO if we are not using selection method for each step, we can create single threads that run all steps for an individual. No need to come back each step.\nself.evaluate_population_selection_early_stop(survival_counts=self.survival_counts, thresholds=self.thresholds, budget=self.budget)\nelse:\n#parallelize one step at a time. After each step, come together and select the next individuals to run the next step on.\nself.evaluate_population_selection_early_stop(survival_counts=self.survival_counts, thresholds=self.thresholds, budget=self.budget)\nelse:\nself.evaluate_population_full(budget=self.budget)\n# Save population, TODO remove some of these\nif self.population_file is not None: # and time.time() - last_save_time &gt; 60*10:\npickle.dump(self.population, open(self.population_file, \"wb\"))\nlast_save_time = time.time()\ndef evaluate_population_full(self, budget=None):\nindividuals_to_evaluate = self.get_unevaluated_individuals(self.objective_names, budget=budget,)\n#print(\"evaluating this many individuals: \", len(individuals_to_evaluate))\nif len(individuals_to_evaluate) == 0:\nif self.verbose &gt; 3:\nprint(\"No new individuals to evaluate\")\nreturn\nif self.max_eval_time_seconds is not None:\ntheoretical_timeout = self.max_eval_time_seconds * math.ceil(len(individuals_to_evaluate) / self.n_jobs)\ntheoretical_timeout = theoretical_timeout*2\nelse:\ntheoretical_timeout = np.inf\nscheduled_timeout_time_left = self.scheduled_timeout_time - time.time()\nparallel_timeout = min(theoretical_timeout, scheduled_timeout_time_left)\nif parallel_timeout &lt; 0:\nparallel_timeout = 10\nscores = tpot2.utils.eval_utils.parallel_eval_objective_list(individuals_to_evaluate, self.objective_functions, self.n_jobs, verbose=self.verbose, timeout=self.max_eval_time_seconds, budget=budget, n_expected_columns=len(self.objective_names), client=self._client, parallel_timeout=parallel_timeout, **self.objective_kwargs)\nself.population.update_column(individuals_to_evaluate, column_names=self.objective_names, data=scores)\nif budget is not None:\nself.population.update_column(individuals_to_evaluate, column_names=\"Budget\", data=budget)\nself.population.update_column(individuals_to_evaluate, column_names=\"Completed Timestamp\", data=time.time())\nself.population.remove_invalid_from_population(column_names=self.objective_names)\nself.population.remove_invalid_from_population(column_names=self.objective_names, invalid_value=\"TIMEOUT\")\ndef get_unevaluated_individuals(self, column_names, budget=None, individual_list=None):\nif individual_list is not None:\ncur_pop = np.array(individual_list)\nelse:\ncur_pop = np.array(self.population.population)\nif all([name_step in self.population.evaluated_individuals.columns for name_step in column_names]):\nif budget is not None:\noffspring_scores = self.population.get_column(cur_pop, column_names=column_names+[\"Budget\"], to_numpy=False)\n#Individuals are unevaluated if we have a higher budget OR if any of the objectives are nan\nunevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna()) or (offspring_scores.loc[offspring_scores.index[i]][\"Budget\"] &lt; budget)\nelse:\noffspring_scores = self.population.get_column(cur_pop, column_names=column_names, to_numpy=False)\nunevaluated_filter = lambda i: any(offspring_scores.loc[offspring_scores.index[i]][column_names].isna())\nunevaluated_individuals_this_step = [i for i in range(len(cur_pop)) if unevaluated_filter(i)]\nreturn cur_pop[unevaluated_individuals_this_step]\nelse: #if column names are not in the evaluated_individuals, then we have not evaluated any individuals yet\nfor name_step in column_names:\nself.population.evaluated_individuals[name_step] = np.nan\nreturn cur_pop\ndef evaluate_population_selection_early_stop(self,survival_counts, thresholds=None, budget=None):\nsurvival_selector = tpot2.parent_selectors.survival_select_NSGA2\n################\nobjective_function_signs = np.sign(self.objective_function_weights)\ncur_individuals = self.population.population.copy()\nall_step_names = []\nfor step in range(self.evaluation_early_stop_steps):\nif budget is None:\nthis_step_names = [f\"{n}_step_{step}\" for n in self.objective_names]\nelse:\nthis_step_names = [f\"{n}_budget_{budget}_step_{step}\" for n in self.objective_names]\nall_step_names.append(this_step_names)\nunevaluated_individuals_this_step = self.get_unevaluated_individuals(this_step_names, budget=None, individual_list=cur_individuals)\nif len(unevaluated_individuals_this_step) == 0:\nif self.verbose &gt; 3:\nprint(\"No new individuals to evaluate\")\ncontinue\nif self.max_eval_time_seconds is not None:\ntheoretical_timeout = self.max_eval_time_seconds * math.ceil(len(unevaluated_individuals_this_step) / self.n_jobs)\ntheoretical_timeout = theoretical_timeout*2\nelse:\ntheoretical_timeout = np.inf\nscheduled_timeout_time_left = self.scheduled_timeout_time - time.time()\nparallel_timeout = min(theoretical_timeout, scheduled_timeout_time_left)\nif parallel_timeout &lt; 0:\nparallel_timeout = 10\nscores = tpot2.utils.eval_utils.parallel_eval_objective_list(individual_list=unevaluated_individuals_this_step,\nobjective_list=self.objective_functions,\nn_jobs = self.n_jobs,\nverbose=self.verbose,\ntimeout=self.max_eval_time_seconds,\nstep=step,\nbudget = self.budget,\ngeneration = self.generation,\nn_expected_columns=len(self.objective_names),\nclient=self._client,\nparallel_timeout=parallel_timeout,\n**self.objective_kwargs,\n)\nself.population.update_column(unevaluated_individuals_this_step, column_names=this_step_names, data=scores)\nself.population.remove_invalid_from_population(column_names=this_step_names)\nself.population.remove_invalid_from_population(column_names=this_step_names, invalid_value=\"TIMEOUT\")\n#remove invalids:\ninvalids = []\n#find indeces of invalids\nfor j in range(len(scores)):\nif  any([s==\"INVALID\" for s in scores[j]]):\ninvalids.append(j)\nfor j in range(len(scores)):\nif  any([s==\"TIMEOUT\" for s in scores[j]]):\ninvalids.append(j)\n#already evaluated\nalready_evaluated = list(set(cur_individuals) - set(unevaluated_individuals_this_step))\n#evaluated and valid\nvalid_evaluations_this_step = remove_items(unevaluated_individuals_this_step,invalids)\n#update cur_individuals with current individuals with valid scores\ncur_individuals = np.concatenate([already_evaluated, valid_evaluations_this_step])\n#Get average scores\n#array of shape (steps, individuals, objectives)\noffspring_scores = [self.population.get_column(cur_individuals, column_names=step_names) for step_names in all_step_names]\noffspring_scores = np.array(offspring_scores)\nif self.final_score_strategy == 'mean':\noffspring_scores  = offspring_scores.mean(axis=0)\nelif self.final_score_strategy == 'last':\noffspring_scores = offspring_scores[-1]\n#if last step, add the final metrics\nif step == self.evaluation_early_stop_steps-1:\nself.population.update_column(cur_individuals, column_names=self.objective_names, data=offspring_scores)\nif budget is not None:\nself.population.update_column(cur_individuals, column_names=\"Budget\", data=budget)\nreturn\n#If we have more threads than remaining individuals, we may as well evaluate the extras too\nif self.n_jobs &lt; len(cur_individuals):\n#Remove based on thresholds\nif thresholds is not None:\nthreshold = thresholds[step]\ninvalids = []\nfor i in range(len(offspring_scores)):\nif all([s*w&gt;t*w for s,t,w in zip(offspring_scores[i],threshold,objective_function_signs)  ]):\ninvalids.append(i)\nif len(invalids) &gt; 0:\nmax_to_remove = min(len(cur_individuals) - self.n_jobs, len(invalids))\nif max_to_remove &lt; len(invalids):\ninvalids = np.random.choice(invalids, max_to_remove, replace=False)\ncur_individuals = remove_items(cur_individuals,invalids)\noffspring_scores = remove_items(offspring_scores,invalids)\n#Remove based on selection\nif survival_counts is not None:\nif step &lt; self.evaluation_early_stop_steps - 1 and survival_counts[step]&gt;1: #don't do selection for the last loop since they are completed\nk = survival_counts[step] + len(invalids) #TODO can remove the min if the selections method can ignore k&gt;population size\nif len(cur_individuals)&gt; 1 and k &gt; self.n_jobs and k &lt; len(cur_individuals):\nweighted_scores = np.array([s * self.objective_function_weights for s in offspring_scores ])\nnew_population_index = survival_selector(weighted_scores, k=k)\ncur_individuals = np.array(cur_individuals)[new_population_index]\n</code></pre>"},{"location":"documentation/tpot2/evolvers/base_evolver/#tpot2.evolvers.base_evolver.BaseEvolver.__init__","title":"<code>__init__(individual_generator, objective_functions, objective_function_weights, objective_names=None, objective_kwargs=None, bigger_is_better=True, population_size=50, initial_population_size=None, population_scaling=0.5, generations_until_end_population=1, generations=50, early_stop=None, early_stop_tol=0.001, max_time_seconds=float('inf'), max_eval_time_seconds=60 * 5, n_jobs=1, memory_limit='4GB', client=None, survival_percentage=1, crossover_probability=0.2, mutate_probability=0.7, mutate_then_crossover_probability=0.05, crossover_then_mutate_probability=0.05, n_parents=2, survival_selector=survival_select_NSGA2, parent_selector=tournament_selection_dominated, budget_range=None, budget_scaling=0.5, generations_until_end_budget=1, stepwise_steps=5, threshold_evaluation_early_stop=None, threshold_evaluation_scaling=0.5, min_history_threshold=20, selection_evaluation_early_stop=None, selection_evaluation_scaling=0.5, evaluation_early_stop_steps=None, final_score_strategy='mean', verbose=0, periodic_checkpoint_folder=None, callback=None)</code>","text":"<p>Uses mutation, crossover, and optimization functions to evolve a population of individuals towards the given objective functions.</p> <p>Parameters:</p> Name Type Description Default <code>individual_generator</code> <code>generator</code> <p>Generator that yields new base individuals. Used to generate initial population.</p> required <code>objective_functions</code> <code>list of callables</code> <p>list of functions that get applied to the individual and return a float or list of floats If an objective function returns multiple values, they are all concatenated in order  with respect to objective_function_weights and early_stop_tol.</p> required <code>objective_function_weights</code> <code>list of floats</code> <p>list of weights for each objective function. Sign flips whether bigger is better or not</p> required <code>objective_names</code> <code>list of strings</code> <p>Names of the objectives. If None, objective0, objective1, etc. will be used</p> <code>None</code> <code>objective_kwargs</code> <code>dict</code> <p>Dictionary of keyword arguments to pass to the objective function</p> <code>None</code> <code>bigger_is_better</code> <code>bool</code> <p>If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <code>True</code> <code>population_size</code> <code>int</code> <p>Size of the population</p> <code>50</code> <code>initial_population_size</code> <code>int</code> <p>Size of the initial population. If None, population_size will be used.</p> <code>None</code> <code>population_scaling</code> <code>int</code> <p>Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.</p> <code>0.5</code> <code>generations_until_end_population</code> <code>int</code> <p>Number of generations until the population size reaches population_size</p> <code>1  </code> <code>generations</code> <code>int</code> <p>Number of generations to run</p> <code>50</code> <code>early_stop</code> <code>int</code> <p>Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered.</p> <code>None</code> <code>early_stop_tol</code> <code>float, list of floats, or None</code> <p>-list of floats     list of tolerances for each objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged     If an index of the list is None, that item will not be used for early stopping -int      If an int is given, it will be used as the tolerance for all objectives</p> <code>0.001</code> <code>max_time_seconds</code> <code>float</code> <p>Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <code>float(\"inf\")</code> <code>max_eval_time_seconds</code> <code>float</code> <p>Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <code>60*5</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to run in parallel.</p> <code>1</code> <code>memory_limit</code> <code>str</code> <p>Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <code>\"4GB\"</code> <code>client</code> <code>dask.distributed.Client</code> <p>A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit.</p> <code>None</code> <code>survival_percentage</code> <code>float</code> <p>Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive.  For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.</p> <code>1</code> <code>crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals.</p> <code>.2</code> <code>mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between one individuals.</p> <code>.7</code> <code>mutate_then_crossover_probability</code> <code>float</code> <p>Probability of generating a new individual by mutating two individuals followed by crossover.</p> <code>.05</code> <code>crossover_then_mutate_probability</code> <code>float</code> <p>Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.</p> <code>.05</code> <code>n_parents</code> <code>int</code> <p>Number of parents to use for crossover. Must be greater than 1.</p> <code>2</code> <code>survival_selector</code> <code>function</code> <p>Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes. Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.</p> <code>survival_select_NSGA2</code> <code>parent_selector</code> <code>function</code> <p>Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.</p> <code>parent_select_NSGA2</code> <code>budget_range</code> <code>list[start, end]</code> <p>A starting and ending budget to use for the budget scaling.</p> <code>None</code> <code>budget_scaling</code> <p>A scaling factor to use when determining how fast we move the budget from the start to end budget.</p> <code>0.5</code> <code>generations_until_end_budget</code> <code>int</code> <p>The number of generations to run before reaching the max budget.</p> <code>1</code> <code>stepwise_steps</code> <code>int</code> <p>The number of staircase steps to take when scaling the budget and population size.</p> <code>1</code> <code>threshold_evaluation_early_stop</code> <code>list[start, end]</code> <p>starting and ending percentile to use as a threshold for the evaluation early stopping. Values between 0 and 100.</p> <code>None</code> <code>threshold_evaluation_scaling</code> <code>float [0,inf)</code> <p>A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile. Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <code>0.5</code> <code>min_history_threshold</code> <code>int</code> <p>The minimum number of previous scores needed before using threshold early stopping.</p> <code>0</code> <code>selection_evaluation_early_stop</code> <code>list</code> <p>A lower and upper percent of the population size to select each round of CV. Values between 0 and 1.</p> <code>None</code> <code>selection_evaluation_scaling</code> <code>float</code> <p>A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile. Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <code>0.5 </code> <code>evaluation_early_stop_steps</code> <code>int</code> <p>The number of steps that will be taken from the objective function. (e.g., the number of CV folds to evaluate)</p> <code>1</code> <code>final_score_strategy</code> <code>str</code> <p>The strategy to use when determining the final score for an individual. \"mean\": The mean of all objective scores \"last\": The score returned by the last call. Currently each objective is evaluated with a clone of the individual.</p> <code>\"mean\" </code> <code>verbose</code> <code>int</code> <p>How much information to print during the optimization process. Higher values include the information from lower values. 0. nothing 1. progress bar 2. evaluations progress bar 3. best individual 4. warnings</p> <p>=5. full warnings trace</p> <code>0</code> <code>periodic_checkpoint_folder</code> <code>str</code> <p>Folder to save the population to periodically. If None, no periodic saving will be done. If provided, training will resume from this checkpoint.</p> <code>None</code> <code>callback</code> <code>tpot2.CallBackInterface</code> <p>Callback object. Not implemented</p> <code>None</code> Source code in <code>tpot2/evolvers/base_evolver.py</code> <pre><code>def __init__(   self, \nindividual_generator ,\nobjective_functions,\nobjective_function_weights,\nobjective_names = None,\nobjective_kwargs = None,\nbigger_is_better = True,\npopulation_size = 50,\ninitial_population_size = None,\npopulation_scaling = .5, \ngenerations_until_end_population = 1, \ngenerations = 50, \nearly_stop = None,\nearly_stop_tol = 0.001,\nmax_time_seconds=float(\"inf\"), \nmax_eval_time_seconds=60*5,\nn_jobs=1,\nmemory_limit=\"4GB\",\nclient=None,\nsurvival_percentage = 1,\ncrossover_probability=.2,\nmutate_probability=.7,\nmutate_then_crossover_probability=.05,\ncrossover_then_mutate_probability=.05,\nn_parents=2,\nsurvival_selector = survival_select_NSGA2,\nparent_selector = tournament_selection_dominated,\nbudget_range = None, \nbudget_scaling = .5, \ngenerations_until_end_budget = 1,                    \nstepwise_steps = 5,\nthreshold_evaluation_early_stop = None, \nthreshold_evaluation_scaling = .5,\nmin_history_threshold = 20,\nselection_evaluation_early_stop = None,\nselection_evaluation_scaling = .5,\nevaluation_early_stop_steps = None, \nfinal_score_strategy = \"mean\",\nverbose = 0, \nperiodic_checkpoint_folder = None,\ncallback = None,\n) -&gt; None:\n\"\"\"\n    Uses mutation, crossover, and optimization functions to evolve a population of individuals towards the given objective functions.\n    Parameters\n    ----------\n    individual_generator : generator\n        Generator that yields new base individuals. Used to generate initial population.\n    objective_functions : list of callables\n        list of functions that get applied to the individual and return a float or list of floats\n        If an objective function returns multiple values, they are all concatenated in order \n        with respect to objective_function_weights and early_stop_tol.\n    objective_function_weights : list of floats\n        list of weights for each objective function. Sign flips whether bigger is better or not\n    objective_names : list of strings, default=None\n        Names of the objectives. If None, objective0, objective1, etc. will be used\n    objective_kwargs : dict, default=None\n        Dictionary of keyword arguments to pass to the objective function\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n    population_size : int, default=50\n        Size of the population\n    initial_population_size : int, default=None\n        Size of the initial population. If None, population_size will be used.\n    population_scaling : int, default=0.5\n        Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n    generations_until_end_population : int, default=1  \n        Number of generations until the population size reaches population_size            \n    generations : int, default=50\n        Number of generations to run\n    early_stop : int, default=None\n        Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered.\n    early_stop_tol : float, list of floats, or None, default=0.001\n        -list of floats\n            list of tolerances for each objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int \n            If an int is given, it will be used as the tolerance for all objectives\n    max_time_seconds : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n    max_eval_time_seconds : float, default=60*5\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n    memory_limit : str, default=\"4GB\"\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit. \n    survival_percentage : float, default=1\n        Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive. \n        For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.\n    crossover_probability : float, default=.2\n        Probability of generating a new individual by crossover between two individuals.\n    mutate_probability : float, default=.7\n        Probability of generating a new individual by crossover between one individuals.\n    mutate_then_crossover_probability : float, default=.05\n        Probability of generating a new individual by mutating two individuals followed by crossover.\n    crossover_then_mutate_probability : float, default=.05\n        Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n    n_parents : int, default=2\n        Number of parents to use for crossover. Must be greater than 1.\n    survival_selector : function, default=survival_select_NSGA2\n        Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n        Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n    parent_selector : function, default=parent_select_NSGA2\n        Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n    budget_range : list [start, end], default=None\n        A starting and ending budget to use for the budget scaling.\n    budget_scaling float : [0,1], default=0.5\n        A scaling factor to use when determining how fast we move the budget from the start to end budget.\n    generations_until_end_budget : int, default=1\n        The number of generations to run before reaching the max budget.\n    stepwise_steps : int, default=1\n        The number of staircase steps to take when scaling the budget and population size.\n    threshold_evaluation_early_stop : list [start, end], default=None\n        starting and ending percentile to use as a threshold for the evaluation early stopping.\n        Values between 0 and 100.\n    threshold_evaluation_scaling : float [0,inf), default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n    min_history_threshold : int, default=0\n        The minimum number of previous scores needed before using threshold early stopping.\n    selection_evaluation_early_stop : list, default=None\n        A lower and upper percent of the population size to select each round of CV.\n        Values between 0 and 1.\n    selection_evaluation_scaling : float, default=0.5 \n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n    evaluation_early_stop_steps : int, default=1\n        The number of steps that will be taken from the objective function. (e.g., the number of CV folds to evaluate)\n    final_score_strategy : str, default=\"mean\" \n        The strategy to use when determining the final score for an individual.\n        \"mean\": The mean of all objective scores\n        \"last\": The score returned by the last call. Currently each objective is evaluated with a clone of the individual.\n    verbose : int, default=0\n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n        2. evaluations progress bar\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n    callback : tpot2.CallBackInterface, default=None\n        Callback object. Not implemented\n    \"\"\"\nif threshold_evaluation_early_stop is not None or selection_evaluation_early_stop is not None:\nif evaluation_early_stop_steps is None:\nraise ValueError(\"evaluation_early_stop_steps must be set when using threshold_evaluation_early_stop or selection_evaluation_early_stop\")\nself.individual_generator = individual_generator \nself.population_size = population_size \nself.objective_functions = objective_functions \nself.objective_function_weights = np.array(objective_function_weights)\nself.bigger_is_better = bigger_is_better\nif not bigger_is_better:\nself.objective_function_weights = np.array(self.objective_function_weights)*-1\nself.initial_population_size = initial_population_size\nif self.initial_population_size is None:\nself.cur_population_size = population_size\nelse:\nself.cur_population_size = initial_population_size\nself.population_scaling = population_scaling\nself.generations_until_end_population = generations_until_end_population\nself.population_size_list = None\nself.periodic_checkpoint_folder = periodic_checkpoint_folder\nself.verbose  = verbose  \nself.callback = callback \nself.generations = generations \nself.n_jobs = n_jobs\nif max_time_seconds is None:\nself.max_time_seconds = float(\"inf\")\nelse:\nself.max_time_seconds = max_time_seconds  \n#functools requires none for infinite time, doesn't support inf\nif max_eval_time_seconds is not None and math.isinf(max_eval_time_seconds ):\nself.max_eval_time_seconds = None\nelse:\nself.max_eval_time_seconds = max_eval_time_seconds\nself.generation = 0\nself.threshold_evaluation_early_stop =threshold_evaluation_early_stop\nself.threshold_evaluation_scaling =  max(0.00001,threshold_evaluation_scaling ) \nself.min_history_threshold = min_history_threshold\nself.selection_evaluation_early_stop = selection_evaluation_early_stop\nself.selection_evaluation_scaling =  max(0.00001,selection_evaluation_scaling )\nself.evaluation_early_stop_steps = evaluation_early_stop_steps\nself.final_score_strategy = final_score_strategy\nself.budget_range = budget_range\nself.budget_scaling = budget_scaling\nself.generations_until_end_budget = generations_until_end_budget\nself.stepwise_steps = stepwise_steps\nself.memory_limit = memory_limit\nself.client = client\nself.survival_selector=survival_selector\nself.parent_selector=parent_selector\nself.survival_percentage = survival_percentage\ntotal_var_p = crossover_probability + mutate_probability + mutate_then_crossover_probability + crossover_then_mutate_probability\nself.crossover_probability = crossover_probability / total_var_p\nself.mutate_probability = mutate_probability  / total_var_p\nself.mutate_then_crossover_probability= mutate_then_crossover_probability / total_var_p\nself.crossover_then_mutate_probability= crossover_then_mutate_probability / total_var_p\nself.n_parents = n_parents\nif objective_kwargs is None:\nself.objective_kwargs = {}\nelse:\nself.objective_kwargs = objective_kwargs\n# if objective_kwargs is None:\n#     self.objective_kwargs = [{}] * len(self.objective_functions)\n# elif isinstance(objective_kwargs, dict):\n#     self.objective_kwargs = [objective_kwargs] * len(self.objective_functions)\n# else:\n#     self.objective_kwargs = objective_kwargs\n###########\nif self.initial_population_size != self.population_size:\nself.population_size_list = beta_interpolation(start=self.cur_population_size, end=self.population_size, scale=self.population_scaling, n=generations_until_end_population, n_steps=self.stepwise_steps)\nself.population_size_list = np.round(self.population_size_list).astype(int)\nif self.budget_range is None:\nself.budget_list = None\nelse:\nself.budget_list = beta_interpolation(start=self.budget_range[0], end=self.budget_range[1], n=self.generations_until_end_budget, scale=self.budget_scaling, n_steps=self.stepwise_steps)\nif objective_names is None:\nself.objective_names = [\"objective\"+str(i) for i in range(len(objective_function_weights))]\nelse:\nself.objective_names = objective_names\nif self.budget_list is not None:\nif len(self.budget_list) &lt;= self.generation:\nself.budget = self.budget_list[-1]\nelse:\nself.budget = self.budget_list[self.generation]\nelse:\nself.budget = None\nself.early_stop_tol = early_stop_tol\nself.early_stop = early_stop\nif isinstance(self.early_stop_tol, float):\nself.early_stop_tol = [self.early_stop_tol for _ in range(len(self.objective_names))]\nself.early_stop_tol = [np.inf if tol is None else tol for tol in self.early_stop_tol]\nself.population = None\nself.population_file = None\nif self.periodic_checkpoint_folder is not None:\nself.population_file = os.path.join(self.periodic_checkpoint_folder, \"population.pkl\")\nif not os.path.exists(self.periodic_checkpoint_folder):\nos.makedirs(self.periodic_checkpoint_folder)\nif os.path.exists(self.population_file):\nself.population = pickle.load(open(self.population_file, \"rb\"))\nif len(self.population.evaluated_individuals)&gt;0 and \"Generation\" in self.population.evaluated_individuals.columns:      \nself.generation = self.population.evaluated_individuals['Generation'].max() + 1 #TODO check if this is empty?\ninit_names = self.objective_names\nif self.budget_range is not None:\ninit_names = init_names + [\"Budget\"]\nif self.population is None:\nself.population = tpot2.Population(column_names=init_names)\ninitial_population = [next(self.individual_generator) for _ in range(self.cur_population_size)]\nself.population.add_to_population(initial_population)\nself.population.update_column(self.population.population, column_names=\"Generation\", data=self.generation)\n</code></pre>"},{"location":"documentation/tpot2/evolvers/steady_state_evolver/","title":"Steady state evolver","text":""},{"location":"documentation/tpot2/individual_representations/individual/","title":"Individual","text":""},{"location":"documentation/tpot2/individual_representations/graph_pipeline_individual/individual/","title":"Individual","text":""},{"location":"documentation/tpot2/individual_representations/graph_pipeline_individual/individual/#tpot2.individual_representations.graph_pipeline_individual.individual.GraphIndividual","title":"<code>GraphIndividual</code>","text":"<p>         Bases: <code>BaseIndividual</code></p> <p>An individual that contains a template for a graph sklearn pipeline. </p> <p>Parameters:</p> Name Type Description Default <code>root_config_dict</code> <p>A dictionary of methods and functions that return a dictionary of hyperparameters. Used to create the root node of the graph.</p> <code>dict with format {method class: param_function}</code> <code>inner_config_dict</code> <p>A dictionary of methods and functions that return a dictionary of hyperparameters.  Used to create the inner nodes of the graph. If None, uses root_config_dict.</p> <code>dict with format {method class: param_function}</code> <code>leaf_config_dict</code> <p>A dictionary of methods and functions that return a dictionary of hyperparameters. Used to create the leaf nodes of the graph. If not None, then all leafs must be created from this dictionary.  Otherwise leaves will be created from inner_config_dict.</p> <code>dict with format {method class: param_function}</code> <code>initial_graph</code> <code>(nx.DiGraph or list):</code> <p>A graph to initialize the individual with.  If a list, it will initialize a linear graph with the methods in the list in the sequence provided. If the items in the list are dictionaries, nodes will be itialized with those dictionaries. Strings in the list correspond to the default configuration files. They can be 'Selector', 'Regressor', 'Transformer', 'Classifier'.</p> <code>None</code> <code>max_depth</code> <code>int</code> <p>The maximum depth of the graph as measured by the shortest distance from the root.</p> required <code>max_size</code> <code>int</code> <p>The maximum number of nodes in the graph.</p> <code>np.inf</code> <code>max_children</code> <code>int</code> <p>The maximum number of children a node can have.</p> required <code>name</code> <code>str</code> <p>The name of the individual.</p> <code>None</code> <code>crossover_same_depth</code> <code>bool</code> <p>If true, then crossover will only occur between nodes of the same depth as measured by the shortest distance from the root.</p> <code>False</code> <code>crossover_same_recursive_depth</code> <code>bool</code> <p>If the graph is recursive, then crossover will only occur between graphs of the same recursive depth as measured by the shortest distance from the root.</p> <code>True</code> Source code in <code>tpot2/individual_representations/graph_pipeline_individual/individual.py</code> <pre><code>class GraphIndividual(BaseIndividual):\n'''\n    An individual that contains a template for a graph sklearn pipeline. \n    Parameters\n    ----------\n    root_config_dict : {dict with format {method class: param_function}} \n        A dictionary of methods and functions that return a dictionary of hyperparameters.\n        Used to create the root node of the graph.\n    inner_config_dict : {dict with format {method class: param_function}} \n        A dictionary of methods and functions that return a dictionary of hyperparameters. \n        Used to create the inner nodes of the graph. If None, uses root_config_dict.\n    leaf_config_dict : {dict with format {method class: param_function}} \n        A dictionary of methods and functions that return a dictionary of hyperparameters.\n        Used to create the leaf nodes of the graph. If not None, then all leafs must be created from this dictionary. \n        Otherwise leaves will be created from inner_config_dict.\n    initial_graph : (nx.DiGraph or list):\n        A graph to initialize the individual with. \n        If a list, it will initialize a linear graph with the methods in the list in the sequence provided.\n        If the items in the list are dictionaries, nodes will be itialized with those dictionaries.\n        Strings in the list correspond to the default configuration files. They can be 'Selector', 'Regressor', 'Transformer', 'Classifier'.\n    max_depth : (int)\n        The maximum depth of the graph as measured by the shortest distance from the root.\n    max_size : (int)\n        The maximum number of nodes in the graph.\n    max_children : (int)\n        The maximum number of children a node can have.\n    name : (str)\n        The name of the individual.\n    crossover_same_depth : (bool)\n        If true, then crossover will only occur between nodes of the same depth as measured by the shortest distance from the root.\n    crossover_same_recursive_depth : (bool)\n        If the graph is recursive, then crossover will only occur between graphs of the same recursive depth as measured by the shortest distance from the root.\n    '''\ndef __init__(\nself,\nroot_config_dict,\ninner_config_dict=None, \nleaf_config_dict=None,\ninitial_graph = None,\nmax_size = np.inf, \nlinear_pipeline = False,\nname=None,\ncrossover_same_depth = False,\ncrossover_same_recursive_depth = True,\nunique_subset_values = None,\ninitial_subset_values = None,\n):\nself.__debug = False\nself.root_config_dict = root_config_dict\nself.inner_config_dict = inner_config_dict\nself.leaf_config_dict = leaf_config_dict\nself.max_size = max_size\nself.name = name\nself.crossover_same_depth = crossover_same_depth\nself.crossover_same_recursive_depth = crossover_same_recursive_depth\nself.unique_subset_values = unique_subset_values\nself.initial_subset_values = initial_subset_values\nif self.unique_subset_values is not None:\nself.row_subset_selector = tpot2.representations.SubsetSelector(values=unique_subset_values, initial_set=initial_subset_values,k=20)\nif isinstance(initial_graph, nx.DiGraph):\nself.graph = initial_graph\nself.root = list(nx.topological_sort(self.graph))[0]\nif self.leaf_config_dict is not None and len(self.graph.nodes) == 1:\nfirst_leaf = create_node(self.leaf_config_dict)\nself.graph.add_edge(self.root,first_leaf)\nelif isinstance(initial_graph, list):\nnode_list = []\nfor item in initial_graph:\nif isinstance(item, dict):\nnode_list.append(create_node(item))\nelif isinstance(item, str):\nif item == 'Selector':\nfrom tpot2.config import selector_config_dictionary\nnode_list.append(create_node(selector_config_dictionary))\nelif  item == 'Regressor':\nfrom tpot2.config import regressor_config_dictionary\nnode_list.append(create_node(regressor_config_dictionary))\nelif  item == 'Transformer':\nfrom tpot2.config import transformer_config_dictionary\nnode_list.append(create_node(transformer_config_dictionary))\nelif  item == 'Classifier': \nfrom tpot2.config import classifier_config_dictionary\nnode_list.append(create_node(classifier_config_dictionary))\nself.graph = nx.DiGraph()\nfor child, parent in zip(node_list, node_list[1:]):\nself.graph.add_edge(parent, child)\nself.root = node_list[-1]\nelse:\nself.graph = nx.DiGraph()\nself.root = create_node(self.root_config_dict)\nself.graph.add_node(self.root)\nif self.leaf_config_dict is not None:\nfirst_leaf = create_node(self.leaf_config_dict)\nself.graph.add_edge(self.root,first_leaf)\nself.initialize_all_nodes()\n#self.root =list(nx.topological_sort(self.graph))[0]\nself.mutate_methods_list =     [self._mutate_hyperparameters,\nself._mutate_replace_node, \nself._mutate_remove_node,\nself._mutate_remove_edge,\nself._mutate_add_edge,\nself._mutate_insert_leaf,\nself._mutate_insert_bypass_node,\nself._mutate_insert_inner_node,\n]\nself.crossover_methods_list = [\n#self._crossover_swap_node,\n#self._crossover_hyperparameters,\nself._crossover_swap_branch,\nself._crossover_take_branch,\n#self._crossover_swap_leaf_at_node,\n]\nif self.inner_config_dict is not None:\nself.mutate_methods_list.append(self._mutate_insert_inner_node)\nself.crossover_methods_list.append(self._crossover_take_branch) #this is the only crossover method that can create inner nodes\nif not linear_pipeline:\nself.mutate_methods_list.append(self._mutate_insert_bypass_node)\nif not linear_pipeline:\nself.mutate_methods_list.append(self._mutate_insert_leaf)\nif self.unique_subset_values is not None:\nself.crossover_methods_list.append(self._crossover_row_subsets)\nself.mutate_methods_list.append(self._mutate_row_subsets )\nself.optimize_methods_list = [ #self._optimize_optuna_single_method_full_pipeline,\nself._optimize_optuna_all_methods_full_pipeline]\nself.key = None\ndef select_config_dict(self, node):\n#check if the node is root, leaf, or inner\nif len(list(self.graph.predecessors(node))) == 0: #root\nreturn self.root_config_dict\nelif self.leaf_config_dict is not None and len(list(self.graph.successors(node))) == 0: #leaf\nreturn self.leaf_config_dict\nelse: #inner\nreturn self.inner_config_dict\ndef initialize_all_nodes(self,):\nfor node in self.graph:\nif isinstance(node,GraphIndividual):\ncontinue\nif node.method_class is None:\nnode.method_class = random.choice(list(self.select_config_dict(node).keys()))\nif node.hyperparameters is None:\nnode.hyperparameters = self.select_config_dict(node)[node.method_class](config.hyperparametersuggestor)\ndef fix_noncompliant_leafs(self):\nleafs = [node for node in self.graph.nodes if len(list(self.graph.successors(node)))==0]\ncompliant_leafs = []\nnoncompliant_leafs = []\nfor leaf in leafs:\nif leaf.method_class in self.leaf_config_dict:\ncompliant_leafs.append(leaf)\nelse:\nnoncompliant_leafs.append(leaf)\n#find all good leafs. If no good leaves exist, create a new one\nif len(compliant_leafs) == 0:\nfirst_leaf = NodeLabel(config_dict=self.leaf_config_dict)\nfirst_leaf.method_class = random.choice(list(first_leaf.config_dict.keys())) #TODO: check when there is no new method\nfirst_leaf.hyperparameters = first_leaf.config_dict[first_leaf.method_class](config.hyperparametersuggestor)\ncompliant_leafs.append(first_leaf)\n#connect bad leaves to good leaves (making them internal nodes)\nif len(noncompliant_leafs) &gt; 0:\nfor node in noncompliant_leafs:\nself.graph.add_edge(node, random.choice(compliant_leafs))\ndef _merge_duplicated_nodes(self): \ngraph_changed = False\nmerged = False\nwhile(not merged):\nnode_list = list(self.graph.nodes)\nmerged = True\nfor node, other_node in itertools.product(node_list, node_list):\nif node is other_node or isinstance(node,GraphIndividual) or isinstance(other_node,GraphIndividual):\ncontinue\n#If nodes are same class/hyperparameters\nif node.method_class == other_node.method_class and node.hyperparameters == other_node.hyperparameters:\nnode_children = set(self.graph.successors(node))\nother_node_children = set(self.graph.successors(other_node))\n#if nodes have identical children, they can be merged\nif node_children == other_node_children: \nfor other_node_parent in list(self.graph.predecessors(other_node)):\nif other_node_parent not in self.graph.predecessors(node):\nself.graph.add_edge(other_node_parent,node)\nself.graph.remove_node(other_node)\nmerged=False\ngraph_changed = True\nbreak\nreturn graph_changed\n#returns a flattened pipeline\ndef flatten_pipeline(self,depth=0):\nflattened_full_graph = self.graph.copy()\nremove_list = []\nfor node in flattened_full_graph:\nif isinstance(node,GraphIndividual):\nflattened = node.flatten_pipeline(depth+1)\nroots = graph_utils.get_roots(flattened)\nleaves = graph_utils.get_leaves(flattened)\nn1_s = flattened_full_graph.successors(node)\nn1_p = flattened_full_graph.predecessors(node)\nremove_list.append(node)\nflattened_full_graph = nx.compose(flattened_full_graph, flattened)\nflattened_full_graph.add_edges_from([ (n2, n) for n in n1_s for n2 in leaves])\nflattened_full_graph.add_edges_from([ (n, n2) for n in n1_p for n2 in roots])\nelse:\nflattened_full_graph.nodes[node]['recursive depth'] = depth\nfor node in remove_list:\nflattened_full_graph.remove_node(node)\nif self.unique_subset_values is not None:\nfor node in flattened_full_graph:\nif \"subset_values\" not in flattened_full_graph.nodes[node]:\nflattened_full_graph.nodes[node][\"subset_values\"] = list(self.row_subset_selector.subsets)\nelse:\n#intersection\nflattened_full_graph.nodes[node][\"subset_values\"] = list(set(flattened_full_graph.nodes[node][\"subset_values\"]) &amp; set(self.row_subset_selector.subsets))\nreturn flattened_full_graph\ndef get_num_nodes(self,):\nnum_nodes = 0\nfor node in self.graph.nodes:\nif isinstance(node, GraphIndividual):\nnum_nodes+= node.get_num_nodes()\nelse:\nnum_nodes += 1\nreturn num_nodes\ndef export_nested_pipeline(self, **graph_pipeline_args):\nflattened_full_graph = self.graph.copy()\nremove_list = []\nfor node in list(flattened_full_graph.nodes):\nif isinstance(node,GraphIndividual):\ngp = node.export_pipeline(**graph_pipeline_args)\nn1_s = flattened_full_graph.successors(node)\nn1_p = flattened_full_graph.predecessors(node)\nremove_list.append(node)\nflattened_full_graph.add_node(gp)\nflattened_full_graph.add_edges_from([ (gp, n) for n in n1_s])\nflattened_full_graph.add_edges_from([ (n, gp) for n in n1_p])\nfor node in remove_list:\nflattened_full_graph.remove_node(node)\nestimator_graph = flattened_full_graph\n#mapping = {node:node.method_class(**node.hyperparameters) for node in estimator_graph}\nlabel_remapping = {}\nlabel_to_instance = {}\nfor node in estimator_graph: \nfound_unique_label = False\ni=1\nwhile not found_unique_label:\nprint(type(node))\nif type(node) is tpot2.GraphPipeline:\nlabel = \"GraphPipeline_{0}\".format( i)\nelse:\nlabel = \"{0}_{1}\".format(node.method_class.__name__, i)\nif label not in label_to_instance:\nfound_unique_label = True\nelse:\ni+=1\nif type(node) is tpot2.GraphPipeline:\nlabel_remapping[node] = label\nlabel_to_instance[label] = node\nelse:\nlabel_remapping[node] = label\nlabel_to_instance[label] = node.method_class(**node.hyperparameters)\nestimator_graph = nx.relabel_nodes(estimator_graph, label_remapping)\nfor label, instance in label_to_instance.items():\nestimator_graph.nodes[label][\"instance\"] = instance\nreturn tpot2.GraphPipeline(graph=estimator_graph, **graph_pipeline_args)\ndef export_pipeline(self, **graph_pipeline_args):\nestimator_graph = self.flatten_pipeline()\n#mapping = {node:node.method_class(**node.hyperparameters) for node in estimator_graph}\nlabel_remapping = {}\nlabel_to_instance = {}\nfor node in estimator_graph: \nfound_unique_label = False\ni=1\nwhile not found_unique_label:\nlabel = \"{0}_{1}\".format(node.method_class.__name__, i)\nif label not in label_to_instance:\nfound_unique_label = True\nelse:\ni+=1\nlabel_remapping[node] = label\nlabel_to_instance[label] = node.method_class(**node.hyperparameters)\nestimator_graph = nx.relabel_nodes(estimator_graph, label_remapping)\nfor label, instance in label_to_instance.items():\nestimator_graph.nodes[label][\"instance\"] = instance\nreturn tpot2.GraphPipeline(graph=estimator_graph, **graph_pipeline_args)\ndef export_baikal(self,):\ngraph = self.flatten_pipeline()\ntoposorted = list(nx.topological_sort(graph))\ntoposorted.reverse()\nnode_outputs = {}\nX = baikal.Input('X')\ny = baikal.Input('Target')\nfor i in range(len(toposorted)):\nnode = toposorted[i]\nif len(list(graph.successors(node))) == 0: #If this node had no inputs use X\nthis_inputs = X\nelse: #in node has inputs, get those\nthis_inputs = [node_outputs[child] for child in graph.successors(node)]\nthis_output = baikal.make_step(node.method_class, class_name=node.method_class.__name__)(**node.hyperparameters)(this_inputs,y)\nnode_outputs[node] = this_output\nif i == len(toposorted)-1: #last method doesn't need transformed.\nreturn baikal.Model(inputs=X, outputs=this_output, targets=y)\ndef plot(self):\nG = self.flatten_pipeline().reverse() #self.graph.reverse()\n#TODO clean this up\ntry:\npos = nx.planar_layout(G)  # positions for all nodes\nexcept:\npos = nx.shell_layout(G)\n# nodes\noptions = {'edgecolors': 'tab:gray', 'node_size': 800, 'alpha': 0.9}\nnodelist = list(G.nodes)\nnode_color = [plt.cm.Set1(G.nodes[n]['recursive depth']) for n in G]\nfig, ax = plt.subplots()\nnx.draw(G, pos, nodelist=nodelist, node_color=node_color, ax=ax,  **options)\n'''edgelist = []\n        for n in n1.node_set:\n            for child in n.children:\n                edgelist.append((n,child))'''\n# edges\n#nx.draw_networkx_edges(G, pos, width=3.0, arrows=True)\n'''nx.draw_networkx_edges(\n            G,\n            pos,\n            edgelist=[edgelist],\n            width=8,\n            alpha=0.5,\n            edge_color='tab:red',\n        )'''\n# some math labels\nlabels = {}\nfor i, n in enumerate(G.nodes):\nlabels[n] = n.method_class.__name__ + \"\\n\" + str(n.hyperparameters)\nnx.draw_networkx_labels(G, pos, labels,ax=ax, font_size=7, font_color='black')\nplt.tight_layout()\nplt.axis('off')\nplt.show()\n#############\n#TODO currently does not correctly return false when adding a leaf causes a duplicate node that is later merged\ndef mutate(self,):\nself.key = None\ngraph = self.select_graphindividual()\nreturn graph._mutate()\ndef _mutate(self,):\nrandom.shuffle(self.mutate_methods_list)\nfor mutate_method in self.mutate_methods_list:\nif mutate_method():\nself._merge_duplicated_nodes()\nif self.__debug:\nprint(mutate_method)\nif self.root not in self.graph.nodes:\nprint('lost root something went wrong with ', mutate_method)\nif len(self.graph.predecessors(self.root)) &gt; 0:\nprint('root has parents ', mutate_method)\nif any([n in nx.ancestors(self.graph,n) for n in self.graph.nodes]):\nprint('a node is connecting to itself...')\nif self.__debug:\ntry:\nnx.find_cycle(self.graph)\nprint('something went wrong with ', mutate_method)\nexcept: \npass\nreturn True\nreturn False\ndef _mutate_row_subsets(self,):\nif self.unique_subset_values is not None:\nself.row_subset_selector.mutate()\ndef _mutate_hyperparameters(self):\n'''\n        Mutates the hyperparameters for a randomly chosen node in the graph.\n        '''\nsorted_nodes_list = list(self.graph.nodes)\nrandom.shuffle(sorted_nodes_list) \nfor node in sorted_nodes_list:\nif isinstance(node,GraphIndividual):\ncontinue\nif isinstance(self.select_config_dict(node)[node.method_class], dict):\ncontinue\nnode.hyperparameters = self.select_config_dict(node)[node.method_class](config.hyperparametersuggestor) \nreturn True\nreturn False\ndef _mutate_replace_node(self):\n'''\n        Replaces the method in a randomly chosen node by a method from the available methods for that node.\n        '''\nsorted_nodes_list = list(self.graph.nodes)\nrandom.shuffle(sorted_nodes_list) \nfor node in sorted_nodes_list:\nif isinstance(node,GraphIndividual):\ncontinue\nnode.method_class = random.choice(list(self.select_config_dict(node).keys())) \nif isinstance(self.select_config_dict(node)[node.method_class], dict):\nhyperparameters = self.select_config_dict(node)[node.method_class]\nelse: \nhyperparameters = self.select_config_dict(node)[node.method_class](config.hyperparametersuggestor)\nnode.hyperparameters = hyperparameters\nreturn True\nreturn False\ndef _mutate_remove_node(self):\n'''\n        Removes a randomly chosen node and connects its parents to its children.\n        If the node is the only leaf for an inner node and 'leaf_config_dict' is not none, we do not remove it.\n        '''\nnodes_list = list(self.graph.nodes)\nnodes_list.remove(self.root)\nleaves = graph_utils.get_leaves(self.graph)\nwhile len(nodes_list) &gt; 0:\nnode = random.choices(nodes_list,)[0]\nnodes_list.remove(node)\nif self.leaf_config_dict is not None and len(list(nx.descendants(self.graph,node))) == 0 : #if the node is a leaf\nif len(leaves) &lt;= 1:\ncontinue #dont remove the last leaf\nleaf_parents = self.graph.predecessors(node)\n# if any of the parents of the node has one one child, continue\nif any([len(list(self.graph.successors(lp))) &lt; 2 for lp in leaf_parents]): #dont remove a leaf if it is the only input into another node.\ncontinue\ngraph_utils.remove_and_stitch(self.graph, node)\ngraph_utils.remove_nodes_disconnected_from_node(self.graph, self.root)\nreturn True\nelse:\ngraph_utils.remove_and_stitch(self.graph, node)\ngraph_utils.remove_nodes_disconnected_from_node(self.graph, self.root)\nreturn True\nreturn False\ndef _mutate_remove_edge(self):\n'''\n        Deletes an edge as long as deleting that edge does not make the graph disconnected.\n        '''\nsorted_nodes_list = list(self.graph.nodes)\nrandom.shuffle(sorted_nodes_list) \nfor child_node in sorted_nodes_list:\nparents = list(self.graph.predecessors(child_node))\nif len(parents) &gt; 1: # if it has more than one parent, you can remove an edge (if this is the only child of a node, it will become a leaf)\nfor parent_node in parents:\n# if removing the egde will make the parent_node a leaf node, skip\nif self.leaf_config_dict is not None and len(list(self.graph.successors(parent_node))) &lt; 2: \ncontinue\nself.graph.remove_edge(parent_node, child_node)\nreturn True\nreturn False\ndef _mutate_add_edge(self):\n'''\n        Randomly add an edge from a node to another node that is not an ancestor of the first node.\n        '''\nsorted_nodes_list = list(self.graph.nodes)\nrandom.shuffle(sorted_nodes_list)\nfor child_node in sorted_nodes_list:\nfor parent_node in sorted_nodes_list:\nif self.leaf_config_dict is not None:\nif len(list(self.graph.successors(parent_node))) == 0:\ncontinue\n# skip if\n# - parent and child are the same node\n# - edge already exists\n# - child is an ancestor of parent\nif  (child_node is not parent_node) and not self.graph.has_edge(parent_node,child_node) and (child_node not in nx.ancestors(self.graph, parent_node)):         \nself.graph.add_edge(parent_node,child_node)\nreturn True\nreturn False\ndef _mutate_insert_leaf(self):\nif self.max_size &gt; self.graph.number_of_nodes():\nsorted_nodes_list = list(self.graph.nodes)\nrandom.shuffle(sorted_nodes_list) #TODO: sort by number of children and/or parents? bias model one way or another\nfor node in sorted_nodes_list:\n#if leafs are protected, check if node is a leaf\n#if node is a leaf, skip because we don't want to add node on top of node\nif (self.leaf_config_dict is not None #if leafs are protected\nand   len(list(self.graph.successors(node))) == 0 #if node is leaf\nand  len(list(self.graph.predecessors(node))) &gt; 0 #except if node is root, in which case we want to add a leaf even if it happens to be a leaf too\n):\ncontinue\n#If node *is* the root or is not a leaf, add leaf node. (dont want to add leaf on top of leaf)\nif self.leaf_config_dict is not None:\nnew_node = create_node(self.leaf_config_dict)\nelse:\nnew_node = create_node(self.inner_config_dict)\nself.graph.add_node(new_node)\nself.graph.add_edge(node, new_node)\nreturn True\nreturn False\ndef _mutate_insert_bypass_node(self):\nif self.max_size &gt; self.graph.number_of_nodes():\nsorted_nodes_list = list(self.graph.nodes)\nsorted_nodes_list2 = list(self.graph.nodes)\nrandom.shuffle(sorted_nodes_list) #TODO: sort by number of children and/or parents? bias model one way or another\nrandom.shuffle(sorted_nodes_list2)\nfor node in sorted_nodes_list:  \nfor child_node in sorted_nodes_list2:\nif child_node is not node and child_node not in nx.ancestors(self.graph, node):\nif self.leaf_config_dict is not None:\n#If if we are protecting leafs, dont add connection into a leaf\nif len(list(nx.descendants(self.graph,node))) ==0 :\ncontinue\nnew_node = create_node(config_dict = self.inner_config_dict)\nself.graph.add_node(new_node)\nself.graph.add_edges_from([(node, new_node), (new_node, child_node)])\nreturn True\nreturn False\ndef _mutate_insert_inner_node(self):\nif self.max_size &gt; self.graph.number_of_nodes():\nsorted_nodes_list = list(self.graph.nodes)\nsorted_nodes_list2 = list(self.graph.nodes)\nrandom.shuffle(sorted_nodes_list) #TODO: sort by number of children and/or parents? bias model one way or another\nrandom.shuffle(sorted_nodes_list2)\nfor node in sorted_nodes_list:\n#loop through children of node\nfor child_node in list(self.graph.successors(node)):\nif child_node is not node and child_node not in nx.ancestors(self.graph, node):\nif self.leaf_config_dict is not None:\n#If if we are protecting leafs, dont add connection into a leaf\nif len(list(nx.descendants(self.graph,node))) ==0 :\ncontinue\nnew_node = create_node(config_dict = self.inner_config_dict)\nself.graph.add_node(new_node)\nself.graph.add_edges_from([(node, new_node), (new_node, child_node)])\nself.graph.remove_edge(node, child_node)\nreturn True\nreturn False\n######################################################\n# Crossover\ndef get_graphs(self):\ngraphs = [self]\nself.graph.graph['depth'] = 0\nself.graph.graph['recursive depth'] = 0\nfor node in self.graph.nodes:\nif isinstance(node, GraphIndividual):\nnode.graph.graph['depth'] = nx.shortest_path_length(self.graph, self.root, node)\ngraphs = graphs + node._get_graphs(depth=1)\nreturn graphs\ndef _get_graphs(self, depth=1):\ngraphs = [self]\nself.graph.graph['recursive depth'] = depth\nfor node in self.graph.nodes:\nif isinstance(node, GraphIndividual):\nnode.graph.graph['depth'] = nx.shortest_path_length(self.graph, self.root, node)\ngraphs = graphs + node._get_graphs(depth=depth+1)\nreturn graphs\ndef select_graphindividual(self,):\ngraphs = self.get_graphs()\nweights = [g.graph.number_of_nodes() for g in graphs]\nreturn random.choices(graphs, weights=weights)[0]\ndef select_graph_same_recursive_depth(self,ind1,ind2):\ngraphs1 = ind1.get_graphs()\nweights1 = [g.graph.number_of_nodes() for g in graphs1]\ngraphs2 = ind2.get_graphs()\nweights2 = [g.graph.number_of_nodes() for g in graphs2]\ng1_sorted_graphs = random_weighted_sort(graphs1, weights1)\ng2_sorted_graphs = random_weighted_sort(graphs2, weights2)\nfor g1, g2 in zip(g1_sorted_graphs, g2_sorted_graphs):\nif g1.graph.graph['depth'] == g2.graph.graph['depth'] and g1.graph.graph['recursive depth'] == g2.graph.graph['recursive depth']:\nreturn g1, g2\nreturn ind1,ind2\ndef crossover(self, ind2):\n'''\n        self is the first individual, ind2 is the second individual\n        If crossover_same_depth, it will select graphindividuals at the same recursive depth.\n        Otherwise, it will select graphindividuals randomly from the entire graph and its subgraphs.\n        This does not impact graphs without subgraphs. And it does not impacts nodes that are not graphindividuals. Cros\n        '''\nself.key = None\nind2.key = None\nif self.crossover_same_recursive_depth:\n# selects graphs from the same recursive depth and same depth from the root\ng1, g2 = self.select_graph_same_recursive_depth(self, ind2)\nelse:\ng1 = self.select_graphindividual()\ng2 = ind2.select_graphindividual()\nreturn g1._crossover(g2)\ndef _crossover(self, Graph):\nrandom.shuffle(self.crossover_methods_list)\nfor crossover_method in self.crossover_methods_list:\nif crossover_method(Graph):\nself._merge_duplicated_nodes()\nreturn True\nif self.__debug:\ntry:\nnx.find_cycle(self.graph)\nprint('something went wrong with ', crossover_method)\nexcept: \npass\nreturn False\ndef _crossover_row_subsets(self, G2):\nif self.unique_subset_values is not None and G2.unique_subset_values is not None:\nself.row_subset_selector.crossover(G2.row_subset_selector)\ndef _crossover_swap_node(self, G2):\n'''\n        Swaps randomly chosen node from Parent1 with a randomly chosen node from Parent2.\n        '''\nif self.crossover_same_depth:\npair_gen = graph_utils.select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root)\nelse:\npair_gen = graph_utils.select_nodes_randomly(self.graph, G2.graph)\nfor node1, node2 in pair_gen:\nif not (node1 is self.root or node2 is G2.root): #TODO: allow root\nn1_s = self.graph.successors(node1)\nn1_p = self.graph.predecessors(node1)\nn2_s = G2.graph.successors(node2)\nn2_p = G2.graph.predecessors(node2)\nself.graph.remove_node(node1)\nG2.graph.remove_node(node2)\nself.graph.add_node(node2)\nself.graph.add_edges_from([ (node2, n) for n in n1_s])\nG2.graph.add_edges_from([ (node1, n) for n in n2_s])\nself.graph.add_edges_from([ (n, node2) for n in n1_p])\nG2.graph.add_edges_from([ (n, node1) for n in n2_p])\nreturn True\nreturn False\ndef _crossover_swap_branch(self, G2):\n'''\n        swaps a branch from parent1 with a branch from parent2. does not modify parent2\n        '''\nif self.crossover_same_depth:\npair_gen = graph_utils.select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root)\nelse:\npair_gen = graph_utils.select_nodes_randomly(self.graph, G2.graph)\nfor node1, node2 in pair_gen:\n#TODO: if root is in inner_config_dict, then do use it?\nif node1 is self.root or node2 is G2.root: #dont want to add root as inner node\ncontinue\n#check if node1 is a leaf and leafs are protected, don't add an input to the leave \nif self.leaf_config_dict is not None: #if we are protecting leaves, \nnode1_is_leaf = len(list(self.graph.successors(node1))) == 0\nnode2_is_leaf = len(list(G2.graph.successors(node2))) == 0\n#if not ((node1_is_leaf and node1_is_leaf) or (not node1_is_leaf and not node2_is_leaf)): #if node1 is a leaf\nif (node1_is_leaf and (not node2_is_leaf)) or ( (not node1_is_leaf) and node2_is_leaf):\n#only continue if node1 and node2 are both leaves or both not leaves\ncontinue\ntemp_graph_1 = self.graph.copy()\ntemp_graph_1.remove_node(node1)\ngraph_utils.remove_nodes_disconnected_from_node(temp_graph_1, self.root)\n#isolating the branch\nbranch2 = G2.graph.copy()\nn2_descendants = nx.descendants(branch2,node2)\nfor n in list(branch2.nodes):\nif n not in n2_descendants and n is not node2: #removes all nodes not in the branch\nbranch2.remove_node(n)\nbranch2 = copy.deepcopy(branch2)\nbranch2_root = graph_utils.get_roots(branch2)[0]\ntemp_graph_1.add_edges_from(branch2.edges)\nfor p in list(self.graph.predecessors(node1)):\ntemp_graph_1.add_edge(p,branch2_root)\nif temp_graph_1.number_of_nodes() &gt; self.max_size:\ncontinue\nself.graph = temp_graph_1\nreturn True\nreturn False\n#TODO: Currently returns true even if hyperparameters are blank\ndef _crossover_hyperparameters(self, G2):\n'''\n        Swaps the hyperparamters of one randomly chosen node in Parent1 with the hyperparameters of randnomly chosen node in Parent2.\n        '''\nif self.crossover_same_depth:\npair_gen = graph_utils.select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root)\nelse:\npair_gen = graph_utils.select_nodes_randomly(self.graph, G2.graph)\nfor node1, node2 in pair_gen:\nif isinstance(node1,GraphIndividual) or isinstance(node2,GraphIndividual):\ncontinue\nif node1.method_class == node2.method_class:\ntmp = node1.hyperparameters\nnode1.hyperparameters = node2.hyperparameters\nnode2.hyperparameters = tmp\nreturn True\nreturn False\n#not including the nodes, just their children\n#Finds leaves attached to nodes and swaps them\ndef _crossover_swap_leaf_at_node(self, G2):\nif self.crossover_same_depth:\npair_gen = graph_utils.select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root)\nelse:\npair_gen = graph_utils.select_nodes_randomly(self.graph, G2.graph)\nsuccess = False\nfor node1, node2 in pair_gen:\n# if leaves are protected node1 and node2 must both be leaves or both be inner nodes \nif self.leaf_config_dict is not None and not (len(list(self.graph.successors(node1)))==0 ^ len(list(G2.graph.successors(node2)))==0):\ncontinue\n#self_leafs = [c for c in nx.descendants(self.graph,node1) if len(list(self.graph.successors(c)))==0 and c is not node1]\nnode_leafs = [c for c in nx.descendants(G2.graph,node2) if len(list(G2.graph.successors(c)))==0 and c is not node2]\n# if len(self_leafs) &gt;0:\n#     for c in self_leafs:\n#         if random.choice([True,False]):\n#             self.graph.remove_node(c)\n#             G2.graph.add_edge(node2, c)\n#             success = True\nif len(node_leafs) &gt;0:\nfor c in node_leafs:\nif random.choice([True,False]):\nG2.graph.remove_node(c)\nself.graph.add_edge(node1, c)\nsuccess = True\nreturn success\ndef _crossover_take_branch(self, G2):\n'''\n        Takes a subgraph from Parent2 and add it to a randomly chosen node in Parent1.\n        '''\nif self.crossover_same_depth:\npair_gen = graph_utils.select_nodes_same_depth(self.graph, self.root, G2.graph, G2.root)\nelse:\npair_gen = graph_utils.select_nodes_randomly(self.graph, G2.graph)\nfor node1, node2 in pair_gen:\n#TODO: if root is in inner_config_dict, then do use it?\nif node2 is G2.root: #dont want to add root as inner node\ncontinue\n#check if node1 is a leaf and leafs are protected, don't add an input to the leave \nif self.leaf_config_dict is not None and len(list(self.graph.successors(node1))) == 0:\ncontinue\n#icheck if node2 is graph individual\n# if isinstance(node2,GraphIndividual):\n#     if not ((isinstance(node2,GraphIndividual) and (\"Recursive\" in self.inner_config_dict or \"Recursive\" in self.leaf_config_dict))):\n#         continue\n#isolating the branch\nbranch2 = G2.graph.copy()\nn2_descendants = nx.descendants(branch2,node2)\nfor n in list(branch2.nodes):\nif n not in n2_descendants and n is not node2: #removes all nodes not in the branch\nbranch2.remove_node(n)\n#if node1 plus node2 branch has more than max_children, skip\nif branch2.number_of_nodes() + self.graph.number_of_nodes() &gt; self.max_size:\ncontinue\nbranch2 = copy.deepcopy(branch2)\nbranch2_root = graph_utils.get_roots(branch2)[0]\nself.graph.add_edges_from(branch2.edges)\nself.graph.add_edge(node1,branch2_root)\nreturn True\nreturn False\n#TODO: swap all leaf nodes\ndef _crossover_swap_all_leafs(self, G2):\npass\n#TODO: currently ignores ensembles, make it include nodes inside of ensembles\ndef optimize(self, objective_function, steps=5):\nrandom.shuffle(self.optimize_methods_list) #select an optimization method\nfor optimize_method in self.optimize_methods_list:\nif optimize_method(objective_function, steps=steps):\nreturn True\n#optimize the hyperparameters of one method to improve the entire pipeline\ndef _optimize_optuna_single_method_full_pipeline(self, objective_function, steps=5):\nnodes_list = list(self.graph.nodes)\nrandom.shuffle(nodes_list) #TODO: sort by number of children and/or parents? bias model one way or another\nfor node in nodes_list:\nif not isinstance(node, NodeLabel) or isinstance(self.select_config_dict(node)[node.method_class],dict):\ncontinue\nelse:\nstudy = optuna.create_study()\ndef objective(trial):\nparams = self.select_config_dict(node)[node.method_class](trial)\nnode.hyperparameters = params\ntrial.set_user_attr('params', params)\ntry:\nreturn objective_function(self)\nexcept:\nreturn np.NAN\nstudy.optimize(objective, n_trials=steps)\nnode.hyperparameters = study.best_trial.user_attrs['params']\nreturn True\n#optimize the hyperparameters of all methods simultaneously to improve the entire pipeline\ndef _optimize_optuna_all_methods_full_pipeline(self, objective_function, steps=5):\nnodes_list = list(self.graph.nodes)\nstudy = optuna.create_study()\nnodes_to_optimize = []\nfor node in nodes_list:\nif not isinstance(node, NodeLabel) or isinstance(self.select_config_dict(node)[node.method_class],dict):\ncontinue\nelse:\nnodes_to_optimize.append(node)\ndef objective(trial):\nparam_list = []\nfor i, node in enumerate(nodes_to_optimize):\nparams = self.select_config_dict(node)[node.method_class](trial, name=f'node_{i}')\nnode.hyperparameters = params\nparam_list.append(params)\ntrial.set_user_attr('params', param_list)\ntry:\nreturn objective_function(self)\nexcept:\nreturn np.NAN\nstudy.optimize(objective, n_trials=steps)\nbest_params = study.best_trial.user_attrs['params']\nfor node, params in zip(nodes_to_optimize,best_params):\nnode.hyperparameters = params\nreturn True\ndef _cached_transform(cache_nunber=0):\n#use a cache for models at each CV fold?\n#cache just transformations at each fold?\n#TODO how to separate full model?\npass\ndef __str__(self):\nreturn self.export_pipeline().__str__()\ndef unique_id(self) -&gt; GraphKey:\nif self.key is None:\ng = self.flatten_pipeline()\nfor n in g.nodes:\nif \"subset_values\" in g.nodes[n]:\ng.nodes[n]['label'] = {n.method_class: n.hyperparameters, \"subset_values\":g.nodes[n][\"subset_values\"]}\nelse:\ng.nodes[n]['label'] = {n.method_class: n.hyperparameters}\ng.nodes[n]['method_class'] = n.method_class #TODO making this transformation doesn't feel very clean? \ng.nodes[n]['hyperparameters'] = n.hyperparameters\ng = nx.convert_node_labels_to_integers(g)\nself.key = GraphKey(graph=g)\nreturn self.key\ndef full_node_list(self):\nnode_list = list(self.graph.nodes)\nfor node in node_list:\nif isinstance(node, GraphIndividual):\nnode_list.pop(node_list.index(node))\nnode_list.extend(node.graph.nodes)\nreturn node_list\n</code></pre>"},{"location":"documentation/tpot2/individual_representations/graph_pipeline_individual/individual/#tpot2.individual_representations.graph_pipeline_individual.individual.GraphIndividual.crossover","title":"<code>crossover(ind2)</code>","text":"<p>self is the first individual, ind2 is the second individual If crossover_same_depth, it will select graphindividuals at the same recursive depth. Otherwise, it will select graphindividuals randomly from the entire graph and its subgraphs.</p> <p>This does not impact graphs without subgraphs. And it does not impacts nodes that are not graphindividuals. Cros</p> Source code in <code>tpot2/individual_representations/graph_pipeline_individual/individual.py</code> <pre><code>def crossover(self, ind2):\n'''\n    self is the first individual, ind2 is the second individual\n    If crossover_same_depth, it will select graphindividuals at the same recursive depth.\n    Otherwise, it will select graphindividuals randomly from the entire graph and its subgraphs.\n    This does not impact graphs without subgraphs. And it does not impacts nodes that are not graphindividuals. Cros\n    '''\nself.key = None\nind2.key = None\nif self.crossover_same_recursive_depth:\n# selects graphs from the same recursive depth and same depth from the root\ng1, g2 = self.select_graph_same_recursive_depth(self, ind2)\nelse:\ng1 = self.select_graphindividual()\ng2 = ind2.select_graphindividual()\nreturn g1._crossover(g2)\n</code></pre>"},{"location":"documentation/tpot2/individual_representations/graph_pipeline_individual/individual/#tpot2.individual_representations.graph_pipeline_individual.individual.GraphKey","title":"<code>GraphKey</code>","text":"<p>A class that can be used as a key for a graph.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>nx.Graph</code> <p>The graph to use as a key. Node Attributes are used for the hash.</p> required <code>matched_label</code> <code>str</code> <p>The node attribute to consider for the hash.</p> <code>'label'</code> Source code in <code>tpot2/individual_representations/graph_pipeline_individual/individual.py</code> <pre><code>class GraphKey():\n'''\n    A class that can be used as a key for a graph.\n    Parameters\n    ----------\n    graph : (nx.Graph)\n        The graph to use as a key. Node Attributes are used for the hash.\n    matched_label : (str)\n        The node attribute to consider for the hash.\n    '''\ndef __init__(self, graph, matched_label='label') -&gt; None:#['hyperparameters', 'method_class']) -&gt; None:\nself.graph = graph\nself.matched_label = matched_label\nself.node_match = partial(node_match, matched_labels=[matched_label])\nself.key = int(nx.weisfeiler_lehman_graph_hash(self.graph, node_attr=self.matched_label),16) #hash(tuple(sorted([val for (node, val) in self.graph.degree()])))\n#If hash is different, node is definitely different\n# https://arxiv.org/pdf/2002.06653.pdf\ndef __hash__(self) -&gt; int:\nreturn self.key\n#If hash is same, use __eq__ to know if they are actually different\ndef __eq__(self, other):\nreturn nx.is_isomorphic(self.graph, other.graph, node_match=self.node_match)\n</code></pre>"},{"location":"documentation/tpot2/individual_representations/graph_pipeline_individual/individual/#tpot2.individual_representations.graph_pipeline_individual.individual.create_node","title":"<code>create_node(config_dict)</code>","text":"<p>Takes a config_dict and returns a node with a random method_class and hyperparameters</p> Source code in <code>tpot2/individual_representations/graph_pipeline_individual/individual.py</code> <pre><code>def create_node(config_dict):\n'''\n    Takes a config_dict and returns a node with a random method_class and hyperparameters\n    '''\nmethod_class = random.choice(list(config_dict.keys()))\n#if method_class == GraphIndividual or method_class == 'Recursive':\nif method_class == 'Recursive':\nnode = GraphIndividual(**config_dict[method_class])\nelse:\nif isinstance(config_dict[method_class], dict):\nhyperparameters = config_dict[method_class]\nelse: \nhyperparameters = config_dict[method_class](config.hyperparametersuggestor)\nnode = NodeLabel(\nmethod_class=method_class,\nhyperparameters=hyperparameters)\nreturn node\n</code></pre>"},{"location":"documentation/tpot2/individual_representations/graph_pipeline_individual/optuna_optimize/","title":"Optuna optimize","text":""},{"location":"documentation/tpot2/individual_representations/graph_pipeline_individual/templates/","title":"Templates","text":""},{"location":"documentation/tpot2/individual_representations/subset_selector/subsetselector/","title":"Subsetselector","text":""},{"location":"documentation/tpot2/objectives/average_path_length_objective/","title":"Average path length objective","text":""},{"location":"documentation/tpot2/objectives/complexity_objective/","title":"Complexity objective","text":""},{"location":"documentation/tpot2/objectives/number_of_leaves_scorer/","title":"Number of leaves scorer","text":""},{"location":"documentation/tpot2/objectives/number_of_nodes_objective/","title":"Number of nodes objective","text":""},{"location":"documentation/tpot2/selectors/lexicase_selection/","title":"Lexicase selection","text":""},{"location":"documentation/tpot2/selectors/lexicase_selection/#tpot2.selectors.lexicase_selection.lexicase_selection","title":"<code>lexicase_selection(scores, k, n_parents=1)</code>","text":"<p>Select the best individual according to Lexicase Selection, k times.  The returned list contains the indices of the chosen individuals. :param scores: The score matrix, where rows the individulas and the columns are the corresponds to scores on different objectives. :returns: A list of indices of selected individuals. This function uses the :func:<code>~random.choice</code> function from the python base :mod:<code>random</code> module.</p> Source code in <code>tpot2/selectors/lexicase_selection.py</code> <pre><code>def lexicase_selection(scores, k, n_parents=1,):\n\"\"\"Select the best individual according to Lexicase Selection, *k* times. \n    The returned list contains the indices of the chosen *individuals*.\n    :param scores: The score matrix, where rows the individulas and the columns are the corresponds to scores on different objectives.\n    :returns: A list of indices of selected individuals.\n    This function uses the :func:`~random.choice` function from the python base\n    :mod:`random` module.\n    \"\"\"\nchosen =[]\nfor i in range(k*n_parents):\ncandidates = list(range(len(scores)))\ncases = list(range(len(scores[0])))\nrandom.shuffle(cases)\nwhile len(cases) &gt; 0 and len(candidates) &gt; 1:\nbest_val_for_case = max(scores[candidates,cases[0]])\ncandidates = [x for x in candidates if scores[x, cases[0]] == best_val_for_case]\ncases.pop(0)\nchosen.append(random.choice(candidates))\nreturn np.reshape(chosen, (k, n_parents))\n</code></pre>"},{"location":"documentation/tpot2/selectors/max_weighted_average_selector/","title":"Max weighted average selector","text":""},{"location":"documentation/tpot2/selectors/nsgaii/","title":"Nsgaii","text":""},{"location":"documentation/tpot2/selectors/nsgaii/#tpot2.selectors.nsgaii.dominates","title":"<code>dominates(list1, list2)</code>","text":"<p>returns true is all values in list1 are not strictly worse than list2 AND at least one item in list1 is better than list2</p> Source code in <code>tpot2/selectors/nsgaii.py</code> <pre><code>def dominates(list1, list2):\n\"\"\"\n    returns true is all values in list1 are not strictly worse than list2 AND at least one item in list1 is better than list2\n    \"\"\"\nreturn all(list1[i] &gt;= list2[i] for i in range(len(list1))) and any(list1[i] &gt; list2[i] for i in range(len(list1)))\n</code></pre>"},{"location":"documentation/tpot2/selectors/nsgaii/#tpot2.selectors.nsgaii.nondominated_sorting","title":"<code>nondominated_sorting(matrix)</code>","text":"<p>Returns the indexes of the matrix  bigger is better</p> Source code in <code>tpot2/selectors/nsgaii.py</code> <pre><code>def nondominated_sorting(matrix):\n\"\"\"\n    Returns the indexes of the matrix \n    bigger is better\n    \"\"\"\n# Initialize the front list and the rank list\n# Initialize the current front\nfronts = {0:set()}\n# Initialize the list of dominated points\ndominated = [set() for _ in range(len(matrix))] #si the set of solutions which solution i dominates\n# Initialize the list of points that dominate the current point\ndominating = [0 for _ in range(len(matrix))] #ni the number of solutions that denominate solution i\n# Iterate over all points\nfor p, p_scores in enumerate(matrix):\n# Iterate over all other points\nfor q, q_scores in enumerate(matrix):\n# If the current point dominates the other point, increment the count of points dominated by the current point\nif dominates(p_scores, q_scores):\ndominated[p].add(q)\n# If the current point is dominated by the other point, add it to the list of dominated points\nelif dominates(q_scores, p_scores):\ndominating[p] += 1\nif dominating[p] == 0:\nfronts[0].add(p)\ni=0\n# Iterate until all points have been added to a front\nwhile len(fronts[i]) &gt; 0:\nH = set()\nfor p in fronts[i]:\nfor q in dominated[p]:\ndominating[q] -= 1\nif dominating[q] == 0:\nH.add(q)\ni += 1\nfronts[i] = H\nreturn [fronts[j] for j in range(i)]\n</code></pre>"},{"location":"documentation/tpot2/selectors/random_selector/","title":"Random selector","text":""},{"location":"documentation/tpot2/selectors/tournament_selection/","title":"Tournament selection","text":""},{"location":"documentation/tpot2/selectors/tournament_selection/#tpot2.selectors.tournament_selection.tournament_selection","title":"<code>tournament_selection(scores, k, n_parents=1, tournament_size=2, score_index=0)</code>","text":"<p>Select the best individual among tournsize randomly chosen individuals, k times. The returned list contains the indices of the chosen individuals. :param scores: The score matrix, where rows the individulas and the columns are the corresponds to scores on different objectives. :param k: The number of individuals to select. :param tournsize: The number of individuals participating in each tournament. :param score_index: The number of individuals participating in each tournament. :returns: A list of indices of selected individuals. This function uses the :func:<code>~random.choice</code> function from the python base :mod:<code>random</code> module.</p> Source code in <code>tpot2/selectors/tournament_selection.py</code> <pre><code>def tournament_selection(scores, k, n_parents=1, tournament_size=2, score_index=0):\n\"\"\"Select the best individual among *tournsize* randomly chosen\n    individuals, *k* times. The returned list contains the indices of the chosen *individuals*.\n    :param scores: The score matrix, where rows the individulas and the columns are the corresponds to scores on different objectives.\n    :param k: The number of individuals to select.\n    :param tournsize: The number of individuals participating in each tournament.\n    :param score_index: The number of individuals participating in each tournament.\n    :returns: A list of indices of selected individuals.\n    This function uses the :func:`~random.choice` function from the python base\n    :mod:`random` module.\n    \"\"\"\nif isinstance(score_index,int):\nkey=lambda x:x[1][score_index]\nelif score_index == \"average\":\nkey=lambda x:np.mean(x[1])\nchosen = []\nfor i in range(k*n_parents):\naspirants_idx =[random.randrange(len(scores)) for i in range(tournament_size)]\naspirants  = list(zip(aspirants_idx, scores[aspirants_idx])) # Zip indices and elements together\nchosen.append(max(aspirants, key=key)[0]) # Retrun the index of the maximum element\nreturn np.reshape(chosen, (k, n_parents))\n</code></pre>"},{"location":"documentation/tpot2/selectors/tournament_selection_dominated/","title":"Tournament selection dominated","text":""},{"location":"documentation/tpot2/selectors/tournament_selection_dominated/#tpot2.selectors.tournament_selection_dominated.tournament_selection_dominated","title":"<code>tournament_selection_dominated(scores, k, n_parents=2)</code>","text":"<p>Select the best individual among tournsize randomly chosen individuals, k times. The returned list contains the indices of the chosen individuals. :param scores: The score matrix, where rows the individulas and the columns are the corresponds to scores on different objectives. :param k: The number of individuals to select. :param tournsize: The number of individuals participating in each tournament. :param score_index: The number of individuals participating in each tournament. :returns: A list of indices of selected individuals. This function uses the :func:<code>~random.choice</code> function from the python base :mod:<code>random</code> module.</p> Source code in <code>tpot2/selectors/tournament_selection_dominated.py</code> <pre><code>def tournament_selection_dominated(scores, k, n_parents=2):\n\"\"\"Select the best individual among *tournsize* randomly chosen\n    individuals, *k* times. The returned list contains the indices of the chosen *individuals*.\n    :param scores: The score matrix, where rows the individulas and the columns are the corresponds to scores on different objectives.\n    :param k: The number of individuals to select.\n    :param tournsize: The number of individuals participating in each tournament.\n    :param score_index: The number of individuals participating in each tournament.\n    :returns: A list of indices of selected individuals.\n    This function uses the :func:`~random.choice` function from the python base\n    :mod:`random` module.\n    \"\"\"\npareto_fronts = nondominated_sorting(scores)\n# chosen = list(itertools.chain.from_iterable(fronts))\n# if len(chosen) &gt;= k:\n#     return chosen[0:k]\ncrowding_dict = {}\nchosen = []\ncurrent_front_number = 0\nwhile current_front_number &lt; len(pareto_fronts):\ncurrent_front = np.array(list(pareto_fronts[current_front_number]))\nfront_scores = [scores[i] for i in current_front]\ncrowding_distances = crowding_distance(front_scores)\nfor i, crowding in zip(current_front,crowding_distances):\ncrowding_dict[i] = crowding\ncurrent_front_number += 1\nchosen = []\nfor i in range(k*n_parents):\nasp1 = random.randrange(len(scores))\nasp2 = random.randrange(len(scores))\nif dominates(scores[asp1], scores[asp2]):\nchosen.append(asp1)\nelif dominates(scores[asp2], scores[asp1]):\nchosen.append(asp2)\nelif crowding_dict[asp1] &gt; crowding_dict[asp2]:\nchosen.append(asp1)\nelif crowding_dict[asp1] &lt; crowding_dict[asp2]:\nchosen.append(asp2)\nelse:\nchosen.append(random.choice([asp1,asp2]))\nreturn np.reshape(chosen, (k, n_parents))\n</code></pre>"},{"location":"documentation/tpot2/tpot_estimator/cross_val_utils/","title":"Cross val utils","text":""},{"location":"documentation/tpot2/tpot_estimator/estimator/","title":"Estimator","text":""},{"location":"documentation/tpot2/tpot_estimator/estimator/#tpot2.tpot_estimator.estimator.TPOTEstimator","title":"<code>TPOTEstimator</code>","text":"<p>         Bases: <code>BaseEstimator</code></p> Source code in <code>tpot2/tpot_estimator/estimator.py</code> <pre><code>class TPOTEstimator(BaseEstimator):\ndef __init__(self,  scorers, \nscorers_weights,\nclassification,\ncv = 5,\nother_objective_functions=[],\nother_objective_functions_weights = [],\nobjective_function_names = None,\nbigger_is_better = True,\nmax_size = np.inf, \nlinear_pipeline = False,\nroot_config_dict= 'Auto',\ninner_config_dict=[\"selectors\", \"transformers\"],\nleaf_config_dict= None,                        \ncross_val_predict_cv = 0,\ncategorical_features = None,\nsubsets = None,\nmemory = None,\npreprocessing = False,\npopulation_size = 50,\ninitial_population_size = None,\npopulation_scaling = .5, \ngenerations_until_end_population = 1,  \ngenerations = 50,\nmax_time_seconds=float('inf'), \nmax_eval_time_seconds=60*10, \nvalidation_strategy = \"none\",\nvalidation_fraction = .2,\n#early stopping parameters \nearly_stop = None,\nscorers_early_stop_tol = 0.001,\nother_objectives_early_stop_tol =None,\nthreshold_evaluation_early_stop = None, \nthreshold_evaluation_scaling = .5,\nselection_evaluation_early_stop = None, \nselection_evaluation_scaling = .5, \nmin_history_threshold = 20,\n#evolver parameters\nsurvival_percentage = 1,\ncrossover_probability=.2,\nmutate_probability=.7,\nmutate_then_crossover_probability=.05,\ncrossover_then_mutate_probability=.05,\nn_parents = 2,\nsurvival_selector = survival_select_NSGA2,\nparent_selector = tournament_selection_dominated,\n#budget parameters\nbudget_range = None,\nbudget_scaling = .5,\ngenerations_until_end_budget = 1,  \nstepwise_steps = 5,\noptuna_optimize_pareto_front = False,\noptuna_optimize_pareto_front_trials = 100,\noptuna_optimize_pareto_front_timeout = 60*10,\noptuna_storage = \"sqlite:///optuna.db\",\n#dask parameters\nn_jobs=1,\nmemory_limit = \"4GB\",\nclient = None,\nprocesses = True,\n#debugging and logging parameters\nwarm_start = False,\nsubset_column = None,\nperiodic_checkpoint_folder = None, \ncallback = None,\nverbose = 0,\nscatter = True,\n):\n'''\n        An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n        Parameters\n        ----------\n        scorers : (list, scorer)\n            A scorer or list of scorers to be used in the cross-validation process. \n            see https://scikit-learn.org/stable/modules/model_evaluation.html\n        scorers_weights : list\n            A list of weights to be applied to the scorers during the optimization process.\n        classification : bool\n            If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n            Used to determine the CV strategy.\n        cv : int, cross-validator\n            - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n            - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n                - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n        other_objective_functions : list, default=[tpot2.objectives.estimator_objective_functions.average_path_length_objective]\n            A list of other objective functions to apply to the pipeline.\n        other_objective_functions_weights : list, default=[-1]\n            A list of weights to be applied to the other objective functions.\n        objective_function_names : list, default=None\n            A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n        max_size : int, default=np.inf\n            The maximum number of nodes of the pipelines to be generated.\n        linear_pipeline : bool, default=False\n            If True, the pipelines generated will be linear. If False, the pipelines generated will be directed acyclic graphs.\n        root_config_dict : dict, default='auto'\n            The configuration dictionary to use for the root node of the model.\n            If 'auto', will use \"classifiers\" if classification=True, else \"regressors\".\n            - 'selectors' : A selection of sklearn Selector methods.\n            - 'classifiers' : A selection of sklearn Classifier methods.\n            - 'regressors' : A selection of sklearn Regressor methods.\n            - 'transformers' : A selection of sklearn Transformer methods.\n            - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n            - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n            - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                        Subsets are set with the subsets parameter.\n            - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n            - 'MDR' : Includes MDR.\n            - 'ContinuousMDR' : Includes ContinuousMDR.\n            - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n            - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n            - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n        inner_config_dict : dict, default=[\"selectors\", \"transformers\"]\n            The configuration dictionary to use for the inner nodes of the model generation.\n            Default [\"selectors\", \"transformers\"]\n            - 'selectors' : A selection of sklearn Selector methods.\n            - 'classifiers' : A selection of sklearn Classifier methods.\n            - 'regressors' : A selection of sklearn Regressor methods.\n            - 'transformers' : A selection of sklearn Transformer methods.\n            - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n            - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n            - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                        Subsets are set with the subsets parameter.\n            - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n            - 'MDR' : Includes MDR.\n            - 'ContinuousMDR' : Includes ContinuousMDR.\n            - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n            - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n            - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n            - None : If None and max_depth&gt;1, the root_config_dict will be used for the inner nodes as well.\n        leaf_config_dict : dict, default=None \n            The configuration dictionary to use for the leaf node of the model. If set, leaf nodes must be from this dictionary.\n            Otherwise leaf nodes will be generated from the root_config_dict. \n            Default None\n            - 'selectors' : A selection of sklearn Selector methods.\n            - 'classifiers' : A selection of sklearn Classifier methods.\n            - 'regressors' : A selection of sklearn Regressor methods.\n            - 'transformers' : A selection of sklearn Transformer methods.\n            - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n            - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n            - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                        Subsets are set with the subsets parameter.\n            - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n            - 'MDR' : Includes MDR.\n            - 'ContinuousMDR' : Includes ContinuousMDR.\n            - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n            - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n            - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n            - None : If None, a leaf will not be required (i.e. the pipeline can be a single root node). Leaf nodes will be generated from the inner_config_dict.\n        cross_val_predict_cv : int, default=0\n            Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n            - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n            - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset. \n                    However, the output to the next node will come from cross_val_predict with the specified number of folds.\n        categorical_features: list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n            - None : If None, TPOT2 will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n            - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n        subsets : str or list, default=None\n            Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.\n            - str : If a string, it is assumed to be a path to a csv file with the subsets. \n                The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n            - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets.\n            - None : If None, each column will be treated as a subset. One column will be selected per subset.\n            If subsets is None, each column will be treated as a subset. One column will be selected per subset.\n        memory: Memory object or string, default=None\n            If supplied, pipeline will cache each transformer after calling fit. This feature\n            is used to avoid computing the fit transformers within a pipeline if the parameters\n            and input data are identical with another fitted pipeline during optimization process.\n            - String 'auto':\n                TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n            - String path of a caching directory\n                TPOT uses memory caching with the provided directory and TPOT does NOT clean\n                the caching directory up upon shutdown. If the directory does not exist, TPOT will\n                create it.\n            - Memory object:\n                TPOT uses the instance of joblib.Memory for memory caching,\n                and TPOT does NOT clean the caching directory up upon shutdown.\n            - None:\n                TPOT does not use memory caching.\n        preprocessing : bool or BaseEstimator/Pipeline, \n            EXPERIMENTAL\n            A pipeline that will be used to preprocess the data before CV.\n            - bool : If True, will use a default preprocessing pipeline.\n            - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n        population_size : int, default=50\n            Size of the population\n        initial_population_size : int, default=None\n            Size of the initial population. If None, population_size will be used.\n        population_scaling : int, default=0.5\n            Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        generations_until_end_population : int, default=1  \n            Number of generations until the population size reaches population_size            \n        generations : int, default=50\n            Number of generations to run\n        max_time_seconds : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n        max_eval_time_seconds : float, default=60*5\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n        validation_strategy : str, default='none'\n            EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT2 may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n            - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n            - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. \n            - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. \n            - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n        validation_fraction : float, default=0.2\n          EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n        early_stop : int, default=None\n            Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered.\n        scorers_early_stop_tol : \n            -list of floats\n                list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int \n                If an int is given, it will be used as the tolerance for all objectives\n        other_objectives_early_stop_tol : \n            -list of floats\n                list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int \n                If an int is given, it will be used as the tolerance for all objectives\n        threshold_evaluation_early_stop : list [start, end], default=None\n            starting and ending percentile to use as a threshold for the evaluation early stopping.\n            Values between 0 and 100.\n        threshold_evaluation_scaling : float [0,inf), default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n        selection_evaluation_early_stop : list, default=None\n            A lower and upper percent of the population size to select each round of CV.\n            Values between 0 and 1.\n        selection_evaluation_scaling : float, default=0.5 \n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.    \n        min_history_threshold : int, default=0\n            The minimum number of previous scores needed before using threshold early stopping.\n        evolver : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver), default=eaNSGA2_Evolver\n            The evolver to use for the optimization process. See tpot2.evolutionary_algorithms\n            - type : an type or subclass of a BaseEvolver\n            - \"nsga2\" : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver\n        survival_percentage : float, default=1\n            Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive. \n            For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.\n        crossover_probability : float, default=.2\n            Probability of generating a new individual by crossover between two individuals.\n        mutate_probability : float, default=.7\n            Probability of generating a new individual by crossover between one individuals.\n        mutate_then_crossover_probability : float, default=.05\n            Probability of generating a new individual by mutating two individuals followed by crossover.\n        crossover_then_mutate_probability : float, default=.05\n            Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n        n_parents : int, default=2\n            Number of parents to use for crossover. Must be greater than 1.\n        survival_selector : function, default=survival_select_NSGA2\n            Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n            Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n        parent_selector : function, default=parent_select_NSGA2\n            Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n        budget_range : list [start, end], default=None\n            A starting and ending budget to use for the budget scaling.\n        budget_scaling float : [0,1], default=0.5\n            A scaling factor to use when determining how fast we move the budget from the start to end budget.\n        generations_until_end_budget : int, default=1\n            The number of generations to run before reaching the max budget.\n        stepwise_steps : int, default=1\n            The number of staircase steps to take when scaling the budget and population size.\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n        memory_limit : str, default=\"4GB\"\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit. \n        processes : bool, default=True\n            If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.\n            True seems to perform better. However, False is required for interactive debugging.\n        warm_start : bool, default=False\n            If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n        subset_column : str or int, default=None\n            EXPERIMENTAL The column to use for the subset selection. Must also pass in unique_subset_values to GraphIndividual to function.\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n        callback : tpot2.CallBackInterface, default=None\n            Callback object. Not implemented\n        verbose : int, default=1 \n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n            6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_seconds in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n        Attributes\n        ----------\n        fitted_pipeline_ : GraphPipeline\n            A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n        evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run. \n            Columns: \n            - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n            - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n            - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n            - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n            - Generation : The generation the pipeline first appeared. \n            - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual. \n                            To save on computational time, the best frontier is updated iteratively each generation. \n                            The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                            All other pipelines are set to NaN. \n            - Instance\t: The unfitted GraphPipeline BaseEstimator. \n            - *validation objective functions : Objective function scores evaluated on the validation set.\n            - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n        pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n        '''\n# sklearn BaseEstimator must have a corresponding attribute for each parameter.\n# These should not be modified once set.\nself.scorers = scorers\nself.scorers_weights = scorers_weights\nself.classification = classification\nself.cv = cv\nself.other_objective_functions = other_objective_functions\nself.other_objective_functions_weights = other_objective_functions_weights\nself.objective_function_names = objective_function_names\nself.bigger_is_better = bigger_is_better\nself.max_size = max_size\nself.linear_pipeline = linear_pipeline\nself.root_config_dict= root_config_dict\nself.inner_config_dict= inner_config_dict\nself.leaf_config_dict= leaf_config_dict\nself.cross_val_predict_cv = cross_val_predict_cv\nself.categorical_features = categorical_features\nself.subsets = subsets\nself.memory = memory\nself.preprocessing = preprocessing\nself.validation_strategy = validation_strategy\nself.validation_fraction = validation_fraction\nself.population_size = population_size\nself.initial_population_size = initial_population_size\nself.population_scaling = population_scaling\nself.generations_until_end_population = generations_until_end_population\nself.generations = generations\nself.early_stop = early_stop\nself.scorers_early_stop_tol = scorers_early_stop_tol\nself.other_objectives_early_stop_tol = other_objectives_early_stop_tol\nself.max_time_seconds = max_time_seconds \nself.max_eval_time_seconds = max_eval_time_seconds\nself.n_jobs= n_jobs\nself.memory_limit = memory_limit\nself.client = client\nself.survival_percentage = survival_percentage\nself.crossover_probability = crossover_probability\nself.mutate_probability = mutate_probability\nself.mutate_then_crossover_probability= mutate_then_crossover_probability\nself.crossover_then_mutate_probability= crossover_then_mutate_probability\nself.survival_selector=survival_selector\nself.parent_selector=parent_selector\nself.budget_range = budget_range\nself.budget_scaling = budget_scaling\nself.generations_until_end_budget = generations_until_end_budget\nself.stepwise_steps = stepwise_steps\nself.threshold_evaluation_early_stop =threshold_evaluation_early_stop\nself.threshold_evaluation_scaling =  threshold_evaluation_scaling\nself.min_history_threshold = min_history_threshold\nself.selection_evaluation_early_stop = selection_evaluation_early_stop\nself.selection_evaluation_scaling =  selection_evaluation_scaling\nself.warm_start = warm_start\nself.subset_column = subset_column\nself.verbose = verbose\nself.periodic_checkpoint_folder = periodic_checkpoint_folder\nself.callback = callback\nself.processes = processes\nself.scatter = scatter\nself.optuna_optimize_pareto_front = optuna_optimize_pareto_front\nself.optuna_optimize_pareto_front_trials = optuna_optimize_pareto_front_trials\nself.optuna_optimize_pareto_front_timeout = optuna_optimize_pareto_front_timeout\nself.optuna_storage = optuna_storage\n#Initialize other used params\nif self.initial_population_size is None:\nself._initial_population_size = self.population_size\nelse:\nself._initial_population_size = self.initial_population_size\nif isinstance(self.scorers, str):\nself._scorers = [self.scorers]\nelif callable(self.scorers):\nself._scorers = [self.scorers]\nelse:\nself._scorers = self.scorers\nself._scorers = [sklearn.metrics.get_scorer(scoring) for scoring in self._scorers]\nself._scorers_early_stop_tol = self.scorers_early_stop_tol\nself._evolver = tpot2.evolvers.BaseEvolver\nself.objective_function_weights = [*scorers_weights, *other_objective_functions_weights]\nif self.objective_function_names is None:\nobj_names = [f.__name__ for f in other_objective_functions]\nelse:\nobj_names = self.objective_function_names\nself.objective_names = [f._score_func.__name__ if hasattr(f,\"_score_func\") else f.__name__ for f in self._scorers] + obj_names\nif not isinstance(self.other_objectives_early_stop_tol, list):\nself._other_objectives_early_stop_tol = [self.other_objectives_early_stop_tol for _ in range(len(self.other_objective_functions))]\nelse:\nself._other_objectives_early_stop_tol = self.other_objectives_early_stop_tol\nif not isinstance(self._scorers_early_stop_tol, list):\nself._scorers_early_stop_tol = [self._scorers_early_stop_tol for _ in range(len(self._scorers))]\nelse:\nself._scorers_early_stop_tol = self._scorers_early_stop_tol\nself.early_stop_tol = [*self._scorers_early_stop_tol, *self._other_objectives_early_stop_tol]\nself._evolver_instance = None\nself.evaluated_individuals = None\nset_dask_settings()\ndef fit(self, X, y):\nif self.client is not None: #If user passed in a client manually\n_client = self.client\nelse:\nif self.verbose &gt;= 4:\nsilence_logs = 30\nelif self.verbose &gt;=5:\nsilence_logs = 40\nelse:\nsilence_logs = 50\ncluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\nthreads_per_worker=1,\nprocesses=self.processes,\nsilence_logs=silence_logs,\nmemory_limit=self.memory_limit)\n_client = Client(cluster)\nself.evaluated_individuals = None\n#determine validation strategy\nif self.validation_strategy == 'auto':\nnrows = X.shape[0]\nncols = X.shape[1]\nif nrows/ncols &lt; 20:\nvalidation_strategy = 'reshuffled'\nelif nrows/ncols &lt; 100:\nvalidation_strategy = 'split'\nelse:\nvalidation_strategy = 'none'\nelse:\nvalidation_strategy = self.validation_strategy\nif validation_strategy == 'split':\nif self.classification:\nX, X_val, y, y_val = train_test_split(X, y, test_size=self.validation_fraction, stratify=y, random_state=42)\nelse:\nX, X_val, y, y_val = train_test_split(X, y, test_size=self.validation_fraction, random_state=42)\nX_original = X\ny_original = y\nif isinstance(self.cv, int) or isinstance(self.cv, float):\nn_folds = self.cv\nelse:\nn_folds = self.cv.get_n_splits(X, y)\nX, y = remove_underrepresented_classes(X, y, n_folds)\nif self.preprocessing:\n#X = pd.DataFrame(X)\n#TODO: check if there are missing values in X before imputation. If not, don't include imputation in pipeline. Check if there are categorical columns. If not, don't include one hot encoding in pipeline\nif isinstance(X, pd.DataFrame): #pandas dataframe\nif self.categorical_features is not None:\nX[self.categorical_features] = X[self.categorical_features].astype(object)\nself._preprocessing_pipeline = sklearn.pipeline.make_pipeline(tpot2.builtin_modules.ColumnSimpleImputer(\"categorical\", strategy='most_frequent'), #impute categorical columns\ntpot2.builtin_modules.ColumnSimpleImputer(\"numeric\", strategy='mean'),              #impute numeric columns\ntpot2.builtin_modules.ColumnOneHotEncoder(\"categorical\", min_frequency=0.0001))     #one hot encode categorical columns\nX = self._preprocessing_pipeline.fit_transform(X)\nelse:\nif self.categorical_features is not None: #numpy array and categorical columns specified\nself._preprocessing_pipeline = sklearn.pipeline.make_pipeline(tpot2.builtin_modules.ColumnSimpleImputer(self.categorical_features, strategy='most_frequent'),   #impute categorical columns\ntpot2.builtin_modules.ColumnSimpleImputer(\"all\", strategy='mean'),                                      #impute remaining numeric columns\ntpot2.builtin_modules.ColumnOneHotEncoder(self.categorical_features, min_frequency=0.0001))             #one hot encode categorical columns\nelse: #numpy array and no categorical columns specified, just do imputation\nself._preprocessing_pipeline = sklearn.pipeline.make_pipeline(tpot2.builtin_modules.ColumnSimpleImputer(\"all\", strategy='mean'))   \nelse:\nself._preprocessing_pipeline = None\n#_, y = sklearn.utils.check_X_y(X, y, y_numeric=True)\n#Set up the configuation dictionaries and the search spaces\nn_samples= int(math.floor(X.shape[0]/n_folds))\nn_features=X.shape[1]\nif isinstance(X, pd.DataFrame):\nself.feature_names = X.columns\nelse:\nself.feature_names = None\nif self.root_config_dict == 'Auto':\nif self.classification:\nn_classes = len(np.unique(y))\nroot_config_dict = get_configuration_dictionary(\"classifiers\", n_samples, n_features, self.classification, subsets=self.subsets, feature_names=self.feature_names, n_classes=n_classes)\nelse:\nroot_config_dict = get_configuration_dictionary(\"regressors\", n_samples, n_features, self.classification,subsets=self.subsets, feature_names=self.feature_names)\nelse:\nroot_config_dict = get_configuration_dictionary(self.root_config_dict, n_samples, n_features, self.classification, subsets=self.subsets,feature_names=self.feature_names)\ninner_config_dict = get_configuration_dictionary(self.inner_config_dict, n_samples, n_features, self.classification,subsets=self.subsets, feature_names=self.feature_names)\nleaf_config_dict = get_configuration_dictionary(self.leaf_config_dict, n_samples, n_features, self.classification, subsets=self.subsets, feature_names=self.feature_names)\n#check if self.cv is a number\nif isinstance(self.cv, int) or isinstance(self.cv, float):\nif self.classification:\nself.cv_gen = sklearn.model_selection.StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=42)\nelse:\nself.cv_gen = sklearn.model_selection.KFold(n_splits=self.cv, shuffle=True, random_state=42)\nelse:\nself.cv_gen = sklearn.model_selection.check_cv(self.cv, y, classifier=self.classification)\ndef objective_function(pipeline_individual, \nX, \ny,\nis_classification=self.classification,\nscorers= self._scorers, \ncv=self.cv_gen, \nother_objective_functions=self.other_objective_functions,\nmemory=self.memory, \ncross_val_predict_cv=self.cross_val_predict_cv, \nsubset_column=self.subset_column, \n**kwargs): \nreturn objective_function_generator(\npipeline_individual,\nX, \ny, \nis_classification=is_classification,\nscorers= scorers, \ncv=cv, \nother_objective_functions=other_objective_functions,\nmemory=memory, \ncross_val_predict_cv=cross_val_predict_cv, \nsubset_column=subset_column,\n**kwargs,\n)\nself.individual_generator_instance = tpot2.individual_representations.graph_pipeline_individual.estimator_graph_individual_generator(   \ninner_config_dict=inner_config_dict,\nroot_config_dict=root_config_dict,\nleaf_config_dict=leaf_config_dict,\nmax_size = self.max_size,\nlinear_pipeline=self.linear_pipeline,\n)\nif self.threshold_evaluation_early_stop is not None or self.selection_evaluation_early_stop is not None:\nevaluation_early_stop_steps = self.cv\nelse:\nevaluation_early_stop_steps = None\nif self.scatter:\nX_future = _client.scatter(X)\ny_future = _client.scatter(y)\nelse:\nX_future = X\ny_future = y\n#If warm start and we have an evolver instance, use the existing one\nif not(self.warm_start and self._evolver_instance is not None):\nself._evolver_instance = self._evolver(   individual_generator=self.individual_generator_instance, \nobjective_functions= [objective_function],\nobjective_function_weights = self.objective_function_weights,\nobjective_names=self.objective_names,\nbigger_is_better = self.bigger_is_better,\npopulation_size= self.population_size,\ngenerations=self.generations,\ninitial_population_size = self._initial_population_size,\nn_jobs=self.n_jobs,\nverbose = self.verbose,\nmax_time_seconds =      self.max_time_seconds ,\nmax_eval_time_seconds = self.max_eval_time_seconds,\nperiodic_checkpoint_folder = self.periodic_checkpoint_folder,\nthreshold_evaluation_early_stop = self.threshold_evaluation_early_stop,\nthreshold_evaluation_scaling =  self.threshold_evaluation_scaling,\nmin_history_threshold = self.min_history_threshold,\nselection_evaluation_early_stop = self.selection_evaluation_early_stop,\nselection_evaluation_scaling =  self.selection_evaluation_scaling,\nevaluation_early_stop_steps = evaluation_early_stop_steps,\nearly_stop_tol = self.early_stop_tol,\nearly_stop= self.early_stop,\nbudget_range = self.budget_range,\nbudget_scaling = self.budget_scaling,\ngenerations_until_end_budget = self.generations_until_end_budget,\npopulation_scaling = self.population_scaling,\ngenerations_until_end_population = self.generations_until_end_population,\nstepwise_steps = self.stepwise_steps,\nclient = _client,\nobjective_kwargs = {\"X\": X_future, \"y\": y_future},\nsurvival_selector=self.survival_selector,\nparent_selector=self.parent_selector,\nsurvival_percentage = self.survival_percentage,\ncrossover_probability = self.crossover_probability,\nmutate_probability = self.mutate_probability,\nmutate_then_crossover_probability= self.mutate_then_crossover_probability,\ncrossover_then_mutate_probability= self.crossover_then_mutate_probability,\n)\nself._evolver_instance.optimize()\n#self._evolver_instance.population.update_pareto_fronts(self.objective_names, self.objective_function_weights)\nself.make_evaluated_individuals()\nif self.optuna_optimize_pareto_front:\npareto_front_inds = self.pareto_front['Individual'].values\nall_graphs, all_scores = tpot2.individual_representations.graph_pipeline_individual.simple_parallel_optuna(pareto_front_inds,  objective_function, self.objective_function_weights, _client, storage=self.optuna_storage, steps=self.optuna_optimize_pareto_front_trials, verbose=self.verbose, max_eval_time_seconds=self.max_eval_time_seconds, max_time_seconds=self.optuna_optimize_pareto_front_timeout, **{\"X\": X, \"y\": y})\nall_scores = tpot2.utils.eval_utils.process_scores(all_scores, len(self.objective_function_weights))\nif len(all_graphs) &gt; 0:\ndf = pd.DataFrame(np.column_stack((all_graphs, all_scores,np.repeat(\"Optuna\",len(all_graphs)))), columns=[\"Individual\"] + self.objective_names +[\"Parents\"])\nfor obj in self.objective_names:\ndf[obj] = df[obj].apply(convert_to_float)\nself.evaluated_individuals = pd.concat([self.evaluated_individuals, df], ignore_index=True)\nelse:\nprint(\"WARNING NO OPTUNA TRIALS COMPLETED\")\ntpot2.utils.get_pareto_frontier(self.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights, invalid_values=[\"TIMEOUT\",\"INVALID\"])\nif validation_strategy == 'reshuffled':\nbest_pareto_front_idx = list(self.pareto_front.index)\nbest_pareto_front = list(self.pareto_front.loc[best_pareto_front_idx]['Individual'])\n#reshuffle rows\nX, y = sklearn.utils.shuffle(X, y, random_state=1)\nif self.scatter:\nX_future = _client.scatter(X)\ny_future = _client.scatter(y)\nelse:\nX_future = X\ny_future = y\nval_objective_function_list = [lambda   ind, \nX, \ny, \nis_classification=self.classification,\nscorers= self._scorers, \ncv=self.cv_gen, \nother_objective_functions=self.other_objective_functions, \nmemory=self.memory, \ncross_val_predict_cv=self.cross_val_predict_cv, \nsubset_column=self.subset_column, \n**kwargs: objective_function_generator(\nind,\nX,\ny, \nis_classification=is_classification,\nscorers= scorers, \ncv=cv, \nother_objective_functions=other_objective_functions,\nmemory=memory, \ncross_val_predict_cv=cross_val_predict_cv, \nsubset_column=subset_column,\n**kwargs,\n)]\nobjective_kwargs = {\"X\": X_future, \"y\": y_future}\nval_scores = tpot2.utils.eval_utils.parallel_eval_objective_list(\nbest_pareto_front,\nval_objective_function_list, n_jobs=self.n_jobs, verbose=self.verbose, timeout=self.max_eval_time_seconds,n_expected_columns=len(self.objective_names), client=_client, **objective_kwargs)\nval_objective_names = ['validation_'+name for name in self.objective_names]\nself.objective_names_for_selection = val_objective_names\nself.evaluated_individuals.loc[best_pareto_front_idx,val_objective_names] = val_scores\nself.evaluated_individuals[\"Validation_Pareto_Front\"] = tpot2.utils.get_pareto_front(self.evaluated_individuals, val_objective_names, self.objective_function_weights, invalid_values=[\"TIMEOUT\",\"INVALID\"])\nelif validation_strategy == 'split':\nif self.scatter:            \nX_future = _client.scatter(X)\ny_future = _client.scatter(y)\nX_val_future = _client.scatter(X_val)\ny_val_future = _client.scatter(y_val)\nelse:\nX_future = X\ny_future = y\nX_val_future = X_val\ny_val_future = y_val\nobjective_kwargs = {\"X\": X_future, \"y\": y_future, \"X_val\" : X_val_future, \"y_val\":y_val_future }\nbest_pareto_front_idx = list(self.pareto_front.index)\nbest_pareto_front = list(self.pareto_front.loc[best_pareto_front_idx]['Individual'])\nval_objective_function_list = [lambda   ind, \nX, \ny, \nX_val, \ny_val, \nscorers= self._scorers, \nother_objective_functions=self.other_objective_functions, \nmemory=self.memory, \ncross_val_predict_cv=self.cross_val_predict_cv, \nsubset_column=self.subset_column, \n**kwargs: val_objective_function_generator(\nind,\nX,\ny,\nX_val, \ny_val, \nscorers= scorers, \nother_objective_functions=other_objective_functions,\nmemory=memory, \ncross_val_predict_cv=cross_val_predict_cv, \nsubset_column=subset_column,\n**kwargs,\n)]\nval_scores = tpot2.utils.eval_utils.parallel_eval_objective_list(\nbest_pareto_front,\nval_objective_function_list, n_jobs=self.n_jobs, verbose=self.verbose, timeout=self.max_eval_time_seconds,n_expected_columns=len(self.objective_names),client=_client, **objective_kwargs)\nval_objective_names = ['validation_'+name for name in self.objective_names]\nself.objective_names_for_selection = val_objective_names\nself.evaluated_individuals.loc[best_pareto_front_idx,val_objective_names] = val_scores\nself.evaluated_individuals[\"Validation_Pareto_Front\"] = tpot2.utils.get_pareto_front(self.evaluated_individuals, val_objective_names, self.objective_function_weights, invalid_values=[\"TIMEOUT\",\"INVALID\"])\nelse:\nself.objective_names_for_selection = self.objective_names\nval_scores = self.evaluated_individuals[~self.evaluated_individuals[self.objective_names_for_selection].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names_for_selection].astype(float)                                     \nweighted_scores = val_scores*self.objective_function_weights\nif self.bigger_is_better:\nbest_idx = weighted_scores[self.objective_names_for_selection[0]].idxmax()\nelse:\nbest_idx = weighted_scores[self.objective_names_for_selection[0]].idxmin()\nbest_individual = self.evaluated_individuals.loc[best_idx]['Individual']\nself.selected_best_score =  self.evaluated_individuals.loc[best_idx]\nbest_individual_pipeline = best_individual.export_pipeline(memory=self.memory, cross_val_predict_cv=self.cross_val_predict_cv, subset_column=self.subset_column)\nif self.preprocessing:\nself.fitted_pipeline_ = sklearn.pipeline.make_pipeline(sklearn.base.clone(self._preprocessing_pipeline), best_individual_pipeline )\nelse:\nself.fitted_pipeline_ = best_individual_pipeline \nself.fitted_pipeline_.fit(X_original,y_original) #TODO use y_original as well?\nif self.client is None: #no client was passed in\n#close cluster and client\n_client.close()\ncluster.close()\nreturn self\ndef _estimator_has(attr):\n'''Check if we can delegate a method to the underlying estimator.\n        First, we check the first fitted final estimator if available, otherwise we\n        check the unfitted final estimator.\n        '''\nreturn  lambda self: (self.fitted_pipeline_ is not None and\nhasattr(self.fitted_pipeline_, attr)\n)\n@available_if(_estimator_has('predict'))\ndef predict(self, X, **predict_params):\ncheck_is_fitted(self)\n#X = check_array(X)\nreturn self.fitted_pipeline_.predict(X,**predict_params)\n@available_if(_estimator_has('predict_proba'))\ndef predict_proba(self, X, **predict_params):\ncheck_is_fitted(self)\n#X = check_array(X)\nreturn self.fitted_pipeline_.predict_proba(X,**predict_params)\n@available_if(_estimator_has('decision_function'))\ndef decision_function(self, X, **predict_params):\ncheck_is_fitted(self)\n#X = check_array(X)\nreturn self.fitted_pipeline_.decision_function(X,**predict_params)\n@available_if(_estimator_has('transform'))\ndef transform(self, X, **predict_params):\ncheck_is_fitted(self)\n#X = check_array(X)\nreturn self.fitted_pipeline_.transform(X,**predict_params)\n@property\ndef classes_(self):\n\"\"\"The classes labels. Only exist if the last step is a classifier.\"\"\"\nreturn self.fitted_pipeline_.classes_\ndef make_evaluated_individuals(self):\n#check if _evolver_instance exists\nif self.evaluated_individuals is None:\nself.evaluated_individuals  =  self._evolver_instance.population.evaluated_individuals.copy()\nobjects = list(self.evaluated_individuals.index)\nobject_to_int = dict(zip(objects, range(len(objects))))\nself.evaluated_individuals = self.evaluated_individuals.set_index(self.evaluated_individuals.index.map(object_to_int))\nself.evaluated_individuals['Parents'] = self.evaluated_individuals['Parents'].apply(lambda row: convert_parents_tuples_to_integers(row, object_to_int))\nself.evaluated_individuals[\"Instance\"] = self.evaluated_individuals[\"Individual\"].apply(lambda ind: apply_make_pipeline(ind, preprocessing_pipeline=self._preprocessing_pipeline))\nreturn self.evaluated_individuals\n@property\ndef pareto_front(self):\n#check if _evolver_instance exists\nif self.evaluated_individuals is None:\nreturn None\nelse:\nif \"Pareto_Front\" not in self.evaluated_individuals:\nreturn self.evaluated_individuals\nelse:\nreturn self.evaluated_individuals[self.evaluated_individuals[\"Pareto_Front\"]==1]\n</code></pre>"},{"location":"documentation/tpot2/tpot_estimator/estimator/#tpot2.tpot_estimator.estimator.TPOTEstimator.classes_","title":"<code>classes_</code>  <code>property</code>","text":"<p>The classes labels. Only exist if the last step is a classifier.</p>"},{"location":"documentation/tpot2/tpot_estimator/estimator/#tpot2.tpot_estimator.estimator.TPOTEstimator.__init__","title":"<code>__init__(scorers, scorers_weights, classification, cv=5, other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, max_size=np.inf, linear_pipeline=False, root_config_dict='Auto', inner_config_dict=['selectors', 'transformers'], leaf_config_dict=None, cross_val_predict_cv=0, categorical_features=None, subsets=None, memory=None, preprocessing=False, population_size=50, initial_population_size=None, population_scaling=0.5, generations_until_end_population=1, generations=50, max_time_seconds=float('inf'), max_eval_time_seconds=60 * 10, validation_strategy='none', validation_fraction=0.2, early_stop=None, scorers_early_stop_tol=0.001, other_objectives_early_stop_tol=None, threshold_evaluation_early_stop=None, threshold_evaluation_scaling=0.5, selection_evaluation_early_stop=None, selection_evaluation_scaling=0.5, min_history_threshold=20, survival_percentage=1, crossover_probability=0.2, mutate_probability=0.7, mutate_then_crossover_probability=0.05, crossover_then_mutate_probability=0.05, n_parents=2, survival_selector=survival_select_NSGA2, parent_selector=tournament_selection_dominated, budget_range=None, budget_scaling=0.5, generations_until_end_budget=1, stepwise_steps=5, optuna_optimize_pareto_front=False, optuna_optimize_pareto_front_trials=100, optuna_optimize_pareto_front_timeout=60 * 10, optuna_storage='sqlite:///optuna.db', n_jobs=1, memory_limit='4GB', client=None, processes=True, warm_start=False, subset_column=None, periodic_checkpoint_folder=None, callback=None, verbose=0, scatter=True)</code>","text":"<p>An sklearn baseestimator that uses genetic programming to optimize a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>scorers</code> <code>(list, scorer)</code> <p>A scorer or list of scorers to be used in the cross-validation process.  see https://scikit-learn.org/stable/modules/model_evaluation.html</p> required <p>scorers_weights : list     A list of weights to be applied to the scorers during the optimization process.</p> <p>classification : bool     If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.     Used to determine the CV strategy.</p> <p>cv : int, cross-validator     - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.     - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.         - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.</p> <p>other_objective_functions : list, default=[tpot2.objectives.estimator_objective_functions.average_path_length_objective]     A list of other objective functions to apply to the pipeline.</p> <p>other_objective_functions_weights : list, default=[-1]     A list of weights to be applied to the other objective functions.</p> <p>objective_function_names : list, default=None     A list of names to be applied to the objective functions. If None, will use the names of the objective functions.</p> <p>bigger_is_better : bool, default=True     If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <p>max_size : int, default=np.inf     The maximum number of nodes of the pipelines to be generated.</p> <p>linear_pipeline : bool, default=False     If True, the pipelines generated will be linear. If False, the pipelines generated will be directed acyclic graphs.</p> <p>root_config_dict : dict, default='auto'     The configuration dictionary to use for the root node of the model.     If 'auto', will use \"classifiers\" if classification=True, else \"regressors\".     - 'selectors' : A selection of sklearn Selector methods.     - 'classifiers' : A selection of sklearn Classifier methods.     - 'regressors' : A selection of sklearn Regressor methods.     - 'transformers' : A selection of sklearn Transformer methods.     - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.     - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.     - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.                                 Subsets are set with the subsets parameter.     - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.     - 'MDR' : Includes MDR.     - 'ContinuousMDR' : Includes ContinuousMDR.     - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.     - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.     - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.</p> <p>inner_config_dict : dict, default=[\"selectors\", \"transformers\"]     The configuration dictionary to use for the inner nodes of the model generation.     Default [\"selectors\", \"transformers\"]     - 'selectors' : A selection of sklearn Selector methods.     - 'classifiers' : A selection of sklearn Classifier methods.     - 'regressors' : A selection of sklearn Regressor methods.     - 'transformers' : A selection of sklearn Transformer methods.     - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.     - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.     - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.                                 Subsets are set with the subsets parameter.     - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.     - 'MDR' : Includes MDR.     - 'ContinuousMDR' : Includes ContinuousMDR.     - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.     - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.     - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.     - None : If None and max_depth&gt;1, the root_config_dict will be used for the inner nodes as well.</p> <p>leaf_config_dict : dict, default=None      The configuration dictionary to use for the leaf node of the model. If set, leaf nodes must be from this dictionary.     Otherwise leaf nodes will be generated from the root_config_dict.      Default None     - 'selectors' : A selection of sklearn Selector methods.     - 'classifiers' : A selection of sklearn Classifier methods.     - 'regressors' : A selection of sklearn Regressor methods.     - 'transformers' : A selection of sklearn Transformer methods.     - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.     - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.     - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.                                 Subsets are set with the subsets parameter.     - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.     - 'MDR' : Includes MDR.     - 'ContinuousMDR' : Includes ContinuousMDR.     - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.     - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.     - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.     - None : If None, a leaf will not be required (i.e. the pipeline can be a single root node). Leaf nodes will be generated from the inner_config_dict.</p> <p>cross_val_predict_cv : int, default=0     Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.</p> <pre><code>- 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n- &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset. \n        However, the output to the next node will come from cross_val_predict with the specified number of folds.\n</code></pre> <p>categorical_features: list or None     Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.     - None : If None, TPOT2 will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.     - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices</p> <p>subsets : str or list, default=None     Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.     - str : If a string, it is assumed to be a path to a csv file with the subsets.          The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.     - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets.     - None : If None, each column will be treated as a subset. One column will be selected per subset.     If subsets is None, each column will be treated as a subset. One column will be selected per subset.</p> <p>memory: Memory object or string, default=None     If supplied, pipeline will cache each transformer after calling fit. This feature     is used to avoid computing the fit transformers within a pipeline if the parameters     and input data are identical with another fitted pipeline during optimization process.     - String 'auto':         TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.     - String path of a caching directory         TPOT uses memory caching with the provided directory and TPOT does NOT clean         the caching directory up upon shutdown. If the directory does not exist, TPOT will         create it.     - Memory object:         TPOT uses the instance of joblib.Memory for memory caching,         and TPOT does NOT clean the caching directory up upon shutdown.     - None:         TPOT does not use memory caching.</p> <p>preprocessing : bool or BaseEstimator/Pipeline,      EXPERIMENTAL     A pipeline that will be used to preprocess the data before CV.     - bool : If True, will use a default preprocessing pipeline.     - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.</p> <p>population_size : int, default=50     Size of the population</p> <p>initial_population_size : int, default=None     Size of the initial population. If None, population_size will be used.</p> <p>population_scaling : int, default=0.5     Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.</p> <p>generations_until_end_population : int, default=1     Number of generations until the population size reaches population_size            </p> <p>generations : int, default=50     Number of generations to run</p> <p>max_time_seconds : float, default=float(\"inf\")     Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <p>max_eval_time_seconds : float, default=60*5     Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <p>validation_strategy : str, default='none'     EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT2 may overfit the cross validation score. A second validation set can be used to select the final pipeline.     - 'auto' : Automatically determine the validation strategy based on the dataset shape.     - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.      - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.      - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.</p> <p>validation_fraction : float, default=0.2   EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.</p> <p>early_stop : int, default=None     Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered.</p> <p>scorers_early_stop_tol :      -list of floats         list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged         If an index of the list is None, that item will not be used for early stopping     -int          If an int is given, it will be used as the tolerance for all objectives</p> <p>other_objectives_early_stop_tol :      -list of floats         list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged         If an index of the list is None, that item will not be used for early stopping     -int          If an int is given, it will be used as the tolerance for all objectives</p> <p>threshold_evaluation_early_stop : list [start, end], default=None     starting and ending percentile to use as a threshold for the evaluation early stopping.     Values between 0 and 100.</p> <p>threshold_evaluation_scaling : float [0,inf), default=0.5     A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.     Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <p>selection_evaluation_early_stop : list, default=None     A lower and upper percent of the population size to select each round of CV.     Values between 0 and 1.</p> <p>selection_evaluation_scaling : float, default=0.5      A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.     Must be greater than zero. Higher numbers will move the threshold to the end faster.    </p> <p>min_history_threshold : int, default=0     The minimum number of previous scores needed before using threshold early stopping.</p> <p>evolver : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver), default=eaNSGA2_Evolver     The evolver to use for the optimization process. See tpot2.evolutionary_algorithms     - type : an type or subclass of a BaseEvolver     - \"nsga2\" : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver</p> <p>survival_percentage : float, default=1     Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive.      For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.</p> <p>crossover_probability : float, default=.2     Probability of generating a new individual by crossover between two individuals.</p> <p>mutate_probability : float, default=.7     Probability of generating a new individual by crossover between one individuals.</p> <p>mutate_then_crossover_probability : float, default=.05     Probability of generating a new individual by mutating two individuals followed by crossover.</p> <p>crossover_then_mutate_probability : float, default=.05     Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.</p> <p>n_parents : int, default=2     Number of parents to use for crossover. Must be greater than 1.</p> <p>survival_selector : function, default=survival_select_NSGA2     Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.     Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.</p> <p>parent_selector : function, default=parent_select_NSGA2     Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.</p> <p>budget_range : list [start, end], default=None     A starting and ending budget to use for the budget scaling.</p> <p>budget_scaling float : [0,1], default=0.5     A scaling factor to use when determining how fast we move the budget from the start to end budget.</p> <p>generations_until_end_budget : int, default=1     The number of generations to run before reaching the max budget.</p> <p>stepwise_steps : int, default=1     The number of staircase steps to take when scaling the budget and population size.</p> <p>n_jobs : int, default=1     Number of processes to run in parallel.</p> <p>memory_limit : str, default=\"4GB\"     Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <p>client : dask.distributed.Client, default=None     A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit. </p> <p>processes : bool, default=True     If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.     True seems to perform better. However, False is required for interactive debugging.</p> <p>warm_start : bool, default=False     If True, will use the continue the evolutionary algorithm from the last generation of the previous run.</p> <p>subset_column : str or int, default=None     EXPERIMENTAL The column to use for the subset selection. Must also pass in unique_subset_values to GraphIndividual to function.</p> <p>periodic_checkpoint_folder : str, default=None     Folder to save the population to periodically. If None, no periodic saving will be done.     If provided, training will resume from this checkpoint.</p> <p>callback : tpot2.CallBackInterface, default=None     Callback object. Not implemented</p> <p>verbose : int, default=1      How much information to print during the optimization process. Higher values include the information from lower values.     0. nothing     1. progress bar</p> <pre><code>3. best individual\n4. warnings\n&gt;=5. full warnings trace\n6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_seconds in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n</code></pre> <p>Attributes:</p> Name Type Description <code>fitted_pipeline_</code> <code>GraphPipeline</code> <p>A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.</p> <p>evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.      Columns:      - objective functions : The first few columns correspond to the passed in scorers and objective functions     - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.     - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.     - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.     - Generation : The generation the pipeline first appeared.      - Pareto_Front      : The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.                      To save on computational time, the best frontier is updated iteratively each generation.                      The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.                     All other pipelines are set to NaN.      - Instance  : The unfitted GraphPipeline BaseEstimator.      - validation objective functions : Objective function scores evaluated on the validation set.     - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.</p> <p>pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.</p> Source code in <code>tpot2/tpot_estimator/estimator.py</code> <pre><code>def __init__(self,  scorers, \nscorers_weights,\nclassification,\ncv = 5,\nother_objective_functions=[],\nother_objective_functions_weights = [],\nobjective_function_names = None,\nbigger_is_better = True,\nmax_size = np.inf, \nlinear_pipeline = False,\nroot_config_dict= 'Auto',\ninner_config_dict=[\"selectors\", \"transformers\"],\nleaf_config_dict= None,                        \ncross_val_predict_cv = 0,\ncategorical_features = None,\nsubsets = None,\nmemory = None,\npreprocessing = False,\npopulation_size = 50,\ninitial_population_size = None,\npopulation_scaling = .5, \ngenerations_until_end_population = 1,  \ngenerations = 50,\nmax_time_seconds=float('inf'), \nmax_eval_time_seconds=60*10, \nvalidation_strategy = \"none\",\nvalidation_fraction = .2,\n#early stopping parameters \nearly_stop = None,\nscorers_early_stop_tol = 0.001,\nother_objectives_early_stop_tol =None,\nthreshold_evaluation_early_stop = None, \nthreshold_evaluation_scaling = .5,\nselection_evaluation_early_stop = None, \nselection_evaluation_scaling = .5, \nmin_history_threshold = 20,\n#evolver parameters\nsurvival_percentage = 1,\ncrossover_probability=.2,\nmutate_probability=.7,\nmutate_then_crossover_probability=.05,\ncrossover_then_mutate_probability=.05,\nn_parents = 2,\nsurvival_selector = survival_select_NSGA2,\nparent_selector = tournament_selection_dominated,\n#budget parameters\nbudget_range = None,\nbudget_scaling = .5,\ngenerations_until_end_budget = 1,  \nstepwise_steps = 5,\noptuna_optimize_pareto_front = False,\noptuna_optimize_pareto_front_trials = 100,\noptuna_optimize_pareto_front_timeout = 60*10,\noptuna_storage = \"sqlite:///optuna.db\",\n#dask parameters\nn_jobs=1,\nmemory_limit = \"4GB\",\nclient = None,\nprocesses = True,\n#debugging and logging parameters\nwarm_start = False,\nsubset_column = None,\nperiodic_checkpoint_folder = None, \ncallback = None,\nverbose = 0,\nscatter = True,\n):\n'''\n    An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n    Parameters\n    ----------\n    scorers : (list, scorer)\n        A scorer or list of scorers to be used in the cross-validation process. \n        see https://scikit-learn.org/stable/modules/model_evaluation.html\n    scorers_weights : list\n        A list of weights to be applied to the scorers during the optimization process.\n    classification : bool\n        If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n        Used to determine the CV strategy.\n    cv : int, cross-validator\n        - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n        - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n            - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n    other_objective_functions : list, default=[tpot2.objectives.estimator_objective_functions.average_path_length_objective]\n        A list of other objective functions to apply to the pipeline.\n    other_objective_functions_weights : list, default=[-1]\n        A list of weights to be applied to the other objective functions.\n    objective_function_names : list, default=None\n        A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n    max_size : int, default=np.inf\n        The maximum number of nodes of the pipelines to be generated.\n    linear_pipeline : bool, default=False\n        If True, the pipelines generated will be linear. If False, the pipelines generated will be directed acyclic graphs.\n    root_config_dict : dict, default='auto'\n        The configuration dictionary to use for the root node of the model.\n        If 'auto', will use \"classifiers\" if classification=True, else \"regressors\".\n        - 'selectors' : A selection of sklearn Selector methods.\n        - 'classifiers' : A selection of sklearn Classifier methods.\n        - 'regressors' : A selection of sklearn Regressor methods.\n        - 'transformers' : A selection of sklearn Transformer methods.\n        - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n        - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n        - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                    Subsets are set with the subsets parameter.\n        - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n        - 'MDR' : Includes MDR.\n        - 'ContinuousMDR' : Includes ContinuousMDR.\n        - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n        - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n        - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n    inner_config_dict : dict, default=[\"selectors\", \"transformers\"]\n        The configuration dictionary to use for the inner nodes of the model generation.\n        Default [\"selectors\", \"transformers\"]\n        - 'selectors' : A selection of sklearn Selector methods.\n        - 'classifiers' : A selection of sklearn Classifier methods.\n        - 'regressors' : A selection of sklearn Regressor methods.\n        - 'transformers' : A selection of sklearn Transformer methods.\n        - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n        - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n        - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                    Subsets are set with the subsets parameter.\n        - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n        - 'MDR' : Includes MDR.\n        - 'ContinuousMDR' : Includes ContinuousMDR.\n        - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n        - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n        - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n        - None : If None and max_depth&gt;1, the root_config_dict will be used for the inner nodes as well.\n    leaf_config_dict : dict, default=None \n        The configuration dictionary to use for the leaf node of the model. If set, leaf nodes must be from this dictionary.\n        Otherwise leaf nodes will be generated from the root_config_dict. \n        Default None\n        - 'selectors' : A selection of sklearn Selector methods.\n        - 'classifiers' : A selection of sklearn Classifier methods.\n        - 'regressors' : A selection of sklearn Regressor methods.\n        - 'transformers' : A selection of sklearn Transformer methods.\n        - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n        - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n        - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                    Subsets are set with the subsets parameter.\n        - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n        - 'MDR' : Includes MDR.\n        - 'ContinuousMDR' : Includes ContinuousMDR.\n        - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n        - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n        - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n        - None : If None, a leaf will not be required (i.e. the pipeline can be a single root node). Leaf nodes will be generated from the inner_config_dict.\n    cross_val_predict_cv : int, default=0\n        Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n        - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n        - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset. \n                However, the output to the next node will come from cross_val_predict with the specified number of folds.\n    categorical_features: list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n        - None : If None, TPOT2 will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n        - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n    subsets : str or list, default=None\n        Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.\n        - str : If a string, it is assumed to be a path to a csv file with the subsets. \n            The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n        - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets.\n        - None : If None, each column will be treated as a subset. One column will be selected per subset.\n        If subsets is None, each column will be treated as a subset. One column will be selected per subset.\n    memory: Memory object or string, default=None\n        If supplied, pipeline will cache each transformer after calling fit. This feature\n        is used to avoid computing the fit transformers within a pipeline if the parameters\n        and input data are identical with another fitted pipeline during optimization process.\n        - String 'auto':\n            TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n        - String path of a caching directory\n            TPOT uses memory caching with the provided directory and TPOT does NOT clean\n            the caching directory up upon shutdown. If the directory does not exist, TPOT will\n            create it.\n        - Memory object:\n            TPOT uses the instance of joblib.Memory for memory caching,\n            and TPOT does NOT clean the caching directory up upon shutdown.\n        - None:\n            TPOT does not use memory caching.\n    preprocessing : bool or BaseEstimator/Pipeline, \n        EXPERIMENTAL\n        A pipeline that will be used to preprocess the data before CV.\n        - bool : If True, will use a default preprocessing pipeline.\n        - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n    population_size : int, default=50\n        Size of the population\n    initial_population_size : int, default=None\n        Size of the initial population. If None, population_size will be used.\n    population_scaling : int, default=0.5\n        Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n    generations_until_end_population : int, default=1  \n        Number of generations until the population size reaches population_size            \n    generations : int, default=50\n        Number of generations to run\n    max_time_seconds : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n    max_eval_time_seconds : float, default=60*5\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n    validation_strategy : str, default='none'\n        EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT2 may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n        - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n        - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. \n        - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. \n        - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n    validation_fraction : float, default=0.2\n      EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n    early_stop : int, default=None\n        Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered.\n    scorers_early_stop_tol : \n        -list of floats\n            list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int \n            If an int is given, it will be used as the tolerance for all objectives\n    other_objectives_early_stop_tol : \n        -list of floats\n            list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int \n            If an int is given, it will be used as the tolerance for all objectives\n    threshold_evaluation_early_stop : list [start, end], default=None\n        starting and ending percentile to use as a threshold for the evaluation early stopping.\n        Values between 0 and 100.\n    threshold_evaluation_scaling : float [0,inf), default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n    selection_evaluation_early_stop : list, default=None\n        A lower and upper percent of the population size to select each round of CV.\n        Values between 0 and 1.\n    selection_evaluation_scaling : float, default=0.5 \n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.    \n    min_history_threshold : int, default=0\n        The minimum number of previous scores needed before using threshold early stopping.\n    evolver : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver), default=eaNSGA2_Evolver\n        The evolver to use for the optimization process. See tpot2.evolutionary_algorithms\n        - type : an type or subclass of a BaseEvolver\n        - \"nsga2\" : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver\n    survival_percentage : float, default=1\n        Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive. \n        For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.\n    crossover_probability : float, default=.2\n        Probability of generating a new individual by crossover between two individuals.\n    mutate_probability : float, default=.7\n        Probability of generating a new individual by crossover between one individuals.\n    mutate_then_crossover_probability : float, default=.05\n        Probability of generating a new individual by mutating two individuals followed by crossover.\n    crossover_then_mutate_probability : float, default=.05\n        Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n    n_parents : int, default=2\n        Number of parents to use for crossover. Must be greater than 1.\n    survival_selector : function, default=survival_select_NSGA2\n        Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n        Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n    parent_selector : function, default=parent_select_NSGA2\n        Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n    budget_range : list [start, end], default=None\n        A starting and ending budget to use for the budget scaling.\n    budget_scaling float : [0,1], default=0.5\n        A scaling factor to use when determining how fast we move the budget from the start to end budget.\n    generations_until_end_budget : int, default=1\n        The number of generations to run before reaching the max budget.\n    stepwise_steps : int, default=1\n        The number of staircase steps to take when scaling the budget and population size.\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n    memory_limit : str, default=\"4GB\"\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit. \n    processes : bool, default=True\n        If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.\n        True seems to perform better. However, False is required for interactive debugging.\n    warm_start : bool, default=False\n        If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n    subset_column : str or int, default=None\n        EXPERIMENTAL The column to use for the subset selection. Must also pass in unique_subset_values to GraphIndividual to function.\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n    callback : tpot2.CallBackInterface, default=None\n        Callback object. Not implemented\n    verbose : int, default=1 \n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n        6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_seconds in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n    Attributes\n    ----------\n    fitted_pipeline_ : GraphPipeline\n        A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n    evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run. \n        Columns: \n        - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n        - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n        - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n        - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n        - Generation : The generation the pipeline first appeared. \n        - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual. \n                        To save on computational time, the best frontier is updated iteratively each generation. \n                        The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                        All other pipelines are set to NaN. \n        - Instance\t: The unfitted GraphPipeline BaseEstimator. \n        - *validation objective functions : Objective function scores evaluated on the validation set.\n        - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n    pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n    '''\n# sklearn BaseEstimator must have a corresponding attribute for each parameter.\n# These should not be modified once set.\nself.scorers = scorers\nself.scorers_weights = scorers_weights\nself.classification = classification\nself.cv = cv\nself.other_objective_functions = other_objective_functions\nself.other_objective_functions_weights = other_objective_functions_weights\nself.objective_function_names = objective_function_names\nself.bigger_is_better = bigger_is_better\nself.max_size = max_size\nself.linear_pipeline = linear_pipeline\nself.root_config_dict= root_config_dict\nself.inner_config_dict= inner_config_dict\nself.leaf_config_dict= leaf_config_dict\nself.cross_val_predict_cv = cross_val_predict_cv\nself.categorical_features = categorical_features\nself.subsets = subsets\nself.memory = memory\nself.preprocessing = preprocessing\nself.validation_strategy = validation_strategy\nself.validation_fraction = validation_fraction\nself.population_size = population_size\nself.initial_population_size = initial_population_size\nself.population_scaling = population_scaling\nself.generations_until_end_population = generations_until_end_population\nself.generations = generations\nself.early_stop = early_stop\nself.scorers_early_stop_tol = scorers_early_stop_tol\nself.other_objectives_early_stop_tol = other_objectives_early_stop_tol\nself.max_time_seconds = max_time_seconds \nself.max_eval_time_seconds = max_eval_time_seconds\nself.n_jobs= n_jobs\nself.memory_limit = memory_limit\nself.client = client\nself.survival_percentage = survival_percentage\nself.crossover_probability = crossover_probability\nself.mutate_probability = mutate_probability\nself.mutate_then_crossover_probability= mutate_then_crossover_probability\nself.crossover_then_mutate_probability= crossover_then_mutate_probability\nself.survival_selector=survival_selector\nself.parent_selector=parent_selector\nself.budget_range = budget_range\nself.budget_scaling = budget_scaling\nself.generations_until_end_budget = generations_until_end_budget\nself.stepwise_steps = stepwise_steps\nself.threshold_evaluation_early_stop =threshold_evaluation_early_stop\nself.threshold_evaluation_scaling =  threshold_evaluation_scaling\nself.min_history_threshold = min_history_threshold\nself.selection_evaluation_early_stop = selection_evaluation_early_stop\nself.selection_evaluation_scaling =  selection_evaluation_scaling\nself.warm_start = warm_start\nself.subset_column = subset_column\nself.verbose = verbose\nself.periodic_checkpoint_folder = periodic_checkpoint_folder\nself.callback = callback\nself.processes = processes\nself.scatter = scatter\nself.optuna_optimize_pareto_front = optuna_optimize_pareto_front\nself.optuna_optimize_pareto_front_trials = optuna_optimize_pareto_front_trials\nself.optuna_optimize_pareto_front_timeout = optuna_optimize_pareto_front_timeout\nself.optuna_storage = optuna_storage\n#Initialize other used params\nif self.initial_population_size is None:\nself._initial_population_size = self.population_size\nelse:\nself._initial_population_size = self.initial_population_size\nif isinstance(self.scorers, str):\nself._scorers = [self.scorers]\nelif callable(self.scorers):\nself._scorers = [self.scorers]\nelse:\nself._scorers = self.scorers\nself._scorers = [sklearn.metrics.get_scorer(scoring) for scoring in self._scorers]\nself._scorers_early_stop_tol = self.scorers_early_stop_tol\nself._evolver = tpot2.evolvers.BaseEvolver\nself.objective_function_weights = [*scorers_weights, *other_objective_functions_weights]\nif self.objective_function_names is None:\nobj_names = [f.__name__ for f in other_objective_functions]\nelse:\nobj_names = self.objective_function_names\nself.objective_names = [f._score_func.__name__ if hasattr(f,\"_score_func\") else f.__name__ for f in self._scorers] + obj_names\nif not isinstance(self.other_objectives_early_stop_tol, list):\nself._other_objectives_early_stop_tol = [self.other_objectives_early_stop_tol for _ in range(len(self.other_objective_functions))]\nelse:\nself._other_objectives_early_stop_tol = self.other_objectives_early_stop_tol\nif not isinstance(self._scorers_early_stop_tol, list):\nself._scorers_early_stop_tol = [self._scorers_early_stop_tol for _ in range(len(self._scorers))]\nelse:\nself._scorers_early_stop_tol = self._scorers_early_stop_tol\nself.early_stop_tol = [*self._scorers_early_stop_tol, *self._other_objectives_early_stop_tol]\nself._evolver_instance = None\nself.evaluated_individuals = None\nset_dask_settings()\n</code></pre>"},{"location":"documentation/tpot2/tpot_estimator/estimator_utils/","title":"Estimator utils","text":""},{"location":"documentation/tpot2/tpot_estimator/steady_state_estimator/","title":"Steady state estimator","text":""},{"location":"documentation/tpot2/tpot_estimator/steady_state_estimator/#tpot2.tpot_estimator.steady_state_estimator.TPOTEstimatorSteadyState","title":"<code>TPOTEstimatorSteadyState</code>","text":"<p>         Bases: <code>BaseEstimator</code></p> Source code in <code>tpot2/tpot_estimator/steady_state_estimator.py</code> <pre><code>class TPOTEstimatorSteadyState(BaseEstimator):\ndef __init__(self,  scorers= [], \nscorers_weights = [],\nclassification = False,\ncv = 5,\nother_objective_functions=[], #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = [],\nobjective_function_names = None,\nbigger_is_better = True,\nmax_size = np.inf, \nlinear_pipeline = False,\nroot_config_dict= 'Auto',\ninner_config_dict=[\"selectors\", \"transformers\"],\nleaf_config_dict= None,                        \ncross_val_predict_cv = 0,\ncategorical_features = None,\nsubsets = None,\nmemory = None,\npreprocessing = False,  \nvalidation_strategy = \"none\",\nvalidation_fraction = .2,\ninitial_population_size = 50,\npopulation_size = 50,\nmax_evaluated_individuals = None,\nearly_stop = None,\nscorers_early_stop_tol = 0.001,\nother_objectives_early_stop_tol = None,\nmax_time_seconds=float('inf'), \nmax_eval_time_seconds=60*10, \nn_jobs=1,\nmemory_limit = \"4GB\",\nclient = None,\ncrossover_probability=.2,\nmutate_probability=.7,\nmutate_then_crossover_probability=.05,\ncrossover_then_mutate_probability=.05,\nsurvival_selector = survival_select_NSGA2,\nparent_selector = tournament_selection_dominated,\nbudget_range = None,\nbudget_scaling = .5,\nindividuals_until_end_budget = 1,  \nstepwise_steps = 5,\nwarm_start = False,\nsubset_column = None,\nverbose = 0,\nperiodic_checkpoint_folder = None, \ncallback = None,\nprocesses = True,\nscatter = True,\noptuna_optimize_pareto_front = False,\noptuna_optimize_pareto_front_trials = 100,\noptuna_optimize_pareto_front_timeout = 60*10,\noptuna_storage = \"sqlite:///optuna.db\",\n):\n'''\n        An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n        Parameters\n        ----------\n        scorers : (list, scorer)\n            A scorer or list of scorers to be used in the cross-validation process. \n            see https://scikit-learn.org/stable/modules/model_evaluation.html\n        scorers_weights : list\n            A list of weights to be applied to the scorers during the optimization process.\n        classification : bool\n            If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n            Used to determine the CV strategy.\n        cv : int, cross-validator\n            - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n            - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n        other_objective_functions : list, default=[tpot2.objectives.estimator_objective_functions.average_path_length_objective]\n            A list of other objective functions to apply to the pipeline.\n        other_objective_functions_weights : list, default=[-1]\n            A list of weights to be applied to the other objective functions.\n        objective_function_names : list, default=None\n            A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n        max_size : int, default=np.inf\n            The maximum number of nodes of the pipelines to be generated.\n        linear_pipeline : bool, default=False\n            If True, the pipelines generated will be linear. If False, the pipelines generated will be directed acyclic graphs.\n        root_config_dict : dict, default='auto'\n            The configuration dictionary to use for the root node of the model.\n            If 'auto', will use \"classifiers\" if classification=True, else \"regressors\".\n            - 'selectors' : A selection of sklearn Selector methods.\n            - 'classifiers' : A selection of sklearn Classifier methods.\n            - 'regressors' : A selection of sklearn Regressor methods.\n            - 'transformers' : A selection of sklearn Transformer methods.\n            - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n            - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n            - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                        Subsets are set with the subsets parameter.\n            - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n            - 'MDR' : Includes MDR.\n            - 'ContinuousMDR' : Includes ContinuousMDR.\n            - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n            - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n            - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n        inner_config_dict : dict, default=[\"selectors\", \"transformers\"]\n            The configuration dictionary to use for the inner nodes of the model generation.\n            Default [\"selectors\", \"transformers\"]\n            - 'selectors' : A selection of sklearn Selector methods.\n            - 'classifiers' : A selection of sklearn Classifier methods.\n            - 'regressors' : A selection of sklearn Regressor methods.\n            - 'transformers' : A selection of sklearn Transformer methods.\n            - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n            - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n            - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                        Subsets are set with the subsets parameter.\n            - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n            - 'MDR' : Includes MDR.\n            - 'ContinuousMDR' : Includes ContinuousMDR.\n            - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n            - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n            - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n            - None : If None and max_depth&gt;1, the root_config_dict will be used for the inner nodes as well.\n        leaf_config_dict : dict, default=None \n            The configuration dictionary to use for the leaf node of the model. If set, leaf nodes must be from this dictionary.\n            Otherwise leaf nodes will be generated from the root_config_dict. \n            Default None\n            - 'selectors' : A selection of sklearn Selector methods.\n            - 'classifiers' : A selection of sklearn Classifier methods.\n            - 'regressors' : A selection of sklearn Regressor methods.\n            - 'transformers' : A selection of sklearn Transformer methods.\n            - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n            - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n            - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                        Subsets are set with the subsets parameter.\n            - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n            - 'MDR' : Includes MDR.\n            - 'ContinuousMDR' : Includes ContinuousMDR.\n            - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n            - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n            - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n            - None : If None, a leaf will not be required (i.e. the pipeline can be a single root node). Leaf nodes will be generated from the inner_config_dict.\n        cross_val_predict_cv : int, default=0\n            Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n            - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n            - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset. \n                    However, the output to the next node will come from cross_val_predict with the specified number of folds.\n        categorical_features: list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n            - None : If None, TPOT2 will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n            - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n        subsets : str or list, default=None\n            Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.\n            - str : If a string, it is assumed to be a path to a csv file with the subsets. \n                The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n            - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets.\n            - None : If None, each column will be treated as a subset. One column will be selected per subset.\n            If subsets is None, each column will be treated as a subset. One column will be selected per subset.\n        memory: Memory object or string, default=None\n            If supplied, pipeline will cache each transformer after calling fit. This feature\n            is used to avoid computing the fit transformers within a pipeline if the parameters\n            and input data are identical with another fitted pipeline during optimization process.\n            - String 'auto':\n                TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n            - String path of a caching directory\n                TPOT uses memory caching with the provided directory and TPOT does NOT clean\n                the caching directory up upon shutdown. If the directory does not exist, TPOT will\n                create it.\n            - Memory object:\n                TPOT uses the instance of joblib.Memory for memory caching,\n                and TPOT does NOT clean the caching directory up upon shutdown.\n            - None:\n                TPOT does not use memory caching.\n        preprocessing : bool or BaseEstimator/Pipeline, \n            EXPERIMENTAL\n            A pipeline that will be used to preprocess the data before CV.\n            - bool : If True, will use a default preprocessing pipeline.\n            - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n        validation_strategy : str, default='none'\n            EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT2 may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n            - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n            - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. \n            - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. \n            - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n        validation_fraction : float, default=0.2\n          EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n        population_size : int, default=50\n            Size of the population\n        initial_population_size : int, default=None\n            Size of the initial population. If None, population_size will be used.\n        population_scaling : int, default=0.5\n            Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        generations_until_end_population : int, default=1  \n            Number of generations until the population size reaches population_size            \n        generations : int, default=50\n            Number of generations to run\n        early_stop : int, default=None\n            Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered.\n        scorers_early_stop_tol : \n            -list of floats\n                list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int \n                If an int is given, it will be used as the tolerance for all objectives\n        other_objectives_early_stop_tol : \n            -list of floats\n                list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int \n                If an int is given, it will be used as the tolerance for all objectives\n        max_time_seconds : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n        max_eval_time_seconds : float, default=60*5\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n        memory_limit : str, default=\"4GB\"\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit. \n        crossover_probability : float, default=.2\n            Probability of generating a new individual by crossover between two individuals.\n        mutate_probability : float, default=.7\n            Probability of generating a new individual by crossover between one individuals.\n        mutate_then_crossover_probability : float, default=.05\n            Probability of generating a new individual by mutating two individuals followed by crossover.\n        crossover_then_mutate_probability : float, default=.05\n            Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n        n_parents : int, default=2\n            Number of parents to use for crossover. Must be greater than 1.\n        survival_selector : function, default=survival_select_NSGA2\n            Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n            Used to selected population_size individuals at the start of each generation to use for mutation and crossover.\n        parent_selector : function, default=parent_select_NSGA2\n            Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n        budget_range : list [start, end], default=None\n            A starting and ending budget to use for the budget scaling.\n        budget_scaling float : [0,1], default=0.5\n            A scaling factor to use when determining how fast we move the budget from the start to end budget.\n        individuals_until_end_budget : int, default=1\n            The number of generations to run before reaching the max budget.\n        stepwise_steps : int, default=1\n            The number of staircase steps to take when scaling the budget and population size.\n        threshold_evaluation_early_stop : list [start, end], default=None\n            starting and ending percentile to use as a threshold for the evaluation early stopping.\n            Values between 0 and 100.\n        threshold_evaluation_scaling : float [0,inf), default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n        min_history_threshold : int, default=0\n            The minimum number of previous scores needed before using threshold early stopping.\n        selection_evaluation_early_stop : list, default=None\n            A lower and upper percent of the population size to select each round of CV.\n            Values between 0 and 1.\n        selection_evaluation_scaling : float, default=0.5 \n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n        n_initial_optimizations : int, default=0\n            Number of individuals to optimize before starting the evolution.\n        optimization_cv : int \n           Number of folds to use for the optuna optimization's internal cross-validation.\n        max_optimize_time_seconds : float, default=60*5\n            Maximum time to run an optimization\n        optimization_steps : int, default=10\n            Number of steps per optimization\n        warm_start : bool, default=False\n            If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n        subset_column : str or int, default=None\n            EXPERIMENTAL The column to use for the subset selection. Must also pass in unique_subset_values to GraphIndividual to function.\n        evolver : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver), default=eaNSGA2_Evolver\n            The evolver to use for the optimization process. See tpot2.evolutionary_algorithms\n            - type : an type or subclass of a BaseEvolver\n            - \"nsga2\" : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver\n        verbose : int, default=1 \n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n            6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_seconds in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n        callback : tpot2.CallBackInterface, default=None\n            Callback object. Not implemented\n        processes : bool, default=True\n            If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.\n            True seems to perform better. However, False is required for interactive debugging.\n        Attributes\n        ----------\n        fitted_pipeline_ : GraphPipeline\n            A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n        evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run. \n            Columns: \n            - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n            - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n            - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n            - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n            - Generation : The generation the pipeline first appeared. \n            - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual. \n                            To save on computational time, the best frontier is updated iteratively each generation. \n                            The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                            All other pipelines are set to NaN. \n            - Instance\t: The unfitted GraphPipeline BaseEstimator. \n            - *validation objective functions : Objective function scores evaluated on the validation set.\n            - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n        pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n        '''\n# sklearn BaseEstimator must have a corresponding attribute for each parameter.\n# These should not be modified once set.\nself.scorers = scorers\nself.scorers_weights = scorers_weights\nself.classification = classification\nself.cv = cv\nself.other_objective_functions = other_objective_functions\nself.other_objective_functions_weights = other_objective_functions_weights\nself.objective_function_names = objective_function_names\nself.bigger_is_better = bigger_is_better\nself.max_size = max_size\nself.linear_pipeline = linear_pipeline\nself.root_config_dict= root_config_dict\nself.inner_config_dict= inner_config_dict\nself.leaf_config_dict= leaf_config_dict\nself.cross_val_predict_cv = cross_val_predict_cv\nself.categorical_features = categorical_features\nself.subsets = subsets\nself.memory = memory\nself.preprocessing = preprocessing\nself.validation_strategy = validation_strategy\nself.validation_fraction = validation_fraction\nself.population_size = population_size\nself.initial_population_size = initial_population_size\nself.early_stop = early_stop\nself.scorers_early_stop_tol = scorers_early_stop_tol\nself.other_objectives_early_stop_tol = other_objectives_early_stop_tol\nself.max_time_seconds = max_time_seconds \nself.max_eval_time_seconds = max_eval_time_seconds\nself.n_jobs= n_jobs\nself.memory_limit = memory_limit\nself.client = client\nself.crossover_probability = crossover_probability\nself.mutate_probability = mutate_probability\nself.mutate_then_crossover_probability= mutate_then_crossover_probability\nself.crossover_then_mutate_probability= crossover_then_mutate_probability\nself.survival_selector=survival_selector\nself.parent_selector=parent_selector\nself.budget_range = budget_range\nself.budget_scaling = budget_scaling\nself.individuals_until_end_budget = individuals_until_end_budget\nself.stepwise_steps = stepwise_steps\nself.warm_start = warm_start\nself.subset_column = subset_column\nself.verbose = verbose\nself.periodic_checkpoint_folder = periodic_checkpoint_folder\nself.callback = callback\nself.processes = processes\nself.scatter = scatter\nself.optuna_optimize_pareto_front = optuna_optimize_pareto_front\nself.optuna_optimize_pareto_front_trials = optuna_optimize_pareto_front_trials\nself.optuna_optimize_pareto_front_timeout = optuna_optimize_pareto_front_timeout\nself.optuna_storage = optuna_storage\nself.max_evaluated_individuals = max_evaluated_individuals\n#Initialize other used params\nif self.initial_population_size is None:\nself._initial_population_size = self.population_size\nelse:\nself._initial_population_size = self.initial_population_size\nif isinstance(self.scorers, str):\nself._scorers = [self.scorers]\nelif callable(self.scorers):\nself._scorers = [self.scorers]\nelse:\nself._scorers = self.scorers\nself._scorers = [sklearn.metrics.get_scorer(scoring) for scoring in self._scorers]\nself._scorers_early_stop_tol = self.scorers_early_stop_tol\nself._evolver = tpot2.evolvers.SteadyStateEvolver\nself.objective_function_weights = [*scorers_weights, *other_objective_functions_weights]\nif self.objective_function_names is None:\nobj_names = [f.__name__ for f in other_objective_functions]\nelse:\nobj_names = self.objective_function_names\nself.objective_names = [f._score_func.__name__ if hasattr(f,\"_score_func\") else f.__name__ for f in self._scorers] + obj_names\nif not isinstance(self.other_objectives_early_stop_tol, list):\nself._other_objectives_early_stop_tol = [self.other_objectives_early_stop_tol for _ in range(len(self.other_objective_functions))]\nelse:\nself._other_objectives_early_stop_tol = self.other_objectives_early_stop_tol\nif not isinstance(self._scorers_early_stop_tol, list):\nself._scorers_early_stop_tol = [self._scorers_early_stop_tol for _ in range(len(self._scorers))]\nelse:\nself._scorers_early_stop_tol = self._scorers_early_stop_tol\nself.early_stop_tol = [*self._scorers_early_stop_tol, *self._other_objectives_early_stop_tol]\nself._evolver_instance = None\nself.evaluated_individuals = None\nset_dask_settings()\ndef fit(self, X, y):\nif self.client is not None: #If user passed in a client manually\n_client = self.client\nelse:\nif self.verbose &gt;= 4:\nsilence_logs = 30\nelif self.verbose &gt;=5:\nsilence_logs = 40\nelse:\nsilence_logs = 50\ncluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\nthreads_per_worker=1,\nprocesses=self.processes,\nsilence_logs=silence_logs,\nmemory_limit=self.memory_limit)\n_client = Client(cluster)\nself.evaluated_individuals = None\n#determine validation strategy\nif self.validation_strategy == 'auto':\nnrows = X.shape[0]\nncols = X.shape[1]\nif nrows/ncols &lt; 20:\nvalidation_strategy = 'reshuffled'\nelif nrows/ncols &lt; 100:\nvalidation_strategy = 'split'\nelse:\nvalidation_strategy = 'none'\nelse:\nvalidation_strategy = self.validation_strategy\nif validation_strategy == 'split':\nif self.classification:\nX, X_val, y, y_val = train_test_split(X, y, test_size=self.validation_fraction, stratify=y, random_state=42)\nelse:\nX, X_val, y, y_val = train_test_split(X, y, test_size=self.validation_fraction, random_state=42)\nX_original = X\ny_original = y\nif isinstance(self.cv, int) or isinstance(self.cv, float):\nn_folds = self.cv\nelse:\nn_folds = self.cv.get_n_splits(X, y)\nX, y = remove_underrepresented_classes(X, y, n_folds)\nif self.preprocessing:\n#X = pd.DataFrame(X)\n#TODO: check if there are missing values in X before imputation. If not, don't include imputation in pipeline. Check if there are categorical columns. If not, don't include one hot encoding in pipeline\nif isinstance(X, pd.DataFrame): #pandas dataframe\nif self.categorical_features is not None:\nX[self.categorical_features] = X[self.categorical_features].astype(object)\nself._preprocessing_pipeline = sklearn.pipeline.make_pipeline(tpot2.builtin_modules.ColumnSimpleImputer(\"categorical\", strategy='most_frequent'), #impute categorical columns\ntpot2.builtin_modules.ColumnSimpleImputer(\"numeric\", strategy='mean'),              #impute numeric columns\ntpot2.builtin_modules.ColumnOneHotEncoder(\"categorical\", min_frequency=0.0001))     #one hot encode categorical columns\nX = self._preprocessing_pipeline.fit_transform(X)\nelse:\nif self.categorical_features is not None: #numpy array and categorical columns specified\nself._preprocessing_pipeline = sklearn.pipeline.make_pipeline(tpot2.builtin_modules.ColumnSimpleImputer(self.categorical_features, strategy='most_frequent'),   #impute categorical columns\ntpot2.builtin_modules.ColumnSimpleImputer(\"all\", strategy='mean'),                                      #impute remaining numeric columns\ntpot2.builtin_modules.ColumnOneHotEncoder(self.categorical_features, min_frequency=0.0001))             #one hot encode categorical columns\nelse: #numpy array and no categorical columns specified, just do imputation\nself._preprocessing_pipeline = sklearn.pipeline.make_pipeline(tpot2.builtin_modules.ColumnSimpleImputer(\"all\", strategy='mean'))   \nelse:\nself._preprocessing_pipeline = None\n#_, y = sklearn.utils.check_X_y(X, y, y_numeric=True)\n#Set up the configuation dictionaries and the search spaces\nn_samples= int(math.floor(X.shape[0]/n_folds))\nn_features=X.shape[1]\nif isinstance(X, pd.DataFrame):\nself.feature_names = X.columns\nelse:\nself.feature_names = None\nif self.root_config_dict == 'Auto':\nif self.classification:\nn_classes = len(np.unique(y))\nroot_config_dict = get_configuration_dictionary(\"classifiers\", n_samples, n_features, self.classification, subsets=self.subsets, feature_names=self.feature_names, n_classes=n_classes)\nelse:\nroot_config_dict = get_configuration_dictionary(\"regressors\", n_samples, n_features, self.classification,subsets=self.subsets, feature_names=self.feature_names)\nelse:\nroot_config_dict = get_configuration_dictionary(self.root_config_dict, n_samples, n_features, self.classification, subsets=self.subsets,feature_names=self.feature_names)\ninner_config_dict = get_configuration_dictionary(self.inner_config_dict, n_samples, n_features, self.classification,subsets=self.subsets, feature_names=self.feature_names)\nleaf_config_dict = get_configuration_dictionary(self.leaf_config_dict, n_samples, n_features, self.classification, subsets=self.subsets, feature_names=self.feature_names)\n#check if self.cv is a number\nif isinstance(self.cv, int) or isinstance(self.cv, float):\nif self.classification:\nself.cv_gen = sklearn.model_selection.StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=42)\nelse:\nself.cv_gen = sklearn.model_selection.KFold(n_splits=self.cv, shuffle=True, random_state=42)\nelse:\nself.cv_gen = sklearn.model_selection.check_cv(self.cv, y, classifier=self.classification)\ndef objective_function(pipeline_individual, \nX, \ny,\nis_classification=self.classification,\nscorers= self._scorers, \ncv=self.cv_gen, \nother_objective_functions=self.other_objective_functions,\nmemory=self.memory, \ncross_val_predict_cv=self.cross_val_predict_cv, \nsubset_column=self.subset_column, \n**kwargs): \nreturn objective_function_generator(\npipeline_individual,\nX, \ny, \nis_classification=is_classification,\nscorers= scorers, \ncv=cv, \nother_objective_functions=other_objective_functions,\nmemory=memory, \ncross_val_predict_cv=cross_val_predict_cv, \nsubset_column=subset_column,\n**kwargs,\n)\nself.individual_generator_instance = tpot2.individual_representations.graph_pipeline_individual.estimator_graph_individual_generator(   \ninner_config_dict=inner_config_dict,\nroot_config_dict=root_config_dict,\nleaf_config_dict=leaf_config_dict,\nmax_size = self.max_size,\nlinear_pipeline=self.linear_pipeline,\n)\nif self.scatter:\nX_future = _client.scatter(X)\ny_future = _client.scatter(y)\nelse:\nX_future = X\ny_future = y\n#If warm start and we have an evolver instance, use the existing one\nif not(self.warm_start and self._evolver_instance is not None):\nself._evolver_instance = self._evolver(   individual_generator=self.individual_generator_instance, \nobjective_functions= [objective_function],\nobjective_function_weights = self.objective_function_weights,\nobjective_names=self.objective_names,\nbigger_is_better = self.bigger_is_better,\npopulation_size= self.population_size,\ninitial_population_size = self._initial_population_size,\nn_jobs=self.n_jobs,\nverbose = self.verbose,\nmax_time_seconds =      self.max_time_seconds ,\nmax_eval_time_seconds = self.max_eval_time_seconds,\nperiodic_checkpoint_folder = self.periodic_checkpoint_folder,\nearly_stop_tol = self.early_stop_tol,\nearly_stop= self.early_stop,\nbudget_range = self.budget_range,\nbudget_scaling = self.budget_scaling,\nindividuals_until_end_budget = self.individuals_until_end_budget,\nstepwise_steps = self.stepwise_steps,\nclient = _client,\nobjective_kwargs = {\"X\": X_future, \"y\": y_future},\nsurvival_selector=self.survival_selector,\nparent_selector=self.parent_selector,\ncrossover_probability = self.crossover_probability,\nmutate_probability = self.mutate_probability,\nmutate_then_crossover_probability= self.mutate_then_crossover_probability,\ncrossover_then_mutate_probability= self.crossover_then_mutate_probability,\nmax_evaluated_individuals = self.max_evaluated_individuals\n)\nself._evolver_instance.optimize()\n#self._evolver_instance.population.update_pareto_fronts(self.objective_names, self.objective_function_weights)\nself.make_evaluated_individuals()\nif self.optuna_optimize_pareto_front:\npareto_front_inds = self.pareto_front['Individual'].values\nall_graphs, all_scores = tpot2.individual_representations.graph_pipeline_individual.simple_parallel_optuna(pareto_front_inds,  objective_function, self.objective_function_weights, _client, storage=self.optuna_storage, steps=self.optuna_optimize_pareto_front_trials, verbose=self.verbose, max_eval_time_seconds=self.max_eval_time_seconds, max_time_seconds=self.optuna_optimize_pareto_front_timeout, **{\"X\": X, \"y\": y})\nall_scores = tpot2.utils.eval_utils.process_scores(all_scores, len(self.objective_function_weights))\nif len(all_graphs) &gt; 0:\ndf = pd.DataFrame(np.column_stack((all_graphs, all_scores,np.repeat(\"Optuna\",len(all_graphs)))), columns=[\"Individual\"] + self.objective_names +[\"Parents\"])\nfor obj in self.objective_names:\ndf[obj] = df[obj].apply(convert_to_float)\nself.evaluated_individuals = pd.concat([self.evaluated_individuals, df], ignore_index=True)\nelse:\nprint(\"WARNING NO OPTUNA TRIALS COMPLETED\")\ntpot2.utils.get_pareto_frontier(self.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights, invalid_values=[\"TIMEOUT\",\"INVALID\"])\nif validation_strategy == 'reshuffled':\nbest_pareto_front_idx = list(self.pareto_front.index)\nbest_pareto_front = list(self.pareto_front.loc[best_pareto_front_idx]['Individual'])\n#reshuffle rows\nX, y = sklearn.utils.shuffle(X, y, random_state=1)\nif self.scatter:\nX_future = _client.scatter(X)\ny_future = _client.scatter(y)\nelse:\nX_future = X\ny_future = y\nval_objective_function_list = [lambda   ind, \nX, \ny, \nis_classification=self.classification,\nscorers= self._scorers, \ncv=self.cv_gen, \nother_objective_functions=self.other_objective_functions, \nmemory=self.memory, \ncross_val_predict_cv=self.cross_val_predict_cv, \nsubset_column=self.subset_column, \n**kwargs: objective_function_generator(\nind,\nX,\ny, \nis_classification=is_classification,\nscorers= scorers, \ncv=cv, \nother_objective_functions=other_objective_functions,\nmemory=memory, \ncross_val_predict_cv=cross_val_predict_cv, \nsubset_column=subset_column,\n**kwargs,\n)]\nobjective_kwargs = {\"X\": X_future, \"y\": y_future}\nval_scores = tpot2.utils.eval_utils.parallel_eval_objective_list(\nbest_pareto_front,\nval_objective_function_list, n_jobs=self.n_jobs, verbose=self.verbose, timeout=self.max_eval_time_seconds,n_expected_columns=len(self.objective_names), client=_client, **objective_kwargs)\nval_objective_names = ['validation_'+name for name in self.objective_names]\nself.objective_names_for_selection = val_objective_names\nself.evaluated_individuals.loc[best_pareto_front_idx,val_objective_names] = val_scores\nself.evaluated_individuals[\"Validation_Pareto_Front\"] = tpot2.utils.get_pareto_front(self.evaluated_individuals, val_objective_names, self.objective_function_weights, invalid_values=[\"TIMEOUT\",\"INVALID\"])\nelif validation_strategy == 'split':\nif self.scatter:            \nX_future = _client.scatter(X)\ny_future = _client.scatter(y)\nX_val_future = _client.scatter(X_val)\ny_val_future = _client.scatter(y_val)\nelse:\nX_future = X\ny_future = y\nX_val_future = X_val\ny_val_future = y_val\nobjective_kwargs = {\"X\": X_future, \"y\": y_future, \"X_val\" : X_val_future, \"y_val\":y_val_future }\nbest_pareto_front_idx = list(self.pareto_front.index)\nbest_pareto_front = list(self.pareto_front.loc[best_pareto_front_idx]['Individual'])\nval_objective_function_list = [lambda   ind, \nX, \ny, \nX_val, \ny_val, \nscorers= self._scorers, \nother_objective_functions=self.other_objective_functions, \nmemory=self.memory, \ncross_val_predict_cv=self.cross_val_predict_cv, \nsubset_column=self.subset_column, \n**kwargs: val_objective_function_generator(\nind,\nX,\ny,\nX_val, \ny_val, \nscorers= scorers, \nother_objective_functions=other_objective_functions,\nmemory=memory, \ncross_val_predict_cv=cross_val_predict_cv, \nsubset_column=subset_column,\n**kwargs,\n)]\nval_scores = tpot2.utils.eval_utils.parallel_eval_objective_list(\nbest_pareto_front,\nval_objective_function_list, n_jobs=self.n_jobs, verbose=self.verbose, timeout=self.max_eval_time_seconds,n_expected_columns=len(self.objective_names),client=_client, **objective_kwargs)\nval_objective_names = ['validation_'+name for name in self.objective_names]\nself.objective_names_for_selection = val_objective_names\nself.evaluated_individuals.loc[best_pareto_front_idx,val_objective_names] = val_scores\nself.evaluated_individuals[\"Validation_Pareto_Front\"] = tpot2.utils.get_pareto_front(self.evaluated_individuals, val_objective_names, self.objective_function_weights, invalid_values=[\"TIMEOUT\",\"INVALID\"])\nelse:\nself.objective_names_for_selection = self.objective_names\nval_scores = self.evaluated_individuals[~self.evaluated_individuals[self.objective_names_for_selection].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names_for_selection].astype(float)                                     \nweighted_scores = val_scores*self.objective_function_weights\nif self.bigger_is_better:\nbest_idx = weighted_scores[self.objective_names_for_selection[0]].idxmax()\nelse:\nbest_idx = weighted_scores[self.objective_names_for_selection[0]].idxmin()\nbest_individual = self.evaluated_individuals.loc[best_idx]['Individual']\nself.selected_best_score =  self.evaluated_individuals.loc[best_idx]\nbest_individual_pipeline = best_individual.export_pipeline(memory=self.memory, cross_val_predict_cv=self.cross_val_predict_cv, subset_column=self.subset_column)\nif self.preprocessing:\nself.fitted_pipeline_ = sklearn.pipeline.make_pipeline(sklearn.base.clone(self._preprocessing_pipeline), best_individual_pipeline )\nelse:\nself.fitted_pipeline_ = best_individual_pipeline \nself.fitted_pipeline_.fit(X_original,y_original) #TODO use y_original as well?\nif self.client is None: #no client was passed in\n#close cluster and client\n_client.close()\ncluster.close()\nreturn self\ndef _estimator_has(attr):\n'''Check if we can delegate a method to the underlying estimator.\n        First, we check the first fitted final estimator if available, otherwise we\n        check the unfitted final estimator.\n        '''\nreturn  lambda self: (self.fitted_pipeline_ is not None and\nhasattr(self.fitted_pipeline_, attr)\n)\n@available_if(_estimator_has('predict'))\ndef predict(self, X, **predict_params):\ncheck_is_fitted(self)\n#X = check_array(X)\nreturn self.fitted_pipeline_.predict(X,**predict_params)\n@available_if(_estimator_has('predict_proba'))\ndef predict_proba(self, X, **predict_params):\ncheck_is_fitted(self)\n#X = check_array(X)\nreturn self.fitted_pipeline_.predict_proba(X,**predict_params)\n@available_if(_estimator_has('decision_function'))\ndef decision_function(self, X, **predict_params):\ncheck_is_fitted(self)\n#X = check_array(X)\nreturn self.fitted_pipeline_.decision_function(X,**predict_params)\n@available_if(_estimator_has('transform'))\ndef transform(self, X, **predict_params):\ncheck_is_fitted(self)\n#X = check_array(X)\nreturn self.fitted_pipeline_.transform(X,**predict_params)\n@property\ndef classes_(self):\n\"\"\"The classes labels. Only exist if the last step is a classifier.\"\"\"\nreturn self.fitted_pipeline_.classes_\ndef make_evaluated_individuals(self):\n#check if _evolver_instance exists\nif self.evaluated_individuals is None:\nself.evaluated_individuals  =  self._evolver_instance.population.evaluated_individuals.copy()\nobjects = list(self.evaluated_individuals.index)\nobject_to_int = dict(zip(objects, range(len(objects))))\nself.evaluated_individuals = self.evaluated_individuals.set_index(self.evaluated_individuals.index.map(object_to_int))\nself.evaluated_individuals['Parents'] = self.evaluated_individuals['Parents'].apply(lambda row: convert_parents_tuples_to_integers(row, object_to_int))\nself.evaluated_individuals[\"Instance\"] = self.evaluated_individuals[\"Individual\"].apply(lambda ind: apply_make_pipeline(ind, preprocessing_pipeline=self._preprocessing_pipeline))\nreturn self.evaluated_individuals\n@property\ndef pareto_front(self):\n#check if _evolver_instance exists\nif self.evaluated_individuals is None:\nreturn None\nelse:\nif \"Pareto_Front\" not in self.evaluated_individuals:\nreturn self.evaluated_individuals\nelse:\nreturn self.evaluated_individuals[self.evaluated_individuals[\"Pareto_Front\"]==1]\n</code></pre>"},{"location":"documentation/tpot2/tpot_estimator/steady_state_estimator/#tpot2.tpot_estimator.steady_state_estimator.TPOTEstimatorSteadyState.classes_","title":"<code>classes_</code>  <code>property</code>","text":"<p>The classes labels. Only exist if the last step is a classifier.</p>"},{"location":"documentation/tpot2/tpot_estimator/steady_state_estimator/#tpot2.tpot_estimator.steady_state_estimator.TPOTEstimatorSteadyState.__init__","title":"<code>__init__(scorers=[], scorers_weights=[], classification=False, cv=5, other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, max_size=np.inf, linear_pipeline=False, root_config_dict='Auto', inner_config_dict=['selectors', 'transformers'], leaf_config_dict=None, cross_val_predict_cv=0, categorical_features=None, subsets=None, memory=None, preprocessing=False, validation_strategy='none', validation_fraction=0.2, initial_population_size=50, population_size=50, max_evaluated_individuals=None, early_stop=None, scorers_early_stop_tol=0.001, other_objectives_early_stop_tol=None, max_time_seconds=float('inf'), max_eval_time_seconds=60 * 10, n_jobs=1, memory_limit='4GB', client=None, crossover_probability=0.2, mutate_probability=0.7, mutate_then_crossover_probability=0.05, crossover_then_mutate_probability=0.05, survival_selector=survival_select_NSGA2, parent_selector=tournament_selection_dominated, budget_range=None, budget_scaling=0.5, individuals_until_end_budget=1, stepwise_steps=5, warm_start=False, subset_column=None, verbose=0, periodic_checkpoint_folder=None, callback=None, processes=True, scatter=True, optuna_optimize_pareto_front=False, optuna_optimize_pareto_front_trials=100, optuna_optimize_pareto_front_timeout=60 * 10, optuna_storage='sqlite:///optuna.db')</code>","text":"<p>An sklearn baseestimator that uses genetic programming to optimize a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>scorers</code> <code>(list, scorer)</code> <p>A scorer or list of scorers to be used in the cross-validation process.  see https://scikit-learn.org/stable/modules/model_evaluation.html</p> <code>[]</code> <p>scorers_weights : list     A list of weights to be applied to the scorers during the optimization process.</p> <p>classification : bool     If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.     Used to determine the CV strategy.</p> <p>cv : int, cross-validator     - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.     - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.</p> <p>other_objective_functions : list, default=[tpot2.objectives.estimator_objective_functions.average_path_length_objective]     A list of other objective functions to apply to the pipeline.</p> <p>other_objective_functions_weights : list, default=[-1]     A list of weights to be applied to the other objective functions.</p> <p>objective_function_names : list, default=None     A list of names to be applied to the objective functions. If None, will use the names of the objective functions.</p> <p>bigger_is_better : bool, default=True     If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <p>max_size : int, default=np.inf     The maximum number of nodes of the pipelines to be generated.</p> <p>linear_pipeline : bool, default=False     If True, the pipelines generated will be linear. If False, the pipelines generated will be directed acyclic graphs.</p> <p>root_config_dict : dict, default='auto'     The configuration dictionary to use for the root node of the model.     If 'auto', will use \"classifiers\" if classification=True, else \"regressors\".     - 'selectors' : A selection of sklearn Selector methods.     - 'classifiers' : A selection of sklearn Classifier methods.     - 'regressors' : A selection of sklearn Regressor methods.     - 'transformers' : A selection of sklearn Transformer methods.     - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.     - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.     - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.                                 Subsets are set with the subsets parameter.     - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.     - 'MDR' : Includes MDR.     - 'ContinuousMDR' : Includes ContinuousMDR.     - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.     - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.     - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.</p> <p>inner_config_dict : dict, default=[\"selectors\", \"transformers\"]     The configuration dictionary to use for the inner nodes of the model generation.     Default [\"selectors\", \"transformers\"]     - 'selectors' : A selection of sklearn Selector methods.     - 'classifiers' : A selection of sklearn Classifier methods.     - 'regressors' : A selection of sklearn Regressor methods.     - 'transformers' : A selection of sklearn Transformer methods.     - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.     - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.     - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.                                 Subsets are set with the subsets parameter.     - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.     - 'MDR' : Includes MDR.     - 'ContinuousMDR' : Includes ContinuousMDR.     - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.     - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.     - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.     - None : If None and max_depth&gt;1, the root_config_dict will be used for the inner nodes as well.</p> <p>leaf_config_dict : dict, default=None      The configuration dictionary to use for the leaf node of the model. If set, leaf nodes must be from this dictionary.     Otherwise leaf nodes will be generated from the root_config_dict.      Default None     - 'selectors' : A selection of sklearn Selector methods.     - 'classifiers' : A selection of sklearn Classifier methods.     - 'regressors' : A selection of sklearn Regressor methods.     - 'transformers' : A selection of sklearn Transformer methods.     - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.     - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.     - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.                                 Subsets are set with the subsets parameter.     - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.     - 'MDR' : Includes MDR.     - 'ContinuousMDR' : Includes ContinuousMDR.     - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.     - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.     - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.     - None : If None, a leaf will not be required (i.e. the pipeline can be a single root node). Leaf nodes will be generated from the inner_config_dict.</p> <p>cross_val_predict_cv : int, default=0     Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.</p> <pre><code>- 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n- &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset. \n        However, the output to the next node will come from cross_val_predict with the specified number of folds.\n</code></pre> <p>categorical_features: list or None     Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.     - None : If None, TPOT2 will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.     - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices</p> <p>subsets : str or list, default=None     Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.     - str : If a string, it is assumed to be a path to a csv file with the subsets.          The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.     - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets.     - None : If None, each column will be treated as a subset. One column will be selected per subset.     If subsets is None, each column will be treated as a subset. One column will be selected per subset.</p> <p>memory: Memory object or string, default=None     If supplied, pipeline will cache each transformer after calling fit. This feature     is used to avoid computing the fit transformers within a pipeline if the parameters     and input data are identical with another fitted pipeline during optimization process.     - String 'auto':         TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.     - String path of a caching directory         TPOT uses memory caching with the provided directory and TPOT does NOT clean         the caching directory up upon shutdown. If the directory does not exist, TPOT will         create it.     - Memory object:         TPOT uses the instance of joblib.Memory for memory caching,         and TPOT does NOT clean the caching directory up upon shutdown.     - None:         TPOT does not use memory caching.</p> <p>preprocessing : bool or BaseEstimator/Pipeline,      EXPERIMENTAL     A pipeline that will be used to preprocess the data before CV.     - bool : If True, will use a default preprocessing pipeline.     - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.</p> <p>validation_strategy : str, default='none'     EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT2 may overfit the cross validation score. A second validation set can be used to select the final pipeline.     - 'auto' : Automatically determine the validation strategy based on the dataset shape.     - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.      - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.      - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.</p> <p>validation_fraction : float, default=0.2   EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.</p> <p>population_size : int, default=50     Size of the population</p> <p>initial_population_size : int, default=None     Size of the initial population. If None, population_size will be used.</p> <p>population_scaling : int, default=0.5     Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.</p> <p>generations_until_end_population : int, default=1     Number of generations until the population size reaches population_size            </p> <p>generations : int, default=50     Number of generations to run</p> <p>early_stop : int, default=None     Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered.</p> <p>scorers_early_stop_tol :      -list of floats         list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged         If an index of the list is None, that item will not be used for early stopping     -int          If an int is given, it will be used as the tolerance for all objectives</p> <p>other_objectives_early_stop_tol :      -list of floats         list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged         If an index of the list is None, that item will not be used for early stopping     -int          If an int is given, it will be used as the tolerance for all objectives</p> <p>max_time_seconds : float, default=float(\"inf\")     Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <p>max_eval_time_seconds : float, default=60*5     Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <p>n_jobs : int, default=1     Number of processes to run in parallel.</p> <p>memory_limit : str, default=\"4GB\"     Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <p>client : dask.distributed.Client, default=None     A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit. </p> <p>crossover_probability : float, default=.2     Probability of generating a new individual by crossover between two individuals.</p> <p>mutate_probability : float, default=.7     Probability of generating a new individual by crossover between one individuals.</p> <p>mutate_then_crossover_probability : float, default=.05     Probability of generating a new individual by mutating two individuals followed by crossover.</p> <p>crossover_then_mutate_probability : float, default=.05     Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.</p> <p>n_parents : int, default=2     Number of parents to use for crossover. Must be greater than 1.</p> <p>survival_selector : function, default=survival_select_NSGA2     Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.     Used to selected population_size individuals at the start of each generation to use for mutation and crossover.</p> <p>parent_selector : function, default=parent_select_NSGA2     Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.</p> <p>budget_range : list [start, end], default=None     A starting and ending budget to use for the budget scaling.</p> <p>budget_scaling float : [0,1], default=0.5     A scaling factor to use when determining how fast we move the budget from the start to end budget.</p> <p>individuals_until_end_budget : int, default=1     The number of generations to run before reaching the max budget.</p> <p>stepwise_steps : int, default=1     The number of staircase steps to take when scaling the budget and population size.</p> <p>threshold_evaluation_early_stop : list [start, end], default=None     starting and ending percentile to use as a threshold for the evaluation early stopping.     Values between 0 and 100.</p> <p>threshold_evaluation_scaling : float [0,inf), default=0.5     A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.     Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <p>min_history_threshold : int, default=0     The minimum number of previous scores needed before using threshold early stopping.</p> <p>selection_evaluation_early_stop : list, default=None     A lower and upper percent of the population size to select each round of CV.     Values between 0 and 1.</p> <p>selection_evaluation_scaling : float, default=0.5      A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.     Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <p>n_initial_optimizations : int, default=0     Number of individuals to optimize before starting the evolution.</p> <p>optimization_cv : int     Number of folds to use for the optuna optimization's internal cross-validation.</p> <p>max_optimize_time_seconds : float, default=60*5     Maximum time to run an optimization</p> <p>optimization_steps : int, default=10     Number of steps per optimization</p> <p>warm_start : bool, default=False     If True, will use the continue the evolutionary algorithm from the last generation of the previous run.</p> <p>subset_column : str or int, default=None     EXPERIMENTAL The column to use for the subset selection. Must also pass in unique_subset_values to GraphIndividual to function.</p> <p>evolver : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver), default=eaNSGA2_Evolver     The evolver to use for the optimization process. See tpot2.evolutionary_algorithms     - type : an type or subclass of a BaseEvolver     - \"nsga2\" : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver</p> <p>verbose : int, default=1      How much information to print during the optimization process. Higher values include the information from lower values.     0. nothing     1. progress bar</p> <pre><code>3. best individual\n4. warnings\n&gt;=5. full warnings trace\n6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_seconds in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n</code></pre> <p>periodic_checkpoint_folder : str, default=None     Folder to save the population to periodically. If None, no periodic saving will be done.     If provided, training will resume from this checkpoint.</p> <p>callback : tpot2.CallBackInterface, default=None     Callback object. Not implemented</p> <p>processes : bool, default=True     If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.     True seems to perform better. However, False is required for interactive debugging.</p> <p>Attributes:</p> Name Type Description <code>fitted_pipeline_</code> <code>GraphPipeline</code> <p>A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.</p> <p>evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.      Columns:      - objective functions : The first few columns correspond to the passed in scorers and objective functions     - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.     - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.     - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.     - Generation : The generation the pipeline first appeared.      - Pareto_Front      : The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.                      To save on computational time, the best frontier is updated iteratively each generation.                      The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.                     All other pipelines are set to NaN.      - Instance  : The unfitted GraphPipeline BaseEstimator.      - validation objective functions : Objective function scores evaluated on the validation set.     - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.</p> <p>pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.</p> Source code in <code>tpot2/tpot_estimator/steady_state_estimator.py</code> <pre><code>def __init__(self,  scorers= [], \nscorers_weights = [],\nclassification = False,\ncv = 5,\nother_objective_functions=[], #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = [],\nobjective_function_names = None,\nbigger_is_better = True,\nmax_size = np.inf, \nlinear_pipeline = False,\nroot_config_dict= 'Auto',\ninner_config_dict=[\"selectors\", \"transformers\"],\nleaf_config_dict= None,                        \ncross_val_predict_cv = 0,\ncategorical_features = None,\nsubsets = None,\nmemory = None,\npreprocessing = False,  \nvalidation_strategy = \"none\",\nvalidation_fraction = .2,\ninitial_population_size = 50,\npopulation_size = 50,\nmax_evaluated_individuals = None,\nearly_stop = None,\nscorers_early_stop_tol = 0.001,\nother_objectives_early_stop_tol = None,\nmax_time_seconds=float('inf'), \nmax_eval_time_seconds=60*10, \nn_jobs=1,\nmemory_limit = \"4GB\",\nclient = None,\ncrossover_probability=.2,\nmutate_probability=.7,\nmutate_then_crossover_probability=.05,\ncrossover_then_mutate_probability=.05,\nsurvival_selector = survival_select_NSGA2,\nparent_selector = tournament_selection_dominated,\nbudget_range = None,\nbudget_scaling = .5,\nindividuals_until_end_budget = 1,  \nstepwise_steps = 5,\nwarm_start = False,\nsubset_column = None,\nverbose = 0,\nperiodic_checkpoint_folder = None, \ncallback = None,\nprocesses = True,\nscatter = True,\noptuna_optimize_pareto_front = False,\noptuna_optimize_pareto_front_trials = 100,\noptuna_optimize_pareto_front_timeout = 60*10,\noptuna_storage = \"sqlite:///optuna.db\",\n):\n'''\n    An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n    Parameters\n    ----------\n    scorers : (list, scorer)\n        A scorer or list of scorers to be used in the cross-validation process. \n        see https://scikit-learn.org/stable/modules/model_evaluation.html\n    scorers_weights : list\n        A list of weights to be applied to the scorers during the optimization process.\n    classification : bool\n        If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n        Used to determine the CV strategy.\n    cv : int, cross-validator\n        - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n        - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n    other_objective_functions : list, default=[tpot2.objectives.estimator_objective_functions.average_path_length_objective]\n        A list of other objective functions to apply to the pipeline.\n    other_objective_functions_weights : list, default=[-1]\n        A list of weights to be applied to the other objective functions.\n    objective_function_names : list, default=None\n        A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n    max_size : int, default=np.inf\n        The maximum number of nodes of the pipelines to be generated.\n    linear_pipeline : bool, default=False\n        If True, the pipelines generated will be linear. If False, the pipelines generated will be directed acyclic graphs.\n    root_config_dict : dict, default='auto'\n        The configuration dictionary to use for the root node of the model.\n        If 'auto', will use \"classifiers\" if classification=True, else \"regressors\".\n        - 'selectors' : A selection of sklearn Selector methods.\n        - 'classifiers' : A selection of sklearn Classifier methods.\n        - 'regressors' : A selection of sklearn Regressor methods.\n        - 'transformers' : A selection of sklearn Transformer methods.\n        - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n        - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n        - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                    Subsets are set with the subsets parameter.\n        - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n        - 'MDR' : Includes MDR.\n        - 'ContinuousMDR' : Includes ContinuousMDR.\n        - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n        - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n        - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n    inner_config_dict : dict, default=[\"selectors\", \"transformers\"]\n        The configuration dictionary to use for the inner nodes of the model generation.\n        Default [\"selectors\", \"transformers\"]\n        - 'selectors' : A selection of sklearn Selector methods.\n        - 'classifiers' : A selection of sklearn Classifier methods.\n        - 'regressors' : A selection of sklearn Regressor methods.\n        - 'transformers' : A selection of sklearn Transformer methods.\n        - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n        - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n        - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                    Subsets are set with the subsets parameter.\n        - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n        - 'MDR' : Includes MDR.\n        - 'ContinuousMDR' : Includes ContinuousMDR.\n        - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n        - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n        - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n        - None : If None and max_depth&gt;1, the root_config_dict will be used for the inner nodes as well.\n    leaf_config_dict : dict, default=None \n        The configuration dictionary to use for the leaf node of the model. If set, leaf nodes must be from this dictionary.\n        Otherwise leaf nodes will be generated from the root_config_dict. \n        Default None\n        - 'selectors' : A selection of sklearn Selector methods.\n        - 'classifiers' : A selection of sklearn Classifier methods.\n        - 'regressors' : A selection of sklearn Regressor methods.\n        - 'transformers' : A selection of sklearn Transformer methods.\n        - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n        - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n        - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                    Subsets are set with the subsets parameter.\n        - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n        - 'MDR' : Includes MDR.\n        - 'ContinuousMDR' : Includes ContinuousMDR.\n        - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n        - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n        - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n        - None : If None, a leaf will not be required (i.e. the pipeline can be a single root node). Leaf nodes will be generated from the inner_config_dict.\n    cross_val_predict_cv : int, default=0\n        Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n        - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n        - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset. \n                However, the output to the next node will come from cross_val_predict with the specified number of folds.\n    categorical_features: list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n        - None : If None, TPOT2 will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n        - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n    subsets : str or list, default=None\n        Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.\n        - str : If a string, it is assumed to be a path to a csv file with the subsets. \n            The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n        - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets.\n        - None : If None, each column will be treated as a subset. One column will be selected per subset.\n        If subsets is None, each column will be treated as a subset. One column will be selected per subset.\n    memory: Memory object or string, default=None\n        If supplied, pipeline will cache each transformer after calling fit. This feature\n        is used to avoid computing the fit transformers within a pipeline if the parameters\n        and input data are identical with another fitted pipeline during optimization process.\n        - String 'auto':\n            TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n        - String path of a caching directory\n            TPOT uses memory caching with the provided directory and TPOT does NOT clean\n            the caching directory up upon shutdown. If the directory does not exist, TPOT will\n            create it.\n        - Memory object:\n            TPOT uses the instance of joblib.Memory for memory caching,\n            and TPOT does NOT clean the caching directory up upon shutdown.\n        - None:\n            TPOT does not use memory caching.\n    preprocessing : bool or BaseEstimator/Pipeline, \n        EXPERIMENTAL\n        A pipeline that will be used to preprocess the data before CV.\n        - bool : If True, will use a default preprocessing pipeline.\n        - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n    validation_strategy : str, default='none'\n        EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT2 may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n        - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n        - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. \n        - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. \n        - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n    validation_fraction : float, default=0.2\n      EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n    population_size : int, default=50\n        Size of the population\n    initial_population_size : int, default=None\n        Size of the initial population. If None, population_size will be used.\n    population_scaling : int, default=0.5\n        Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n    generations_until_end_population : int, default=1  \n        Number of generations until the population size reaches population_size            \n    generations : int, default=50\n        Number of generations to run\n    early_stop : int, default=None\n        Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered.\n    scorers_early_stop_tol : \n        -list of floats\n            list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int \n            If an int is given, it will be used as the tolerance for all objectives\n    other_objectives_early_stop_tol : \n        -list of floats\n            list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int \n            If an int is given, it will be used as the tolerance for all objectives\n    max_time_seconds : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n    max_eval_time_seconds : float, default=60*5\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n    memory_limit : str, default=\"4GB\"\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit. \n    crossover_probability : float, default=.2\n        Probability of generating a new individual by crossover between two individuals.\n    mutate_probability : float, default=.7\n        Probability of generating a new individual by crossover between one individuals.\n    mutate_then_crossover_probability : float, default=.05\n        Probability of generating a new individual by mutating two individuals followed by crossover.\n    crossover_then_mutate_probability : float, default=.05\n        Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n    n_parents : int, default=2\n        Number of parents to use for crossover. Must be greater than 1.\n    survival_selector : function, default=survival_select_NSGA2\n        Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n        Used to selected population_size individuals at the start of each generation to use for mutation and crossover.\n    parent_selector : function, default=parent_select_NSGA2\n        Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n    budget_range : list [start, end], default=None\n        A starting and ending budget to use for the budget scaling.\n    budget_scaling float : [0,1], default=0.5\n        A scaling factor to use when determining how fast we move the budget from the start to end budget.\n    individuals_until_end_budget : int, default=1\n        The number of generations to run before reaching the max budget.\n    stepwise_steps : int, default=1\n        The number of staircase steps to take when scaling the budget and population size.\n    threshold_evaluation_early_stop : list [start, end], default=None\n        starting and ending percentile to use as a threshold for the evaluation early stopping.\n        Values between 0 and 100.\n    threshold_evaluation_scaling : float [0,inf), default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n    min_history_threshold : int, default=0\n        The minimum number of previous scores needed before using threshold early stopping.\n    selection_evaluation_early_stop : list, default=None\n        A lower and upper percent of the population size to select each round of CV.\n        Values between 0 and 1.\n    selection_evaluation_scaling : float, default=0.5 \n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n    n_initial_optimizations : int, default=0\n        Number of individuals to optimize before starting the evolution.\n    optimization_cv : int \n       Number of folds to use for the optuna optimization's internal cross-validation.\n    max_optimize_time_seconds : float, default=60*5\n        Maximum time to run an optimization\n    optimization_steps : int, default=10\n        Number of steps per optimization\n    warm_start : bool, default=False\n        If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n    subset_column : str or int, default=None\n        EXPERIMENTAL The column to use for the subset selection. Must also pass in unique_subset_values to GraphIndividual to function.\n    evolver : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver), default=eaNSGA2_Evolver\n        The evolver to use for the optimization process. See tpot2.evolutionary_algorithms\n        - type : an type or subclass of a BaseEvolver\n        - \"nsga2\" : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver\n    verbose : int, default=1 \n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n        6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_seconds in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n    callback : tpot2.CallBackInterface, default=None\n        Callback object. Not implemented\n    processes : bool, default=True\n        If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.\n        True seems to perform better. However, False is required for interactive debugging.\n    Attributes\n    ----------\n    fitted_pipeline_ : GraphPipeline\n        A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n    evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run. \n        Columns: \n        - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n        - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n        - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n        - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n        - Generation : The generation the pipeline first appeared. \n        - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual. \n                        To save on computational time, the best frontier is updated iteratively each generation. \n                        The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                        All other pipelines are set to NaN. \n        - Instance\t: The unfitted GraphPipeline BaseEstimator. \n        - *validation objective functions : Objective function scores evaluated on the validation set.\n        - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n    pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n    '''\n# sklearn BaseEstimator must have a corresponding attribute for each parameter.\n# These should not be modified once set.\nself.scorers = scorers\nself.scorers_weights = scorers_weights\nself.classification = classification\nself.cv = cv\nself.other_objective_functions = other_objective_functions\nself.other_objective_functions_weights = other_objective_functions_weights\nself.objective_function_names = objective_function_names\nself.bigger_is_better = bigger_is_better\nself.max_size = max_size\nself.linear_pipeline = linear_pipeline\nself.root_config_dict= root_config_dict\nself.inner_config_dict= inner_config_dict\nself.leaf_config_dict= leaf_config_dict\nself.cross_val_predict_cv = cross_val_predict_cv\nself.categorical_features = categorical_features\nself.subsets = subsets\nself.memory = memory\nself.preprocessing = preprocessing\nself.validation_strategy = validation_strategy\nself.validation_fraction = validation_fraction\nself.population_size = population_size\nself.initial_population_size = initial_population_size\nself.early_stop = early_stop\nself.scorers_early_stop_tol = scorers_early_stop_tol\nself.other_objectives_early_stop_tol = other_objectives_early_stop_tol\nself.max_time_seconds = max_time_seconds \nself.max_eval_time_seconds = max_eval_time_seconds\nself.n_jobs= n_jobs\nself.memory_limit = memory_limit\nself.client = client\nself.crossover_probability = crossover_probability\nself.mutate_probability = mutate_probability\nself.mutate_then_crossover_probability= mutate_then_crossover_probability\nself.crossover_then_mutate_probability= crossover_then_mutate_probability\nself.survival_selector=survival_selector\nself.parent_selector=parent_selector\nself.budget_range = budget_range\nself.budget_scaling = budget_scaling\nself.individuals_until_end_budget = individuals_until_end_budget\nself.stepwise_steps = stepwise_steps\nself.warm_start = warm_start\nself.subset_column = subset_column\nself.verbose = verbose\nself.periodic_checkpoint_folder = periodic_checkpoint_folder\nself.callback = callback\nself.processes = processes\nself.scatter = scatter\nself.optuna_optimize_pareto_front = optuna_optimize_pareto_front\nself.optuna_optimize_pareto_front_trials = optuna_optimize_pareto_front_trials\nself.optuna_optimize_pareto_front_timeout = optuna_optimize_pareto_front_timeout\nself.optuna_storage = optuna_storage\nself.max_evaluated_individuals = max_evaluated_individuals\n#Initialize other used params\nif self.initial_population_size is None:\nself._initial_population_size = self.population_size\nelse:\nself._initial_population_size = self.initial_population_size\nif isinstance(self.scorers, str):\nself._scorers = [self.scorers]\nelif callable(self.scorers):\nself._scorers = [self.scorers]\nelse:\nself._scorers = self.scorers\nself._scorers = [sklearn.metrics.get_scorer(scoring) for scoring in self._scorers]\nself._scorers_early_stop_tol = self.scorers_early_stop_tol\nself._evolver = tpot2.evolvers.SteadyStateEvolver\nself.objective_function_weights = [*scorers_weights, *other_objective_functions_weights]\nif self.objective_function_names is None:\nobj_names = [f.__name__ for f in other_objective_functions]\nelse:\nobj_names = self.objective_function_names\nself.objective_names = [f._score_func.__name__ if hasattr(f,\"_score_func\") else f.__name__ for f in self._scorers] + obj_names\nif not isinstance(self.other_objectives_early_stop_tol, list):\nself._other_objectives_early_stop_tol = [self.other_objectives_early_stop_tol for _ in range(len(self.other_objective_functions))]\nelse:\nself._other_objectives_early_stop_tol = self.other_objectives_early_stop_tol\nif not isinstance(self._scorers_early_stop_tol, list):\nself._scorers_early_stop_tol = [self._scorers_early_stop_tol for _ in range(len(self._scorers))]\nelse:\nself._scorers_early_stop_tol = self._scorers_early_stop_tol\nself.early_stop_tol = [*self._scorers_early_stop_tol, *self._other_objectives_early_stop_tol]\nself._evolver_instance = None\nself.evaluated_individuals = None\nset_dask_settings()\n</code></pre>"},{"location":"documentation/tpot2/tpot_estimator/templates/tpottemplates/","title":"Tpottemplates","text":""},{"location":"documentation/tpot2/tpot_estimator/templates/tpottemplates/#tpot2.tpot_estimator.templates.tpottemplates.TPOTClassifier","title":"<code>TPOTClassifier</code>","text":"<p>         Bases: <code>TPOTEstimator</code></p> Source code in <code>tpot2/tpot_estimator/templates/tpottemplates.py</code> <pre><code>class TPOTClassifier(TPOTEstimator):\ndef __init__(       self,\nscorers=['roc_auc_ovr'], \nscorers_weights=[1],\nother_objective_functions=[], #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = [],\nobjective_function_names = None,\nbigger_is_better = True,\nmax_size = np.inf, \nlinear_pipeline = False,\nroot_config_dict= 'Auto',\ninner_config_dict=[\"selectors\", \"transformers\"],\nleaf_config_dict= None,                        \ncross_val_predict_cv = 0,\ncategorical_features = None,\nsubsets = None,\nmemory = None,\npreprocessing = False,\nmax_time_seconds=float('inf'), \nmax_eval_time_seconds=60*10, \nn_jobs = 1,\nvalidation_strategy = \"none\",\nvalidation_fraction = .2, \nearly_stop = None,\nwarm_start = False,\nperiodic_checkpoint_folder = None, \nverbose = 0,\n):\n\"\"\"\n        See TPOTEstimator for documentation\n        \"\"\"\nsuper(TPOTClassifier,self).__init__(\nscorers=scorers, \nscorers_weights=scorers_weights,\ncv = 5,\nother_objective_functions=other_objective_functions, #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = other_objective_functions_weights,\nobjective_function_names = objective_function_names,\nbigger_is_better = bigger_is_better,\nmax_size = max_size, \nlinear_pipeline = linear_pipeline,\nroot_config_dict = root_config_dict,\ninner_config_dict=inner_config_dict,\nleaf_config_dict= leaf_config_dict,                        \ncross_val_predict_cv = cross_val_predict_cv,\ncategorical_features = categorical_features,\nsubsets = subsets,\nmemory = memory,\npreprocessing = preprocessing,\nmax_time_seconds=max_time_seconds, \nmax_eval_time_seconds=max_eval_time_seconds, \nn_jobs=n_jobs,\nvalidation_strategy = validation_strategy,\nvalidation_fraction = validation_fraction, \nearly_stop = early_stop,\nwarm_start = warm_start,\nperiodic_checkpoint_folder = periodic_checkpoint_folder, \nverbose = verbose,\nclassification=True,\n)\ndef predict(self, X, **predict_params):\ncheck_is_fitted(self)\n#X=check_array(X)\nreturn self.fitted_pipeline_.predict(X,**predict_params)\n</code></pre>"},{"location":"documentation/tpot2/tpot_estimator/templates/tpottemplates/#tpot2.tpot_estimator.templates.tpottemplates.TPOTClassifier.__init__","title":"<code>__init__(scorers=['roc_auc_ovr'], scorers_weights=[1], other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, max_size=np.inf, linear_pipeline=False, root_config_dict='Auto', inner_config_dict=['selectors', 'transformers'], leaf_config_dict=None, cross_val_predict_cv=0, categorical_features=None, subsets=None, memory=None, preprocessing=False, max_time_seconds=float('inf'), max_eval_time_seconds=60 * 10, n_jobs=1, validation_strategy='none', validation_fraction=0.2, early_stop=None, warm_start=False, periodic_checkpoint_folder=None, verbose=0)</code>","text":"<p>See TPOTEstimator for documentation</p> Source code in <code>tpot2/tpot_estimator/templates/tpottemplates.py</code> <pre><code>def __init__(       self,\nscorers=['roc_auc_ovr'], \nscorers_weights=[1],\nother_objective_functions=[], #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = [],\nobjective_function_names = None,\nbigger_is_better = True,\nmax_size = np.inf, \nlinear_pipeline = False,\nroot_config_dict= 'Auto',\ninner_config_dict=[\"selectors\", \"transformers\"],\nleaf_config_dict= None,                        \ncross_val_predict_cv = 0,\ncategorical_features = None,\nsubsets = None,\nmemory = None,\npreprocessing = False,\nmax_time_seconds=float('inf'), \nmax_eval_time_seconds=60*10, \nn_jobs = 1,\nvalidation_strategy = \"none\",\nvalidation_fraction = .2, \nearly_stop = None,\nwarm_start = False,\nperiodic_checkpoint_folder = None, \nverbose = 0,\n):\n\"\"\"\n    See TPOTEstimator for documentation\n    \"\"\"\nsuper(TPOTClassifier,self).__init__(\nscorers=scorers, \nscorers_weights=scorers_weights,\ncv = 5,\nother_objective_functions=other_objective_functions, #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = other_objective_functions_weights,\nobjective_function_names = objective_function_names,\nbigger_is_better = bigger_is_better,\nmax_size = max_size, \nlinear_pipeline = linear_pipeline,\nroot_config_dict = root_config_dict,\ninner_config_dict=inner_config_dict,\nleaf_config_dict= leaf_config_dict,                        \ncross_val_predict_cv = cross_val_predict_cv,\ncategorical_features = categorical_features,\nsubsets = subsets,\nmemory = memory,\npreprocessing = preprocessing,\nmax_time_seconds=max_time_seconds, \nmax_eval_time_seconds=max_eval_time_seconds, \nn_jobs=n_jobs,\nvalidation_strategy = validation_strategy,\nvalidation_fraction = validation_fraction, \nearly_stop = early_stop,\nwarm_start = warm_start,\nperiodic_checkpoint_folder = periodic_checkpoint_folder, \nverbose = verbose,\nclassification=True,\n)\n</code></pre>"},{"location":"documentation/tpot2/tpot_estimator/templates/tpottemplates/#tpot2.tpot_estimator.templates.tpottemplates.TPOTRegressor","title":"<code>TPOTRegressor</code>","text":"<p>         Bases: <code>TPOTEstimator</code></p> Source code in <code>tpot2/tpot_estimator/templates/tpottemplates.py</code> <pre><code>class TPOTRegressor(TPOTEstimator):\ndef __init__(       self,\nscorers=['neg_mean_squared_error'], \nscorers_weights=[1],\nother_objective_functions=[], #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = [],\nobjective_function_names = None,\nbigger_is_better = True,\nmax_size = np.inf, \nlinear_pipeline = False,\nroot_config_dict= 'Auto',\ninner_config_dict=[\"selectors\", \"transformers\"],\nleaf_config_dict= None,                        \ncross_val_predict_cv = 0,\ncategorical_features = None,\nsubsets = None,\nmemory = None,\npreprocessing = False,\nmax_time_seconds=float('inf'), \nmax_eval_time_seconds=60*10, \nn_jobs = 1,\nvalidation_strategy = \"none\",\nvalidation_fraction = .2, \nearly_stop = None,\nwarm_start = False,\nperiodic_checkpoint_folder = None, \nverbose = 0,\n):\n\"\"\"\n        See TPOTEstimator for documentation\n        \"\"\"\nsuper(TPOTRegressor,self).__init__(\nscorers=scorers, \nscorers_weights=scorers_weights,\ncv=5,\nother_objective_functions=other_objective_functions, #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = other_objective_functions_weights,\nobjective_function_names = objective_function_names,\nbigger_is_better = bigger_is_better,\nmax_size = max_size, \nlinear_pipeline = linear_pipeline,\nroot_config_dict = root_config_dict,\ninner_config_dict=inner_config_dict,\nleaf_config_dict= leaf_config_dict,                        \ncross_val_predict_cv = cross_val_predict_cv,\ncategorical_features = categorical_features,\nsubsets = subsets,\nmemory = memory,\npreprocessing = preprocessing,\nmax_time_seconds=max_time_seconds, \nmax_eval_time_seconds=max_eval_time_seconds, \nn_jobs=n_jobs,\nvalidation_strategy = validation_strategy,\nvalidation_fraction = validation_fraction, \nearly_stop = early_stop,\nwarm_start = warm_start,\nperiodic_checkpoint_folder = periodic_checkpoint_folder, \nverbose = verbose,\nclassification=False,\n)\n</code></pre>"},{"location":"documentation/tpot2/tpot_estimator/templates/tpottemplates/#tpot2.tpot_estimator.templates.tpottemplates.TPOTRegressor.__init__","title":"<code>__init__(scorers=['neg_mean_squared_error'], scorers_weights=[1], other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, max_size=np.inf, linear_pipeline=False, root_config_dict='Auto', inner_config_dict=['selectors', 'transformers'], leaf_config_dict=None, cross_val_predict_cv=0, categorical_features=None, subsets=None, memory=None, preprocessing=False, max_time_seconds=float('inf'), max_eval_time_seconds=60 * 10, n_jobs=1, validation_strategy='none', validation_fraction=0.2, early_stop=None, warm_start=False, periodic_checkpoint_folder=None, verbose=0)</code>","text":"<p>See TPOTEstimator for documentation</p> Source code in <code>tpot2/tpot_estimator/templates/tpottemplates.py</code> <pre><code>    def __init__(       self,\nscorers=['neg_mean_squared_error'], \nscorers_weights=[1],\nother_objective_functions=[], #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = [],\nobjective_function_names = None,\nbigger_is_better = True,\nmax_size = np.inf, \nlinear_pipeline = False,\nroot_config_dict= 'Auto',\ninner_config_dict=[\"selectors\", \"transformers\"],\nleaf_config_dict= None,                        \ncross_val_predict_cv = 0,\ncategorical_features = None,\nsubsets = None,\nmemory = None,\npreprocessing = False,\nmax_time_seconds=float('inf'), \nmax_eval_time_seconds=60*10, \nn_jobs = 1,\nvalidation_strategy = \"none\",\nvalidation_fraction = .2, \nearly_stop = None,\nwarm_start = False,\nperiodic_checkpoint_folder = None, \nverbose = 0,\n):\n\"\"\"\n        See TPOTEstimator for documentation\n        \"\"\"\nsuper(TPOTRegressor,self).__init__(\nscorers=scorers, \nscorers_weights=scorers_weights,\ncv=5,\nother_objective_functions=other_objective_functions, #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = other_objective_functions_weights,\nobjective_function_names = objective_function_names,\nbigger_is_better = bigger_is_better,\nmax_size = max_size, \nlinear_pipeline = linear_pipeline,\nroot_config_dict = root_config_dict,\ninner_config_dict=inner_config_dict,\nleaf_config_dict= leaf_config_dict,                        \ncross_val_predict_cv = cross_val_predict_cv,\ncategorical_features = categorical_features,\nsubsets = subsets,\nmemory = memory,\npreprocessing = preprocessing,\nmax_time_seconds=max_time_seconds, \nmax_eval_time_seconds=max_eval_time_seconds, \nn_jobs=n_jobs,\nvalidation_strategy = validation_strategy,\nvalidation_fraction = validation_fraction, \nearly_stop = early_stop,\nwarm_start = warm_start,\nperiodic_checkpoint_folder = periodic_checkpoint_folder, \nverbose = verbose,\nclassification=False,\n)\n</code></pre>"},{"location":"documentation/tpot2/utils/eval_utils/","title":"Eval utils","text":""},{"location":"documentation/tpot2/utils/eval_utils/#tpot2.utils.eval_utils.process_scores","title":"<code>process_scores(scores, n)</code>","text":"<p>Purpose: This function processes a list of scores to ensure that each score list has the same length, n. If a score list is shorter than n, the function fills the list with either \"TIMEOUT\" or \"INVALID\" values.</p> <p>Parameters:</p> <pre><code>scores: A list of score lists. Each score list represents a set of scores for a particular player or team. The score lists may have different lengths.\nn: An integer representing the desired length for each score list.\n</code></pre> <p>Returns:</p> <pre><code>The scores list, after processing.\n</code></pre> Source code in <code>tpot2/utils/eval_utils.py</code> <pre><code>def process_scores(scores, n):\n'''\n    Purpose: This function processes a list of scores to ensure that each score list has the same length, n. If a score list is shorter than n, the function fills the list with either \"TIMEOUT\" or \"INVALID\" values.\n    Parameters:\n        scores: A list of score lists. Each score list represents a set of scores for a particular player or team. The score lists may have different lengths.\n        n: An integer representing the desired length for each score list.\n    Returns:\n        The scores list, after processing.\n    '''\nfor i in range(len(scores)):\nif len(scores[i]) &lt; n:\nif \"TIMEOUT\" in scores[i]:\nscores[i] = [\"TIMEOUT\" for j in range(n)]\nelse:\nscores[i] = [\"INVALID\" for j in range(n)]\nreturn scores\n</code></pre>"},{"location":"documentation/tpot2/utils/utils/","title":"Utils","text":""},{"location":"documentation/tpot2/utils/utils/#tpot2.utils.utils.is_pareto_efficient","title":"<code>is_pareto_efficient(scores, return_mask=True)</code>","text":"<p>Find the pareto-efficient points :param scores: An (n_points, n_scores) array :param return_mask: True to return a mask :return: An array of indices of pareto-efficient points.     If return_mask is True, this will be an (n_points, ) boolean array     Otherwise it will be a (n_efficient_points, ) integer array of indices.</p> Source code in <code>tpot2/utils/utils.py</code> <pre><code>def is_pareto_efficient(scores, return_mask = True):\n\"\"\"\n    Find the pareto-efficient points\n    :param scores: An (n_points, n_scores) array\n    :param return_mask: True to return a mask\n    :return: An array of indices of pareto-efficient points.\n        If return_mask is True, this will be an (n_points, ) boolean array\n        Otherwise it will be a (n_efficient_points, ) integer array of indices.\n    \"\"\"\nis_efficient = np.arange(scores.shape[0])\nn_points = scores.shape[0]\nnext_point_index = 0  # Next index in the is_efficient array to search for\nwhile next_point_index&lt;len(scores):\nnondominated_point_mask = np.any(scores&gt;scores[next_point_index], axis=1)\nnondominated_point_mask[next_point_index] = True\nis_efficient = is_efficient[nondominated_point_mask]  # Remove dominated points\nscores = scores[nondominated_point_mask]\nnext_point_index = np.sum(nondominated_point_mask[:next_point_index])+1\nif return_mask:\nis_efficient_mask = np.zeros(n_points, dtype = bool)\nis_efficient_mask[is_efficient] = True\nreturn is_efficient_mask\nelse:\nreturn is_efficient\n</code></pre>"},{"location":"tpot2_api/classifier/","title":"Classifier","text":"<p>         Bases: <code>TPOTEstimator</code></p> Source code in <code>tpot2/tpot_estimator/templates/tpottemplates.py</code> <pre><code>class TPOTClassifier(TPOTEstimator):\ndef __init__(       self,\nscorers=['roc_auc_ovr'], \nscorers_weights=[1],\nother_objective_functions=[], #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = [],\nobjective_function_names = None,\nbigger_is_better = True,\nmax_size = np.inf, \nlinear_pipeline = False,\nroot_config_dict= 'Auto',\ninner_config_dict=[\"selectors\", \"transformers\"],\nleaf_config_dict= None,                        \ncross_val_predict_cv = 0,\ncategorical_features = None,\nsubsets = None,\nmemory = None,\npreprocessing = False,\nmax_time_seconds=float('inf'), \nmax_eval_time_seconds=60*10, \nn_jobs = 1,\nvalidation_strategy = \"none\",\nvalidation_fraction = .2, \nearly_stop = None,\nwarm_start = False,\nperiodic_checkpoint_folder = None, \nverbose = 0,\n):\n\"\"\"\n        See TPOTEstimator for documentation\n        \"\"\"\nsuper(TPOTClassifier,self).__init__(\nscorers=scorers, \nscorers_weights=scorers_weights,\ncv = 5,\nother_objective_functions=other_objective_functions, #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = other_objective_functions_weights,\nobjective_function_names = objective_function_names,\nbigger_is_better = bigger_is_better,\nmax_size = max_size, \nlinear_pipeline = linear_pipeline,\nroot_config_dict = root_config_dict,\ninner_config_dict=inner_config_dict,\nleaf_config_dict= leaf_config_dict,                        \ncross_val_predict_cv = cross_val_predict_cv,\ncategorical_features = categorical_features,\nsubsets = subsets,\nmemory = memory,\npreprocessing = preprocessing,\nmax_time_seconds=max_time_seconds, \nmax_eval_time_seconds=max_eval_time_seconds, \nn_jobs=n_jobs,\nvalidation_strategy = validation_strategy,\nvalidation_fraction = validation_fraction, \nearly_stop = early_stop,\nwarm_start = warm_start,\nperiodic_checkpoint_folder = periodic_checkpoint_folder, \nverbose = verbose,\nclassification=True,\n)\ndef predict(self, X, **predict_params):\ncheck_is_fitted(self)\n#X=check_array(X)\nreturn self.fitted_pipeline_.predict(X,**predict_params)\n</code></pre>"},{"location":"tpot2_api/classifier/#tpot2.tpot_estimator.templates.tpottemplates.TPOTClassifier.__init__","title":"<code>__init__(scorers=['roc_auc_ovr'], scorers_weights=[1], other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, max_size=np.inf, linear_pipeline=False, root_config_dict='Auto', inner_config_dict=['selectors', 'transformers'], leaf_config_dict=None, cross_val_predict_cv=0, categorical_features=None, subsets=None, memory=None, preprocessing=False, max_time_seconds=float('inf'), max_eval_time_seconds=60 * 10, n_jobs=1, validation_strategy='none', validation_fraction=0.2, early_stop=None, warm_start=False, periodic_checkpoint_folder=None, verbose=0)</code>","text":"<p>See TPOTEstimator for documentation</p> Source code in <code>tpot2/tpot_estimator/templates/tpottemplates.py</code> <pre><code>def __init__(       self,\nscorers=['roc_auc_ovr'], \nscorers_weights=[1],\nother_objective_functions=[], #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = [],\nobjective_function_names = None,\nbigger_is_better = True,\nmax_size = np.inf, \nlinear_pipeline = False,\nroot_config_dict= 'Auto',\ninner_config_dict=[\"selectors\", \"transformers\"],\nleaf_config_dict= None,                        \ncross_val_predict_cv = 0,\ncategorical_features = None,\nsubsets = None,\nmemory = None,\npreprocessing = False,\nmax_time_seconds=float('inf'), \nmax_eval_time_seconds=60*10, \nn_jobs = 1,\nvalidation_strategy = \"none\",\nvalidation_fraction = .2, \nearly_stop = None,\nwarm_start = False,\nperiodic_checkpoint_folder = None, \nverbose = 0,\n):\n\"\"\"\n    See TPOTEstimator for documentation\n    \"\"\"\nsuper(TPOTClassifier,self).__init__(\nscorers=scorers, \nscorers_weights=scorers_weights,\ncv = 5,\nother_objective_functions=other_objective_functions, #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = other_objective_functions_weights,\nobjective_function_names = objective_function_names,\nbigger_is_better = bigger_is_better,\nmax_size = max_size, \nlinear_pipeline = linear_pipeline,\nroot_config_dict = root_config_dict,\ninner_config_dict=inner_config_dict,\nleaf_config_dict= leaf_config_dict,                        \ncross_val_predict_cv = cross_val_predict_cv,\ncategorical_features = categorical_features,\nsubsets = subsets,\nmemory = memory,\npreprocessing = preprocessing,\nmax_time_seconds=max_time_seconds, \nmax_eval_time_seconds=max_eval_time_seconds, \nn_jobs=n_jobs,\nvalidation_strategy = validation_strategy,\nvalidation_fraction = validation_fraction, \nearly_stop = early_stop,\nwarm_start = warm_start,\nperiodic_checkpoint_folder = periodic_checkpoint_folder, \nverbose = verbose,\nclassification=True,\n)\n</code></pre>"},{"location":"tpot2_api/estimator/","title":"Estimator","text":""},{"location":"tpot2_api/estimator/#tpot2.tpot_estimator.estimator.TPOTEstimator","title":"<code>TPOTEstimator</code>","text":"<p>         Bases: <code>BaseEstimator</code></p> Source code in <code>tpot2/tpot_estimator/estimator.py</code> <pre><code>class TPOTEstimator(BaseEstimator):\ndef __init__(self,  scorers, \nscorers_weights,\nclassification,\ncv = 5,\nother_objective_functions=[],\nother_objective_functions_weights = [],\nobjective_function_names = None,\nbigger_is_better = True,\nmax_size = np.inf, \nlinear_pipeline = False,\nroot_config_dict= 'Auto',\ninner_config_dict=[\"selectors\", \"transformers\"],\nleaf_config_dict= None,                        \ncross_val_predict_cv = 0,\ncategorical_features = None,\nsubsets = None,\nmemory = None,\npreprocessing = False,\npopulation_size = 50,\ninitial_population_size = None,\npopulation_scaling = .5, \ngenerations_until_end_population = 1,  \ngenerations = 50,\nmax_time_seconds=float('inf'), \nmax_eval_time_seconds=60*10, \nvalidation_strategy = \"none\",\nvalidation_fraction = .2,\n#early stopping parameters \nearly_stop = None,\nscorers_early_stop_tol = 0.001,\nother_objectives_early_stop_tol =None,\nthreshold_evaluation_early_stop = None, \nthreshold_evaluation_scaling = .5,\nselection_evaluation_early_stop = None, \nselection_evaluation_scaling = .5, \nmin_history_threshold = 20,\n#evolver parameters\nsurvival_percentage = 1,\ncrossover_probability=.2,\nmutate_probability=.7,\nmutate_then_crossover_probability=.05,\ncrossover_then_mutate_probability=.05,\nn_parents = 2,\nsurvival_selector = survival_select_NSGA2,\nparent_selector = tournament_selection_dominated,\n#budget parameters\nbudget_range = None,\nbudget_scaling = .5,\ngenerations_until_end_budget = 1,  \nstepwise_steps = 5,\noptuna_optimize_pareto_front = False,\noptuna_optimize_pareto_front_trials = 100,\noptuna_optimize_pareto_front_timeout = 60*10,\noptuna_storage = \"sqlite:///optuna.db\",\n#dask parameters\nn_jobs=1,\nmemory_limit = \"4GB\",\nclient = None,\nprocesses = True,\n#debugging and logging parameters\nwarm_start = False,\nsubset_column = None,\nperiodic_checkpoint_folder = None, \ncallback = None,\nverbose = 0,\nscatter = True,\n):\n'''\n        An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n        Parameters\n        ----------\n        scorers : (list, scorer)\n            A scorer or list of scorers to be used in the cross-validation process. \n            see https://scikit-learn.org/stable/modules/model_evaluation.html\n        scorers_weights : list\n            A list of weights to be applied to the scorers during the optimization process.\n        classification : bool\n            If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n            Used to determine the CV strategy.\n        cv : int, cross-validator\n            - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n            - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n                - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n        other_objective_functions : list, default=[tpot2.objectives.estimator_objective_functions.average_path_length_objective]\n            A list of other objective functions to apply to the pipeline.\n        other_objective_functions_weights : list, default=[-1]\n            A list of weights to be applied to the other objective functions.\n        objective_function_names : list, default=None\n            A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n        bigger_is_better : bool, default=True\n            If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n        max_size : int, default=np.inf\n            The maximum number of nodes of the pipelines to be generated.\n        linear_pipeline : bool, default=False\n            If True, the pipelines generated will be linear. If False, the pipelines generated will be directed acyclic graphs.\n        root_config_dict : dict, default='auto'\n            The configuration dictionary to use for the root node of the model.\n            If 'auto', will use \"classifiers\" if classification=True, else \"regressors\".\n            - 'selectors' : A selection of sklearn Selector methods.\n            - 'classifiers' : A selection of sklearn Classifier methods.\n            - 'regressors' : A selection of sklearn Regressor methods.\n            - 'transformers' : A selection of sklearn Transformer methods.\n            - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n            - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n            - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                        Subsets are set with the subsets parameter.\n            - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n            - 'MDR' : Includes MDR.\n            - 'ContinuousMDR' : Includes ContinuousMDR.\n            - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n            - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n            - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n        inner_config_dict : dict, default=[\"selectors\", \"transformers\"]\n            The configuration dictionary to use for the inner nodes of the model generation.\n            Default [\"selectors\", \"transformers\"]\n            - 'selectors' : A selection of sklearn Selector methods.\n            - 'classifiers' : A selection of sklearn Classifier methods.\n            - 'regressors' : A selection of sklearn Regressor methods.\n            - 'transformers' : A selection of sklearn Transformer methods.\n            - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n            - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n            - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                        Subsets are set with the subsets parameter.\n            - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n            - 'MDR' : Includes MDR.\n            - 'ContinuousMDR' : Includes ContinuousMDR.\n            - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n            - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n            - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n            - None : If None and max_depth&gt;1, the root_config_dict will be used for the inner nodes as well.\n        leaf_config_dict : dict, default=None \n            The configuration dictionary to use for the leaf node of the model. If set, leaf nodes must be from this dictionary.\n            Otherwise leaf nodes will be generated from the root_config_dict. \n            Default None\n            - 'selectors' : A selection of sklearn Selector methods.\n            - 'classifiers' : A selection of sklearn Classifier methods.\n            - 'regressors' : A selection of sklearn Regressor methods.\n            - 'transformers' : A selection of sklearn Transformer methods.\n            - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n            - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n            - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                        Subsets are set with the subsets parameter.\n            - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n            - 'MDR' : Includes MDR.\n            - 'ContinuousMDR' : Includes ContinuousMDR.\n            - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n            - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n            - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n            - None : If None, a leaf will not be required (i.e. the pipeline can be a single root node). Leaf nodes will be generated from the inner_config_dict.\n        cross_val_predict_cv : int, default=0\n            Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n            - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n            - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset. \n                    However, the output to the next node will come from cross_val_predict with the specified number of folds.\n        categorical_features: list or None\n            Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n            - None : If None, TPOT2 will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n            - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n        subsets : str or list, default=None\n            Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.\n            - str : If a string, it is assumed to be a path to a csv file with the subsets. \n                The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n            - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets.\n            - None : If None, each column will be treated as a subset. One column will be selected per subset.\n            If subsets is None, each column will be treated as a subset. One column will be selected per subset.\n        memory: Memory object or string, default=None\n            If supplied, pipeline will cache each transformer after calling fit. This feature\n            is used to avoid computing the fit transformers within a pipeline if the parameters\n            and input data are identical with another fitted pipeline during optimization process.\n            - String 'auto':\n                TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n            - String path of a caching directory\n                TPOT uses memory caching with the provided directory and TPOT does NOT clean\n                the caching directory up upon shutdown. If the directory does not exist, TPOT will\n                create it.\n            - Memory object:\n                TPOT uses the instance of joblib.Memory for memory caching,\n                and TPOT does NOT clean the caching directory up upon shutdown.\n            - None:\n                TPOT does not use memory caching.\n        preprocessing : bool or BaseEstimator/Pipeline, \n            EXPERIMENTAL\n            A pipeline that will be used to preprocess the data before CV.\n            - bool : If True, will use a default preprocessing pipeline.\n            - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n        population_size : int, default=50\n            Size of the population\n        initial_population_size : int, default=None\n            Size of the initial population. If None, population_size will be used.\n        population_scaling : int, default=0.5\n            Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        generations_until_end_population : int, default=1  \n            Number of generations until the population size reaches population_size            \n        generations : int, default=50\n            Number of generations to run\n        max_time_seconds : float, default=float(\"inf\")\n            Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n        max_eval_time_seconds : float, default=60*5\n            Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n        validation_strategy : str, default='none'\n            EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT2 may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n            - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n            - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. \n            - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. \n            - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n        validation_fraction : float, default=0.2\n          EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n        early_stop : int, default=None\n            Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered.\n        scorers_early_stop_tol : \n            -list of floats\n                list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int \n                If an int is given, it will be used as the tolerance for all objectives\n        other_objectives_early_stop_tol : \n            -list of floats\n                list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n                If an index of the list is None, that item will not be used for early stopping\n            -int \n                If an int is given, it will be used as the tolerance for all objectives\n        threshold_evaluation_early_stop : list [start, end], default=None\n            starting and ending percentile to use as a threshold for the evaluation early stopping.\n            Values between 0 and 100.\n        threshold_evaluation_scaling : float [0,inf), default=0.5\n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.\n        selection_evaluation_early_stop : list, default=None\n            A lower and upper percent of the population size to select each round of CV.\n            Values between 0 and 1.\n        selection_evaluation_scaling : float, default=0.5 \n            A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n            Must be greater than zero. Higher numbers will move the threshold to the end faster.    \n        min_history_threshold : int, default=0\n            The minimum number of previous scores needed before using threshold early stopping.\n        evolver : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver), default=eaNSGA2_Evolver\n            The evolver to use for the optimization process. See tpot2.evolutionary_algorithms\n            - type : an type or subclass of a BaseEvolver\n            - \"nsga2\" : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver\n        survival_percentage : float, default=1\n            Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive. \n            For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.\n        crossover_probability : float, default=.2\n            Probability of generating a new individual by crossover between two individuals.\n        mutate_probability : float, default=.7\n            Probability of generating a new individual by crossover between one individuals.\n        mutate_then_crossover_probability : float, default=.05\n            Probability of generating a new individual by mutating two individuals followed by crossover.\n        crossover_then_mutate_probability : float, default=.05\n            Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n        n_parents : int, default=2\n            Number of parents to use for crossover. Must be greater than 1.\n        survival_selector : function, default=survival_select_NSGA2\n            Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n            Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n        parent_selector : function, default=parent_select_NSGA2\n            Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n        budget_range : list [start, end], default=None\n            A starting and ending budget to use for the budget scaling.\n        budget_scaling float : [0,1], default=0.5\n            A scaling factor to use when determining how fast we move the budget from the start to end budget.\n        generations_until_end_budget : int, default=1\n            The number of generations to run before reaching the max budget.\n        stepwise_steps : int, default=1\n            The number of staircase steps to take when scaling the budget and population size.\n        n_jobs : int, default=1\n            Number of processes to run in parallel.\n        memory_limit : str, default=\"4GB\"\n            Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n        client : dask.distributed.Client, default=None\n            A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit. \n        processes : bool, default=True\n            If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.\n            True seems to perform better. However, False is required for interactive debugging.\n        warm_start : bool, default=False\n            If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n        subset_column : str or int, default=None\n            EXPERIMENTAL The column to use for the subset selection. Must also pass in unique_subset_values to GraphIndividual to function.\n        periodic_checkpoint_folder : str, default=None\n            Folder to save the population to periodically. If None, no periodic saving will be done.\n            If provided, training will resume from this checkpoint.\n        callback : tpot2.CallBackInterface, default=None\n            Callback object. Not implemented\n        verbose : int, default=1 \n            How much information to print during the optimization process. Higher values include the information from lower values.\n            0. nothing\n            1. progress bar\n            3. best individual\n            4. warnings\n            &gt;=5. full warnings trace\n            6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_seconds in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n        Attributes\n        ----------\n        fitted_pipeline_ : GraphPipeline\n            A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n        evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run. \n            Columns: \n            - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n            - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n            - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n            - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n            - Generation : The generation the pipeline first appeared. \n            - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual. \n                            To save on computational time, the best frontier is updated iteratively each generation. \n                            The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                            All other pipelines are set to NaN. \n            - Instance\t: The unfitted GraphPipeline BaseEstimator. \n            - *validation objective functions : Objective function scores evaluated on the validation set.\n            - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n        pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n        '''\n# sklearn BaseEstimator must have a corresponding attribute for each parameter.\n# These should not be modified once set.\nself.scorers = scorers\nself.scorers_weights = scorers_weights\nself.classification = classification\nself.cv = cv\nself.other_objective_functions = other_objective_functions\nself.other_objective_functions_weights = other_objective_functions_weights\nself.objective_function_names = objective_function_names\nself.bigger_is_better = bigger_is_better\nself.max_size = max_size\nself.linear_pipeline = linear_pipeline\nself.root_config_dict= root_config_dict\nself.inner_config_dict= inner_config_dict\nself.leaf_config_dict= leaf_config_dict\nself.cross_val_predict_cv = cross_val_predict_cv\nself.categorical_features = categorical_features\nself.subsets = subsets\nself.memory = memory\nself.preprocessing = preprocessing\nself.validation_strategy = validation_strategy\nself.validation_fraction = validation_fraction\nself.population_size = population_size\nself.initial_population_size = initial_population_size\nself.population_scaling = population_scaling\nself.generations_until_end_population = generations_until_end_population\nself.generations = generations\nself.early_stop = early_stop\nself.scorers_early_stop_tol = scorers_early_stop_tol\nself.other_objectives_early_stop_tol = other_objectives_early_stop_tol\nself.max_time_seconds = max_time_seconds \nself.max_eval_time_seconds = max_eval_time_seconds\nself.n_jobs= n_jobs\nself.memory_limit = memory_limit\nself.client = client\nself.survival_percentage = survival_percentage\nself.crossover_probability = crossover_probability\nself.mutate_probability = mutate_probability\nself.mutate_then_crossover_probability= mutate_then_crossover_probability\nself.crossover_then_mutate_probability= crossover_then_mutate_probability\nself.survival_selector=survival_selector\nself.parent_selector=parent_selector\nself.budget_range = budget_range\nself.budget_scaling = budget_scaling\nself.generations_until_end_budget = generations_until_end_budget\nself.stepwise_steps = stepwise_steps\nself.threshold_evaluation_early_stop =threshold_evaluation_early_stop\nself.threshold_evaluation_scaling =  threshold_evaluation_scaling\nself.min_history_threshold = min_history_threshold\nself.selection_evaluation_early_stop = selection_evaluation_early_stop\nself.selection_evaluation_scaling =  selection_evaluation_scaling\nself.warm_start = warm_start\nself.subset_column = subset_column\nself.verbose = verbose\nself.periodic_checkpoint_folder = periodic_checkpoint_folder\nself.callback = callback\nself.processes = processes\nself.scatter = scatter\nself.optuna_optimize_pareto_front = optuna_optimize_pareto_front\nself.optuna_optimize_pareto_front_trials = optuna_optimize_pareto_front_trials\nself.optuna_optimize_pareto_front_timeout = optuna_optimize_pareto_front_timeout\nself.optuna_storage = optuna_storage\n#Initialize other used params\nif self.initial_population_size is None:\nself._initial_population_size = self.population_size\nelse:\nself._initial_population_size = self.initial_population_size\nif isinstance(self.scorers, str):\nself._scorers = [self.scorers]\nelif callable(self.scorers):\nself._scorers = [self.scorers]\nelse:\nself._scorers = self.scorers\nself._scorers = [sklearn.metrics.get_scorer(scoring) for scoring in self._scorers]\nself._scorers_early_stop_tol = self.scorers_early_stop_tol\nself._evolver = tpot2.evolvers.BaseEvolver\nself.objective_function_weights = [*scorers_weights, *other_objective_functions_weights]\nif self.objective_function_names is None:\nobj_names = [f.__name__ for f in other_objective_functions]\nelse:\nobj_names = self.objective_function_names\nself.objective_names = [f._score_func.__name__ if hasattr(f,\"_score_func\") else f.__name__ for f in self._scorers] + obj_names\nif not isinstance(self.other_objectives_early_stop_tol, list):\nself._other_objectives_early_stop_tol = [self.other_objectives_early_stop_tol for _ in range(len(self.other_objective_functions))]\nelse:\nself._other_objectives_early_stop_tol = self.other_objectives_early_stop_tol\nif not isinstance(self._scorers_early_stop_tol, list):\nself._scorers_early_stop_tol = [self._scorers_early_stop_tol for _ in range(len(self._scorers))]\nelse:\nself._scorers_early_stop_tol = self._scorers_early_stop_tol\nself.early_stop_tol = [*self._scorers_early_stop_tol, *self._other_objectives_early_stop_tol]\nself._evolver_instance = None\nself.evaluated_individuals = None\nset_dask_settings()\ndef fit(self, X, y):\nif self.client is not None: #If user passed in a client manually\n_client = self.client\nelse:\nif self.verbose &gt;= 4:\nsilence_logs = 30\nelif self.verbose &gt;=5:\nsilence_logs = 40\nelse:\nsilence_logs = 50\ncluster = LocalCluster(n_workers=self.n_jobs, #if no client is passed in and no global client exists, create our own\nthreads_per_worker=1,\nprocesses=self.processes,\nsilence_logs=silence_logs,\nmemory_limit=self.memory_limit)\n_client = Client(cluster)\nself.evaluated_individuals = None\n#determine validation strategy\nif self.validation_strategy == 'auto':\nnrows = X.shape[0]\nncols = X.shape[1]\nif nrows/ncols &lt; 20:\nvalidation_strategy = 'reshuffled'\nelif nrows/ncols &lt; 100:\nvalidation_strategy = 'split'\nelse:\nvalidation_strategy = 'none'\nelse:\nvalidation_strategy = self.validation_strategy\nif validation_strategy == 'split':\nif self.classification:\nX, X_val, y, y_val = train_test_split(X, y, test_size=self.validation_fraction, stratify=y, random_state=42)\nelse:\nX, X_val, y, y_val = train_test_split(X, y, test_size=self.validation_fraction, random_state=42)\nX_original = X\ny_original = y\nif isinstance(self.cv, int) or isinstance(self.cv, float):\nn_folds = self.cv\nelse:\nn_folds = self.cv.get_n_splits(X, y)\nX, y = remove_underrepresented_classes(X, y, n_folds)\nif self.preprocessing:\n#X = pd.DataFrame(X)\n#TODO: check if there are missing values in X before imputation. If not, don't include imputation in pipeline. Check if there are categorical columns. If not, don't include one hot encoding in pipeline\nif isinstance(X, pd.DataFrame): #pandas dataframe\nif self.categorical_features is not None:\nX[self.categorical_features] = X[self.categorical_features].astype(object)\nself._preprocessing_pipeline = sklearn.pipeline.make_pipeline(tpot2.builtin_modules.ColumnSimpleImputer(\"categorical\", strategy='most_frequent'), #impute categorical columns\ntpot2.builtin_modules.ColumnSimpleImputer(\"numeric\", strategy='mean'),              #impute numeric columns\ntpot2.builtin_modules.ColumnOneHotEncoder(\"categorical\", min_frequency=0.0001))     #one hot encode categorical columns\nX = self._preprocessing_pipeline.fit_transform(X)\nelse:\nif self.categorical_features is not None: #numpy array and categorical columns specified\nself._preprocessing_pipeline = sklearn.pipeline.make_pipeline(tpot2.builtin_modules.ColumnSimpleImputer(self.categorical_features, strategy='most_frequent'),   #impute categorical columns\ntpot2.builtin_modules.ColumnSimpleImputer(\"all\", strategy='mean'),                                      #impute remaining numeric columns\ntpot2.builtin_modules.ColumnOneHotEncoder(self.categorical_features, min_frequency=0.0001))             #one hot encode categorical columns\nelse: #numpy array and no categorical columns specified, just do imputation\nself._preprocessing_pipeline = sklearn.pipeline.make_pipeline(tpot2.builtin_modules.ColumnSimpleImputer(\"all\", strategy='mean'))   \nelse:\nself._preprocessing_pipeline = None\n#_, y = sklearn.utils.check_X_y(X, y, y_numeric=True)\n#Set up the configuation dictionaries and the search spaces\nn_samples= int(math.floor(X.shape[0]/n_folds))\nn_features=X.shape[1]\nif isinstance(X, pd.DataFrame):\nself.feature_names = X.columns\nelse:\nself.feature_names = None\nif self.root_config_dict == 'Auto':\nif self.classification:\nn_classes = len(np.unique(y))\nroot_config_dict = get_configuration_dictionary(\"classifiers\", n_samples, n_features, self.classification, subsets=self.subsets, feature_names=self.feature_names, n_classes=n_classes)\nelse:\nroot_config_dict = get_configuration_dictionary(\"regressors\", n_samples, n_features, self.classification,subsets=self.subsets, feature_names=self.feature_names)\nelse:\nroot_config_dict = get_configuration_dictionary(self.root_config_dict, n_samples, n_features, self.classification, subsets=self.subsets,feature_names=self.feature_names)\ninner_config_dict = get_configuration_dictionary(self.inner_config_dict, n_samples, n_features, self.classification,subsets=self.subsets, feature_names=self.feature_names)\nleaf_config_dict = get_configuration_dictionary(self.leaf_config_dict, n_samples, n_features, self.classification, subsets=self.subsets, feature_names=self.feature_names)\n#check if self.cv is a number\nif isinstance(self.cv, int) or isinstance(self.cv, float):\nif self.classification:\nself.cv_gen = sklearn.model_selection.StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=42)\nelse:\nself.cv_gen = sklearn.model_selection.KFold(n_splits=self.cv, shuffle=True, random_state=42)\nelse:\nself.cv_gen = sklearn.model_selection.check_cv(self.cv, y, classifier=self.classification)\ndef objective_function(pipeline_individual, \nX, \ny,\nis_classification=self.classification,\nscorers= self._scorers, \ncv=self.cv_gen, \nother_objective_functions=self.other_objective_functions,\nmemory=self.memory, \ncross_val_predict_cv=self.cross_val_predict_cv, \nsubset_column=self.subset_column, \n**kwargs): \nreturn objective_function_generator(\npipeline_individual,\nX, \ny, \nis_classification=is_classification,\nscorers= scorers, \ncv=cv, \nother_objective_functions=other_objective_functions,\nmemory=memory, \ncross_val_predict_cv=cross_val_predict_cv, \nsubset_column=subset_column,\n**kwargs,\n)\nself.individual_generator_instance = tpot2.individual_representations.graph_pipeline_individual.estimator_graph_individual_generator(   \ninner_config_dict=inner_config_dict,\nroot_config_dict=root_config_dict,\nleaf_config_dict=leaf_config_dict,\nmax_size = self.max_size,\nlinear_pipeline=self.linear_pipeline,\n)\nif self.threshold_evaluation_early_stop is not None or self.selection_evaluation_early_stop is not None:\nevaluation_early_stop_steps = self.cv\nelse:\nevaluation_early_stop_steps = None\nif self.scatter:\nX_future = _client.scatter(X)\ny_future = _client.scatter(y)\nelse:\nX_future = X\ny_future = y\n#If warm start and we have an evolver instance, use the existing one\nif not(self.warm_start and self._evolver_instance is not None):\nself._evolver_instance = self._evolver(   individual_generator=self.individual_generator_instance, \nobjective_functions= [objective_function],\nobjective_function_weights = self.objective_function_weights,\nobjective_names=self.objective_names,\nbigger_is_better = self.bigger_is_better,\npopulation_size= self.population_size,\ngenerations=self.generations,\ninitial_population_size = self._initial_population_size,\nn_jobs=self.n_jobs,\nverbose = self.verbose,\nmax_time_seconds =      self.max_time_seconds ,\nmax_eval_time_seconds = self.max_eval_time_seconds,\nperiodic_checkpoint_folder = self.periodic_checkpoint_folder,\nthreshold_evaluation_early_stop = self.threshold_evaluation_early_stop,\nthreshold_evaluation_scaling =  self.threshold_evaluation_scaling,\nmin_history_threshold = self.min_history_threshold,\nselection_evaluation_early_stop = self.selection_evaluation_early_stop,\nselection_evaluation_scaling =  self.selection_evaluation_scaling,\nevaluation_early_stop_steps = evaluation_early_stop_steps,\nearly_stop_tol = self.early_stop_tol,\nearly_stop= self.early_stop,\nbudget_range = self.budget_range,\nbudget_scaling = self.budget_scaling,\ngenerations_until_end_budget = self.generations_until_end_budget,\npopulation_scaling = self.population_scaling,\ngenerations_until_end_population = self.generations_until_end_population,\nstepwise_steps = self.stepwise_steps,\nclient = _client,\nobjective_kwargs = {\"X\": X_future, \"y\": y_future},\nsurvival_selector=self.survival_selector,\nparent_selector=self.parent_selector,\nsurvival_percentage = self.survival_percentage,\ncrossover_probability = self.crossover_probability,\nmutate_probability = self.mutate_probability,\nmutate_then_crossover_probability= self.mutate_then_crossover_probability,\ncrossover_then_mutate_probability= self.crossover_then_mutate_probability,\n)\nself._evolver_instance.optimize()\n#self._evolver_instance.population.update_pareto_fronts(self.objective_names, self.objective_function_weights)\nself.make_evaluated_individuals()\nif self.optuna_optimize_pareto_front:\npareto_front_inds = self.pareto_front['Individual'].values\nall_graphs, all_scores = tpot2.individual_representations.graph_pipeline_individual.simple_parallel_optuna(pareto_front_inds,  objective_function, self.objective_function_weights, _client, storage=self.optuna_storage, steps=self.optuna_optimize_pareto_front_trials, verbose=self.verbose, max_eval_time_seconds=self.max_eval_time_seconds, max_time_seconds=self.optuna_optimize_pareto_front_timeout, **{\"X\": X, \"y\": y})\nall_scores = tpot2.utils.eval_utils.process_scores(all_scores, len(self.objective_function_weights))\nif len(all_graphs) &gt; 0:\ndf = pd.DataFrame(np.column_stack((all_graphs, all_scores,np.repeat(\"Optuna\",len(all_graphs)))), columns=[\"Individual\"] + self.objective_names +[\"Parents\"])\nfor obj in self.objective_names:\ndf[obj] = df[obj].apply(convert_to_float)\nself.evaluated_individuals = pd.concat([self.evaluated_individuals, df], ignore_index=True)\nelse:\nprint(\"WARNING NO OPTUNA TRIALS COMPLETED\")\ntpot2.utils.get_pareto_frontier(self.evaluated_individuals, column_names=self.objective_names, weights=self.objective_function_weights, invalid_values=[\"TIMEOUT\",\"INVALID\"])\nif validation_strategy == 'reshuffled':\nbest_pareto_front_idx = list(self.pareto_front.index)\nbest_pareto_front = list(self.pareto_front.loc[best_pareto_front_idx]['Individual'])\n#reshuffle rows\nX, y = sklearn.utils.shuffle(X, y, random_state=1)\nif self.scatter:\nX_future = _client.scatter(X)\ny_future = _client.scatter(y)\nelse:\nX_future = X\ny_future = y\nval_objective_function_list = [lambda   ind, \nX, \ny, \nis_classification=self.classification,\nscorers= self._scorers, \ncv=self.cv_gen, \nother_objective_functions=self.other_objective_functions, \nmemory=self.memory, \ncross_val_predict_cv=self.cross_val_predict_cv, \nsubset_column=self.subset_column, \n**kwargs: objective_function_generator(\nind,\nX,\ny, \nis_classification=is_classification,\nscorers= scorers, \ncv=cv, \nother_objective_functions=other_objective_functions,\nmemory=memory, \ncross_val_predict_cv=cross_val_predict_cv, \nsubset_column=subset_column,\n**kwargs,\n)]\nobjective_kwargs = {\"X\": X_future, \"y\": y_future}\nval_scores = tpot2.utils.eval_utils.parallel_eval_objective_list(\nbest_pareto_front,\nval_objective_function_list, n_jobs=self.n_jobs, verbose=self.verbose, timeout=self.max_eval_time_seconds,n_expected_columns=len(self.objective_names), client=_client, **objective_kwargs)\nval_objective_names = ['validation_'+name for name in self.objective_names]\nself.objective_names_for_selection = val_objective_names\nself.evaluated_individuals.loc[best_pareto_front_idx,val_objective_names] = val_scores\nself.evaluated_individuals[\"Validation_Pareto_Front\"] = tpot2.utils.get_pareto_front(self.evaluated_individuals, val_objective_names, self.objective_function_weights, invalid_values=[\"TIMEOUT\",\"INVALID\"])\nelif validation_strategy == 'split':\nif self.scatter:            \nX_future = _client.scatter(X)\ny_future = _client.scatter(y)\nX_val_future = _client.scatter(X_val)\ny_val_future = _client.scatter(y_val)\nelse:\nX_future = X\ny_future = y\nX_val_future = X_val\ny_val_future = y_val\nobjective_kwargs = {\"X\": X_future, \"y\": y_future, \"X_val\" : X_val_future, \"y_val\":y_val_future }\nbest_pareto_front_idx = list(self.pareto_front.index)\nbest_pareto_front = list(self.pareto_front.loc[best_pareto_front_idx]['Individual'])\nval_objective_function_list = [lambda   ind, \nX, \ny, \nX_val, \ny_val, \nscorers= self._scorers, \nother_objective_functions=self.other_objective_functions, \nmemory=self.memory, \ncross_val_predict_cv=self.cross_val_predict_cv, \nsubset_column=self.subset_column, \n**kwargs: val_objective_function_generator(\nind,\nX,\ny,\nX_val, \ny_val, \nscorers= scorers, \nother_objective_functions=other_objective_functions,\nmemory=memory, \ncross_val_predict_cv=cross_val_predict_cv, \nsubset_column=subset_column,\n**kwargs,\n)]\nval_scores = tpot2.utils.eval_utils.parallel_eval_objective_list(\nbest_pareto_front,\nval_objective_function_list, n_jobs=self.n_jobs, verbose=self.verbose, timeout=self.max_eval_time_seconds,n_expected_columns=len(self.objective_names),client=_client, **objective_kwargs)\nval_objective_names = ['validation_'+name for name in self.objective_names]\nself.objective_names_for_selection = val_objective_names\nself.evaluated_individuals.loc[best_pareto_front_idx,val_objective_names] = val_scores\nself.evaluated_individuals[\"Validation_Pareto_Front\"] = tpot2.utils.get_pareto_front(self.evaluated_individuals, val_objective_names, self.objective_function_weights, invalid_values=[\"TIMEOUT\",\"INVALID\"])\nelse:\nself.objective_names_for_selection = self.objective_names\nval_scores = self.evaluated_individuals[~self.evaluated_individuals[self.objective_names_for_selection].isin([\"TIMEOUT\",\"INVALID\"]).any(axis=1)][self.objective_names_for_selection].astype(float)                                     \nweighted_scores = val_scores*self.objective_function_weights\nif self.bigger_is_better:\nbest_idx = weighted_scores[self.objective_names_for_selection[0]].idxmax()\nelse:\nbest_idx = weighted_scores[self.objective_names_for_selection[0]].idxmin()\nbest_individual = self.evaluated_individuals.loc[best_idx]['Individual']\nself.selected_best_score =  self.evaluated_individuals.loc[best_idx]\nbest_individual_pipeline = best_individual.export_pipeline(memory=self.memory, cross_val_predict_cv=self.cross_val_predict_cv, subset_column=self.subset_column)\nif self.preprocessing:\nself.fitted_pipeline_ = sklearn.pipeline.make_pipeline(sklearn.base.clone(self._preprocessing_pipeline), best_individual_pipeline )\nelse:\nself.fitted_pipeline_ = best_individual_pipeline \nself.fitted_pipeline_.fit(X_original,y_original) #TODO use y_original as well?\nif self.client is None: #no client was passed in\n#close cluster and client\n_client.close()\ncluster.close()\nreturn self\ndef _estimator_has(attr):\n'''Check if we can delegate a method to the underlying estimator.\n        First, we check the first fitted final estimator if available, otherwise we\n        check the unfitted final estimator.\n        '''\nreturn  lambda self: (self.fitted_pipeline_ is not None and\nhasattr(self.fitted_pipeline_, attr)\n)\n@available_if(_estimator_has('predict'))\ndef predict(self, X, **predict_params):\ncheck_is_fitted(self)\n#X = check_array(X)\nreturn self.fitted_pipeline_.predict(X,**predict_params)\n@available_if(_estimator_has('predict_proba'))\ndef predict_proba(self, X, **predict_params):\ncheck_is_fitted(self)\n#X = check_array(X)\nreturn self.fitted_pipeline_.predict_proba(X,**predict_params)\n@available_if(_estimator_has('decision_function'))\ndef decision_function(self, X, **predict_params):\ncheck_is_fitted(self)\n#X = check_array(X)\nreturn self.fitted_pipeline_.decision_function(X,**predict_params)\n@available_if(_estimator_has('transform'))\ndef transform(self, X, **predict_params):\ncheck_is_fitted(self)\n#X = check_array(X)\nreturn self.fitted_pipeline_.transform(X,**predict_params)\n@property\ndef classes_(self):\n\"\"\"The classes labels. Only exist if the last step is a classifier.\"\"\"\nreturn self.fitted_pipeline_.classes_\ndef make_evaluated_individuals(self):\n#check if _evolver_instance exists\nif self.evaluated_individuals is None:\nself.evaluated_individuals  =  self._evolver_instance.population.evaluated_individuals.copy()\nobjects = list(self.evaluated_individuals.index)\nobject_to_int = dict(zip(objects, range(len(objects))))\nself.evaluated_individuals = self.evaluated_individuals.set_index(self.evaluated_individuals.index.map(object_to_int))\nself.evaluated_individuals['Parents'] = self.evaluated_individuals['Parents'].apply(lambda row: convert_parents_tuples_to_integers(row, object_to_int))\nself.evaluated_individuals[\"Instance\"] = self.evaluated_individuals[\"Individual\"].apply(lambda ind: apply_make_pipeline(ind, preprocessing_pipeline=self._preprocessing_pipeline))\nreturn self.evaluated_individuals\n@property\ndef pareto_front(self):\n#check if _evolver_instance exists\nif self.evaluated_individuals is None:\nreturn None\nelse:\nif \"Pareto_Front\" not in self.evaluated_individuals:\nreturn self.evaluated_individuals\nelse:\nreturn self.evaluated_individuals[self.evaluated_individuals[\"Pareto_Front\"]==1]\n</code></pre>"},{"location":"tpot2_api/estimator/#tpot2.tpot_estimator.estimator.TPOTEstimator.classes_","title":"<code>classes_</code>  <code>property</code>","text":"<p>The classes labels. Only exist if the last step is a classifier.</p>"},{"location":"tpot2_api/estimator/#tpot2.tpot_estimator.estimator.TPOTEstimator.__init__","title":"<code>__init__(scorers, scorers_weights, classification, cv=5, other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, max_size=np.inf, linear_pipeline=False, root_config_dict='Auto', inner_config_dict=['selectors', 'transformers'], leaf_config_dict=None, cross_val_predict_cv=0, categorical_features=None, subsets=None, memory=None, preprocessing=False, population_size=50, initial_population_size=None, population_scaling=0.5, generations_until_end_population=1, generations=50, max_time_seconds=float('inf'), max_eval_time_seconds=60 * 10, validation_strategy='none', validation_fraction=0.2, early_stop=None, scorers_early_stop_tol=0.001, other_objectives_early_stop_tol=None, threshold_evaluation_early_stop=None, threshold_evaluation_scaling=0.5, selection_evaluation_early_stop=None, selection_evaluation_scaling=0.5, min_history_threshold=20, survival_percentage=1, crossover_probability=0.2, mutate_probability=0.7, mutate_then_crossover_probability=0.05, crossover_then_mutate_probability=0.05, n_parents=2, survival_selector=survival_select_NSGA2, parent_selector=tournament_selection_dominated, budget_range=None, budget_scaling=0.5, generations_until_end_budget=1, stepwise_steps=5, optuna_optimize_pareto_front=False, optuna_optimize_pareto_front_trials=100, optuna_optimize_pareto_front_timeout=60 * 10, optuna_storage='sqlite:///optuna.db', n_jobs=1, memory_limit='4GB', client=None, processes=True, warm_start=False, subset_column=None, periodic_checkpoint_folder=None, callback=None, verbose=0, scatter=True)</code>","text":"<p>An sklearn baseestimator that uses genetic programming to optimize a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>scorers</code> <code>(list, scorer)</code> <p>A scorer or list of scorers to be used in the cross-validation process.  see https://scikit-learn.org/stable/modules/model_evaluation.html</p> required <p>scorers_weights : list     A list of weights to be applied to the scorers during the optimization process.</p> <p>classification : bool     If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.     Used to determine the CV strategy.</p> <p>cv : int, cross-validator     - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.     - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.         - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.</p> <p>other_objective_functions : list, default=[tpot2.objectives.estimator_objective_functions.average_path_length_objective]     A list of other objective functions to apply to the pipeline.</p> <p>other_objective_functions_weights : list, default=[-1]     A list of weights to be applied to the other objective functions.</p> <p>objective_function_names : list, default=None     A list of names to be applied to the objective functions. If None, will use the names of the objective functions.</p> <p>bigger_is_better : bool, default=True     If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.</p> <p>max_size : int, default=np.inf     The maximum number of nodes of the pipelines to be generated.</p> <p>linear_pipeline : bool, default=False     If True, the pipelines generated will be linear. If False, the pipelines generated will be directed acyclic graphs.</p> <p>root_config_dict : dict, default='auto'     The configuration dictionary to use for the root node of the model.     If 'auto', will use \"classifiers\" if classification=True, else \"regressors\".     - 'selectors' : A selection of sklearn Selector methods.     - 'classifiers' : A selection of sklearn Classifier methods.     - 'regressors' : A selection of sklearn Regressor methods.     - 'transformers' : A selection of sklearn Transformer methods.     - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.     - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.     - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.                                 Subsets are set with the subsets parameter.     - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.     - 'MDR' : Includes MDR.     - 'ContinuousMDR' : Includes ContinuousMDR.     - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.     - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.     - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.</p> <p>inner_config_dict : dict, default=[\"selectors\", \"transformers\"]     The configuration dictionary to use for the inner nodes of the model generation.     Default [\"selectors\", \"transformers\"]     - 'selectors' : A selection of sklearn Selector methods.     - 'classifiers' : A selection of sklearn Classifier methods.     - 'regressors' : A selection of sklearn Regressor methods.     - 'transformers' : A selection of sklearn Transformer methods.     - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.     - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.     - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.                                 Subsets are set with the subsets parameter.     - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.     - 'MDR' : Includes MDR.     - 'ContinuousMDR' : Includes ContinuousMDR.     - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.     - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.     - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.     - None : If None and max_depth&gt;1, the root_config_dict will be used for the inner nodes as well.</p> <p>leaf_config_dict : dict, default=None      The configuration dictionary to use for the leaf node of the model. If set, leaf nodes must be from this dictionary.     Otherwise leaf nodes will be generated from the root_config_dict.      Default None     - 'selectors' : A selection of sklearn Selector methods.     - 'classifiers' : A selection of sklearn Classifier methods.     - 'regressors' : A selection of sklearn Regressor methods.     - 'transformers' : A selection of sklearn Transformer methods.     - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.     - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.     - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.                                 Subsets are set with the subsets parameter.     - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.     - 'MDR' : Includes MDR.     - 'ContinuousMDR' : Includes ContinuousMDR.     - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.     - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.     - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.     - None : If None, a leaf will not be required (i.e. the pipeline can be a single root node). Leaf nodes will be generated from the inner_config_dict.</p> <p>cross_val_predict_cv : int, default=0     Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.</p> <pre><code>- 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n- &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset. \n        However, the output to the next node will come from cross_val_predict with the specified number of folds.\n</code></pre> <p>categorical_features: list or None     Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.     - None : If None, TPOT2 will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.     - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices</p> <p>subsets : str or list, default=None     Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.     - str : If a string, it is assumed to be a path to a csv file with the subsets.          The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.     - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets.     - None : If None, each column will be treated as a subset. One column will be selected per subset.     If subsets is None, each column will be treated as a subset. One column will be selected per subset.</p> <p>memory: Memory object or string, default=None     If supplied, pipeline will cache each transformer after calling fit. This feature     is used to avoid computing the fit transformers within a pipeline if the parameters     and input data are identical with another fitted pipeline during optimization process.     - String 'auto':         TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.     - String path of a caching directory         TPOT uses memory caching with the provided directory and TPOT does NOT clean         the caching directory up upon shutdown. If the directory does not exist, TPOT will         create it.     - Memory object:         TPOT uses the instance of joblib.Memory for memory caching,         and TPOT does NOT clean the caching directory up upon shutdown.     - None:         TPOT does not use memory caching.</p> <p>preprocessing : bool or BaseEstimator/Pipeline,      EXPERIMENTAL     A pipeline that will be used to preprocess the data before CV.     - bool : If True, will use a default preprocessing pipeline.     - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.</p> <p>population_size : int, default=50     Size of the population</p> <p>initial_population_size : int, default=None     Size of the initial population. If None, population_size will be used.</p> <p>population_scaling : int, default=0.5     Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.</p> <p>generations_until_end_population : int, default=1     Number of generations until the population size reaches population_size            </p> <p>generations : int, default=50     Number of generations to run</p> <p>max_time_seconds : float, default=float(\"inf\")     Maximum time to run the optimization. If none or inf, will run until the end of the generations.</p> <p>max_eval_time_seconds : float, default=60*5     Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.</p> <p>validation_strategy : str, default='none'     EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT2 may overfit the cross validation score. A second validation set can be used to select the final pipeline.     - 'auto' : Automatically determine the validation strategy based on the dataset shape.     - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets.      - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets.      - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.</p> <p>validation_fraction : float, default=0.2   EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.</p> <p>early_stop : int, default=None     Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered.</p> <p>scorers_early_stop_tol :      -list of floats         list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged         If an index of the list is None, that item will not be used for early stopping     -int          If an int is given, it will be used as the tolerance for all objectives</p> <p>other_objectives_early_stop_tol :      -list of floats         list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged         If an index of the list is None, that item will not be used for early stopping     -int          If an int is given, it will be used as the tolerance for all objectives</p> <p>threshold_evaluation_early_stop : list [start, end], default=None     starting and ending percentile to use as a threshold for the evaluation early stopping.     Values between 0 and 100.</p> <p>threshold_evaluation_scaling : float [0,inf), default=0.5     A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.     Must be greater than zero. Higher numbers will move the threshold to the end faster.</p> <p>selection_evaluation_early_stop : list, default=None     A lower and upper percent of the population size to select each round of CV.     Values between 0 and 1.</p> <p>selection_evaluation_scaling : float, default=0.5      A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.     Must be greater than zero. Higher numbers will move the threshold to the end faster.    </p> <p>min_history_threshold : int, default=0     The minimum number of previous scores needed before using threshold early stopping.</p> <p>evolver : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver), default=eaNSGA2_Evolver     The evolver to use for the optimization process. See tpot2.evolutionary_algorithms     - type : an type or subclass of a BaseEvolver     - \"nsga2\" : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver</p> <p>survival_percentage : float, default=1     Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive.      For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.</p> <p>crossover_probability : float, default=.2     Probability of generating a new individual by crossover between two individuals.</p> <p>mutate_probability : float, default=.7     Probability of generating a new individual by crossover between one individuals.</p> <p>mutate_then_crossover_probability : float, default=.05     Probability of generating a new individual by mutating two individuals followed by crossover.</p> <p>crossover_then_mutate_probability : float, default=.05     Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.</p> <p>n_parents : int, default=2     Number of parents to use for crossover. Must be greater than 1.</p> <p>survival_selector : function, default=survival_select_NSGA2     Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.     Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.</p> <p>parent_selector : function, default=parent_select_NSGA2     Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.</p> <p>budget_range : list [start, end], default=None     A starting and ending budget to use for the budget scaling.</p> <p>budget_scaling float : [0,1], default=0.5     A scaling factor to use when determining how fast we move the budget from the start to end budget.</p> <p>generations_until_end_budget : int, default=1     The number of generations to run before reaching the max budget.</p> <p>stepwise_steps : int, default=1     The number of staircase steps to take when scaling the budget and population size.</p> <p>n_jobs : int, default=1     Number of processes to run in parallel.</p> <p>memory_limit : str, default=\"4GB\"     Memory limit for each job. See Dask LocalCluster documentation for more information.</p> <p>client : dask.distributed.Client, default=None     A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit. </p> <p>processes : bool, default=True     If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.     True seems to perform better. However, False is required for interactive debugging.</p> <p>warm_start : bool, default=False     If True, will use the continue the evolutionary algorithm from the last generation of the previous run.</p> <p>subset_column : str or int, default=None     EXPERIMENTAL The column to use for the subset selection. Must also pass in unique_subset_values to GraphIndividual to function.</p> <p>periodic_checkpoint_folder : str, default=None     Folder to save the population to periodically. If None, no periodic saving will be done.     If provided, training will resume from this checkpoint.</p> <p>callback : tpot2.CallBackInterface, default=None     Callback object. Not implemented</p> <p>verbose : int, default=1      How much information to print during the optimization process. Higher values include the information from lower values.     0. nothing     1. progress bar</p> <pre><code>3. best individual\n4. warnings\n&gt;=5. full warnings trace\n6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_seconds in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n</code></pre> <p>Attributes:</p> Name Type Description <code>fitted_pipeline_</code> <code>GraphPipeline</code> <p>A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.</p> <p>evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run.      Columns:      - objective functions : The first few columns correspond to the passed in scorers and objective functions     - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.     - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.     - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.     - Generation : The generation the pipeline first appeared.      - Pareto_Front      : The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual.                      To save on computational time, the best frontier is updated iteratively each generation.                      The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.                     All other pipelines are set to NaN.      - Instance  : The unfitted GraphPipeline BaseEstimator.      - validation objective functions : Objective function scores evaluated on the validation set.     - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.</p> <p>pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.</p> Source code in <code>tpot2/tpot_estimator/estimator.py</code> <pre><code>def __init__(self,  scorers, \nscorers_weights,\nclassification,\ncv = 5,\nother_objective_functions=[],\nother_objective_functions_weights = [],\nobjective_function_names = None,\nbigger_is_better = True,\nmax_size = np.inf, \nlinear_pipeline = False,\nroot_config_dict= 'Auto',\ninner_config_dict=[\"selectors\", \"transformers\"],\nleaf_config_dict= None,                        \ncross_val_predict_cv = 0,\ncategorical_features = None,\nsubsets = None,\nmemory = None,\npreprocessing = False,\npopulation_size = 50,\ninitial_population_size = None,\npopulation_scaling = .5, \ngenerations_until_end_population = 1,  \ngenerations = 50,\nmax_time_seconds=float('inf'), \nmax_eval_time_seconds=60*10, \nvalidation_strategy = \"none\",\nvalidation_fraction = .2,\n#early stopping parameters \nearly_stop = None,\nscorers_early_stop_tol = 0.001,\nother_objectives_early_stop_tol =None,\nthreshold_evaluation_early_stop = None, \nthreshold_evaluation_scaling = .5,\nselection_evaluation_early_stop = None, \nselection_evaluation_scaling = .5, \nmin_history_threshold = 20,\n#evolver parameters\nsurvival_percentage = 1,\ncrossover_probability=.2,\nmutate_probability=.7,\nmutate_then_crossover_probability=.05,\ncrossover_then_mutate_probability=.05,\nn_parents = 2,\nsurvival_selector = survival_select_NSGA2,\nparent_selector = tournament_selection_dominated,\n#budget parameters\nbudget_range = None,\nbudget_scaling = .5,\ngenerations_until_end_budget = 1,  \nstepwise_steps = 5,\noptuna_optimize_pareto_front = False,\noptuna_optimize_pareto_front_trials = 100,\noptuna_optimize_pareto_front_timeout = 60*10,\noptuna_storage = \"sqlite:///optuna.db\",\n#dask parameters\nn_jobs=1,\nmemory_limit = \"4GB\",\nclient = None,\nprocesses = True,\n#debugging and logging parameters\nwarm_start = False,\nsubset_column = None,\nperiodic_checkpoint_folder = None, \ncallback = None,\nverbose = 0,\nscatter = True,\n):\n'''\n    An sklearn baseestimator that uses genetic programming to optimize a pipeline.\n    Parameters\n    ----------\n    scorers : (list, scorer)\n        A scorer or list of scorers to be used in the cross-validation process. \n        see https://scikit-learn.org/stable/modules/model_evaluation.html\n    scorers_weights : list\n        A list of weights to be applied to the scorers during the optimization process.\n    classification : bool\n        If True, the problem is treated as a classification problem. If False, the problem is treated as a regression problem.\n        Used to determine the CV strategy.\n    cv : int, cross-validator\n        - (int): Number of folds to use in the cross-validation process. By uses the sklearn.model_selection.KFold cross-validator for regression and StratifiedKFold for classification. In both cases, shuffled is set to True.\n        - (sklearn.model_selection.BaseCrossValidator): A cross-validator to use in the cross-validation process.\n            - max_depth (int): The maximum depth from any node to the root of the pipelines to be generated.\n    other_objective_functions : list, default=[tpot2.objectives.estimator_objective_functions.average_path_length_objective]\n        A list of other objective functions to apply to the pipeline.\n    other_objective_functions_weights : list, default=[-1]\n        A list of weights to be applied to the other objective functions.\n    objective_function_names : list, default=None\n        A list of names to be applied to the objective functions. If None, will use the names of the objective functions.\n    bigger_is_better : bool, default=True\n        If True, the objective function is maximized. If False, the objective function is minimized. Use negative weights to reverse the direction.\n    max_size : int, default=np.inf\n        The maximum number of nodes of the pipelines to be generated.\n    linear_pipeline : bool, default=False\n        If True, the pipelines generated will be linear. If False, the pipelines generated will be directed acyclic graphs.\n    root_config_dict : dict, default='auto'\n        The configuration dictionary to use for the root node of the model.\n        If 'auto', will use \"classifiers\" if classification=True, else \"regressors\".\n        - 'selectors' : A selection of sklearn Selector methods.\n        - 'classifiers' : A selection of sklearn Classifier methods.\n        - 'regressors' : A selection of sklearn Regressor methods.\n        - 'transformers' : A selection of sklearn Transformer methods.\n        - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n        - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n        - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                    Subsets are set with the subsets parameter.\n        - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n        - 'MDR' : Includes MDR.\n        - 'ContinuousMDR' : Includes ContinuousMDR.\n        - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n        - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n        - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n    inner_config_dict : dict, default=[\"selectors\", \"transformers\"]\n        The configuration dictionary to use for the inner nodes of the model generation.\n        Default [\"selectors\", \"transformers\"]\n        - 'selectors' : A selection of sklearn Selector methods.\n        - 'classifiers' : A selection of sklearn Classifier methods.\n        - 'regressors' : A selection of sklearn Regressor methods.\n        - 'transformers' : A selection of sklearn Transformer methods.\n        - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n        - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n        - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                    Subsets are set with the subsets parameter.\n        - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n        - 'MDR' : Includes MDR.\n        - 'ContinuousMDR' : Includes ContinuousMDR.\n        - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n        - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n        - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n        - None : If None and max_depth&gt;1, the root_config_dict will be used for the inner nodes as well.\n    leaf_config_dict : dict, default=None \n        The configuration dictionary to use for the leaf node of the model. If set, leaf nodes must be from this dictionary.\n        Otherwise leaf nodes will be generated from the root_config_dict. \n        Default None\n        - 'selectors' : A selection of sklearn Selector methods.\n        - 'classifiers' : A selection of sklearn Classifier methods.\n        - 'regressors' : A selection of sklearn Regressor methods.\n        - 'transformers' : A selection of sklearn Transformer methods.\n        - 'arithmetic_transformer' : A selection of sklearn Arithmetic Transformer methods that replicate symbolic classification/regression operators.\n        - 'passthrough' : A node that just passes though the input. Useful for passing through raw inputs into inner nodes.\n        - 'feature_set_selector' : A selector that pulls out specific subsets of columns from the data. Only well defined as a leaf.\n                                    Subsets are set with the subsets parameter.\n        - 'skrebate' : Includes ReliefF, SURF, SURFstar, MultiSURF.\n        - 'MDR' : Includes MDR.\n        - 'ContinuousMDR' : Includes ContinuousMDR.\n        - 'genetic encoders' : Includes Genetic Encoder methods as used in AutoQTL.\n        - 'FeatureEncodingFrequencySelector': Includes FeatureEncodingFrequencySelector method as used in AutoQTL.\n        - list : a list of strings out of the above options to include the corresponding methods in the configuration dictionary.\n        - None : If None, a leaf will not be required (i.e. the pipeline can be a single root node). Leaf nodes will be generated from the inner_config_dict.\n    cross_val_predict_cv : int, default=0\n        Number of folds to use for the cross_val_predict function for inner classifiers and regressors. Estimators will still be fit on the full dataset, but the following node will get the outputs from cross_val_predict.\n        - 0-1 : When set to 0 or 1, the cross_val_predict function will not be used. The next layer will get the outputs from fitting and transforming the full dataset.\n        - &gt;=2 : When fitting pipelines with inner classifiers or regressors, they will still be fit on the full dataset. \n                However, the output to the next node will come from cross_val_predict with the specified number of folds.\n    categorical_features: list or None\n        Categorical columns to inpute and/or one hot encode during the preprocessing step. Used only if preprocessing is not False.\n        - None : If None, TPOT2 will automatically use object columns in pandas dataframes as objects for one hot encoding in preprocessing.\n        - List of categorical features. If X is a dataframe, this should be a list of column names. If X is a numpy array, this should be a list of column indices\n    subsets : str or list, default=None\n        Sets the subsets that the FeatureSetSeletor will select from if set as an option in one of the configuration dictionaries.\n        - str : If a string, it is assumed to be a path to a csv file with the subsets. \n            The first column is assumed to be the name of the subset and the remaining columns are the features in the subset.\n        - list or np.ndarray : If a list or np.ndarray, it is assumed to be a list of subsets.\n        - None : If None, each column will be treated as a subset. One column will be selected per subset.\n        If subsets is None, each column will be treated as a subset. One column will be selected per subset.\n    memory: Memory object or string, default=None\n        If supplied, pipeline will cache each transformer after calling fit. This feature\n        is used to avoid computing the fit transformers within a pipeline if the parameters\n        and input data are identical with another fitted pipeline during optimization process.\n        - String 'auto':\n            TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n        - String path of a caching directory\n            TPOT uses memory caching with the provided directory and TPOT does NOT clean\n            the caching directory up upon shutdown. If the directory does not exist, TPOT will\n            create it.\n        - Memory object:\n            TPOT uses the instance of joblib.Memory for memory caching,\n            and TPOT does NOT clean the caching directory up upon shutdown.\n        - None:\n            TPOT does not use memory caching.\n    preprocessing : bool or BaseEstimator/Pipeline, \n        EXPERIMENTAL\n        A pipeline that will be used to preprocess the data before CV.\n        - bool : If True, will use a default preprocessing pipeline.\n        - Pipeline : If an instance of a pipeline is given, will use that pipeline as the preprocessing pipeline.\n    population_size : int, default=50\n        Size of the population\n    initial_population_size : int, default=None\n        Size of the initial population. If None, population_size will be used.\n    population_scaling : int, default=0.5\n        Scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n    generations_until_end_population : int, default=1  \n        Number of generations until the population size reaches population_size            \n    generations : int, default=50\n        Number of generations to run\n    max_time_seconds : float, default=float(\"inf\")\n        Maximum time to run the optimization. If none or inf, will run until the end of the generations.\n    max_eval_time_seconds : float, default=60*5\n        Maximum time to evaluate a single individual. If none or inf, there will be no time limit per evaluation.\n    validation_strategy : str, default='none'\n        EXPERIMENTAL The validation strategy to use for selecting the final pipeline from the population. TPOT2 may overfit the cross validation score. A second validation set can be used to select the final pipeline.\n        - 'auto' : Automatically determine the validation strategy based on the dataset shape.\n        - 'reshuffled' : Use the same data for cross validation and final validation, but with different splits for the folds. This is the default for small datasets. \n        - 'split' : Use a separate validation set for final validation. Data will be split according to validation_fraction. This is the default for medium datasets. \n        - 'none' : Do not use a separate validation set for final validation. Select based on the original cross-validation score. This is the default for large datasets.\n    validation_fraction : float, default=0.2\n      EXPERIMENTAL The fraction of the dataset to use for the validation set when validation_strategy is 'split'. Must be between 0 and 1.\n    early_stop : int, default=None\n        Number of generations without improvement before early stopping. All objectives must have converged within the tolerance for this to be triggered.\n    scorers_early_stop_tol : \n        -list of floats\n            list of tolerances for each scorer. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int \n            If an int is given, it will be used as the tolerance for all objectives\n    other_objectives_early_stop_tol : \n        -list of floats\n            list of tolerances for each of the other objective function. If the difference between the best score and the current score is less than the tolerance, the individual is considered to have converged\n            If an index of the list is None, that item will not be used for early stopping\n        -int \n            If an int is given, it will be used as the tolerance for all objectives\n    threshold_evaluation_early_stop : list [start, end], default=None\n        starting and ending percentile to use as a threshold for the evaluation early stopping.\n        Values between 0 and 100.\n    threshold_evaluation_scaling : float [0,inf), default=0.5\n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.\n    selection_evaluation_early_stop : list, default=None\n        A lower and upper percent of the population size to select each round of CV.\n        Values between 0 and 1.\n    selection_evaluation_scaling : float, default=0.5 \n        A scaling factor to use when determining how fast we move the threshold moves from the start to end percentile.\n        Must be greater than zero. Higher numbers will move the threshold to the end faster.    \n    min_history_threshold : int, default=0\n        The minimum number of previous scores needed before using threshold early stopping.\n    evolver : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver), default=eaNSGA2_Evolver\n        The evolver to use for the optimization process. See tpot2.evolutionary_algorithms\n        - type : an type or subclass of a BaseEvolver\n        - \"nsga2\" : tpot2.evolutionary_algorithms.eaNSGA2.eaNSGA2_Evolver\n    survival_percentage : float, default=1\n        Percentage of the population size to utilize for mutation and crossover at the beginning of the generation. The rest are discarded. Individuals are selected with the selector passed into survival_selector. The value of this parameter must be between 0 and 1, inclusive. \n        For example, if the population size is 100 and the survival percentage is .5, 50 individuals will be selected with NSGA2 from the existing population. These will be used for mutation and crossover to generate the next 100 individuals for the next generation. The remainder are discarded from the live population. In the next generation, there will now be the 50 parents + the 100 individuals for a total of 150. Surivival percentage is based of the population size parameter and not the existing population size (current population size when using successive halving). Therefore, in the next generation we will still select 50 individuals from the currently existing 150.\n    crossover_probability : float, default=.2\n        Probability of generating a new individual by crossover between two individuals.\n    mutate_probability : float, default=.7\n        Probability of generating a new individual by crossover between one individuals.\n    mutate_then_crossover_probability : float, default=.05\n        Probability of generating a new individual by mutating two individuals followed by crossover.\n    crossover_then_mutate_probability : float, default=.05\n        Probability of generating a new individual by crossover between two individuals followed by a mutation of the resulting individual.\n    n_parents : int, default=2\n        Number of parents to use for crossover. Must be greater than 1.\n    survival_selector : function, default=survival_select_NSGA2\n        Function to use to select individuals for survival. Must take a matrix of scores and return selected indexes.\n        Used to selected population_size * survival_percentage individuals at the start of each generation to use for mutation and crossover.\n    parent_selector : function, default=parent_select_NSGA2\n        Function to use to select pairs parents for crossover and individuals for mutation. Must take a matrix of scores and return selected indexes.\n    budget_range : list [start, end], default=None\n        A starting and ending budget to use for the budget scaling.\n    budget_scaling float : [0,1], default=0.5\n        A scaling factor to use when determining how fast we move the budget from the start to end budget.\n    generations_until_end_budget : int, default=1\n        The number of generations to run before reaching the max budget.\n    stepwise_steps : int, default=1\n        The number of staircase steps to take when scaling the budget and population size.\n    n_jobs : int, default=1\n        Number of processes to run in parallel.\n    memory_limit : str, default=\"4GB\"\n        Memory limit for each job. See Dask [LocalCluster documentation](https://distributed.dask.org/en/stable/api.html#distributed.Client) for more information.\n    client : dask.distributed.Client, default=None\n        A dask client to use for parallelization. If not None, this will override the n_jobs and memory_limit parameters. If None, will create a new client with num_workers=n_jobs and memory_limit=memory_limit. \n    processes : bool, default=True\n        If True, will use multiprocessing to parallelize the optimization process. If False, will use threading.\n        True seems to perform better. However, False is required for interactive debugging.\n    warm_start : bool, default=False\n        If True, will use the continue the evolutionary algorithm from the last generation of the previous run.\n    subset_column : str or int, default=None\n        EXPERIMENTAL The column to use for the subset selection. Must also pass in unique_subset_values to GraphIndividual to function.\n    periodic_checkpoint_folder : str, default=None\n        Folder to save the population to periodically. If None, no periodic saving will be done.\n        If provided, training will resume from this checkpoint.\n    callback : tpot2.CallBackInterface, default=None\n        Callback object. Not implemented\n    verbose : int, default=1 \n        How much information to print during the optimization process. Higher values include the information from lower values.\n        0. nothing\n        1. progress bar\n        3. best individual\n        4. warnings\n        &gt;=5. full warnings trace\n        6. evaluations progress bar. (Temporary: This used to be 2. Currently, using evaluation progress bar may prevent some instances were we terminate a generation early due to it reaching max_time_seconds in the middle of a generation OR a pipeline failed to be terminated normally and we need to manually terminate it.)\n    Attributes\n    ----------\n    fitted_pipeline_ : GraphPipeline\n        A fitted instance of the GraphPipeline that inherits from sklearn BaseEstimator. This is fitted on the full X, y passed to fit.\n    evaluated_individuals : A pandas data frame containing data for all evaluated individuals in the run. \n        Columns: \n        - *objective functions : The first few columns correspond to the passed in scorers and objective functions\n        - Parents : A tuple containing the indexes of the pipelines used to generate the pipeline of that row. If NaN, this pipeline was generated randomly in the initial population.\n        - Variation_Function : Which variation function was used to mutate or crossover the parents. If NaN, this pipeline was generated randomly in the initial population.\n        - Individual : The internal representation of the individual that is used during the evolutionary algorithm. This is not an sklearn BaseEstimator.\n        - Generation : The generation the pipeline first appeared. \n        - Pareto_Front\t: The nondominated front that this pipeline belongs to. 0 means that its scores is not strictly dominated by any other individual. \n                        To save on computational time, the best frontier is updated iteratively each generation. \n                        The pipelines with the 0th pareto front do represent the exact best frontier. However, the pipelines with pareto front &gt;= 1 are only in reference to the other pipelines in the final population.\n                        All other pipelines are set to NaN. \n        - Instance\t: The unfitted GraphPipeline BaseEstimator. \n        - *validation objective functions : Objective function scores evaluated on the validation set.\n        - Validation_Pareto_Front : The full pareto front calculated on the validation set. This is calculated for all pipelines with Pareto_Front equal to 0. Unlike the Pareto_Front which only calculates the frontier and the final population, the Validation Pareto Front is calculated for all pipelines tested on the validation set.\n    pareto_front : The same pandas dataframe as evaluated individuals, but containing only the frontier pareto front pipelines.\n    '''\n# sklearn BaseEstimator must have a corresponding attribute for each parameter.\n# These should not be modified once set.\nself.scorers = scorers\nself.scorers_weights = scorers_weights\nself.classification = classification\nself.cv = cv\nself.other_objective_functions = other_objective_functions\nself.other_objective_functions_weights = other_objective_functions_weights\nself.objective_function_names = objective_function_names\nself.bigger_is_better = bigger_is_better\nself.max_size = max_size\nself.linear_pipeline = linear_pipeline\nself.root_config_dict= root_config_dict\nself.inner_config_dict= inner_config_dict\nself.leaf_config_dict= leaf_config_dict\nself.cross_val_predict_cv = cross_val_predict_cv\nself.categorical_features = categorical_features\nself.subsets = subsets\nself.memory = memory\nself.preprocessing = preprocessing\nself.validation_strategy = validation_strategy\nself.validation_fraction = validation_fraction\nself.population_size = population_size\nself.initial_population_size = initial_population_size\nself.population_scaling = population_scaling\nself.generations_until_end_population = generations_until_end_population\nself.generations = generations\nself.early_stop = early_stop\nself.scorers_early_stop_tol = scorers_early_stop_tol\nself.other_objectives_early_stop_tol = other_objectives_early_stop_tol\nself.max_time_seconds = max_time_seconds \nself.max_eval_time_seconds = max_eval_time_seconds\nself.n_jobs= n_jobs\nself.memory_limit = memory_limit\nself.client = client\nself.survival_percentage = survival_percentage\nself.crossover_probability = crossover_probability\nself.mutate_probability = mutate_probability\nself.mutate_then_crossover_probability= mutate_then_crossover_probability\nself.crossover_then_mutate_probability= crossover_then_mutate_probability\nself.survival_selector=survival_selector\nself.parent_selector=parent_selector\nself.budget_range = budget_range\nself.budget_scaling = budget_scaling\nself.generations_until_end_budget = generations_until_end_budget\nself.stepwise_steps = stepwise_steps\nself.threshold_evaluation_early_stop =threshold_evaluation_early_stop\nself.threshold_evaluation_scaling =  threshold_evaluation_scaling\nself.min_history_threshold = min_history_threshold\nself.selection_evaluation_early_stop = selection_evaluation_early_stop\nself.selection_evaluation_scaling =  selection_evaluation_scaling\nself.warm_start = warm_start\nself.subset_column = subset_column\nself.verbose = verbose\nself.periodic_checkpoint_folder = periodic_checkpoint_folder\nself.callback = callback\nself.processes = processes\nself.scatter = scatter\nself.optuna_optimize_pareto_front = optuna_optimize_pareto_front\nself.optuna_optimize_pareto_front_trials = optuna_optimize_pareto_front_trials\nself.optuna_optimize_pareto_front_timeout = optuna_optimize_pareto_front_timeout\nself.optuna_storage = optuna_storage\n#Initialize other used params\nif self.initial_population_size is None:\nself._initial_population_size = self.population_size\nelse:\nself._initial_population_size = self.initial_population_size\nif isinstance(self.scorers, str):\nself._scorers = [self.scorers]\nelif callable(self.scorers):\nself._scorers = [self.scorers]\nelse:\nself._scorers = self.scorers\nself._scorers = [sklearn.metrics.get_scorer(scoring) for scoring in self._scorers]\nself._scorers_early_stop_tol = self.scorers_early_stop_tol\nself._evolver = tpot2.evolvers.BaseEvolver\nself.objective_function_weights = [*scorers_weights, *other_objective_functions_weights]\nif self.objective_function_names is None:\nobj_names = [f.__name__ for f in other_objective_functions]\nelse:\nobj_names = self.objective_function_names\nself.objective_names = [f._score_func.__name__ if hasattr(f,\"_score_func\") else f.__name__ for f in self._scorers] + obj_names\nif not isinstance(self.other_objectives_early_stop_tol, list):\nself._other_objectives_early_stop_tol = [self.other_objectives_early_stop_tol for _ in range(len(self.other_objective_functions))]\nelse:\nself._other_objectives_early_stop_tol = self.other_objectives_early_stop_tol\nif not isinstance(self._scorers_early_stop_tol, list):\nself._scorers_early_stop_tol = [self._scorers_early_stop_tol for _ in range(len(self._scorers))]\nelse:\nself._scorers_early_stop_tol = self._scorers_early_stop_tol\nself.early_stop_tol = [*self._scorers_early_stop_tol, *self._other_objectives_early_stop_tol]\nself._evolver_instance = None\nself.evaluated_individuals = None\nset_dask_settings()\n</code></pre>"},{"location":"tpot2_api/regressor/","title":"Regressor","text":"<p>         Bases: <code>TPOTEstimator</code></p> Source code in <code>tpot2/tpot_estimator/templates/tpottemplates.py</code> <pre><code>class TPOTRegressor(TPOTEstimator):\ndef __init__(       self,\nscorers=['neg_mean_squared_error'], \nscorers_weights=[1],\nother_objective_functions=[], #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = [],\nobjective_function_names = None,\nbigger_is_better = True,\nmax_size = np.inf, \nlinear_pipeline = False,\nroot_config_dict= 'Auto',\ninner_config_dict=[\"selectors\", \"transformers\"],\nleaf_config_dict= None,                        \ncross_val_predict_cv = 0,\ncategorical_features = None,\nsubsets = None,\nmemory = None,\npreprocessing = False,\nmax_time_seconds=float('inf'), \nmax_eval_time_seconds=60*10, \nn_jobs = 1,\nvalidation_strategy = \"none\",\nvalidation_fraction = .2, \nearly_stop = None,\nwarm_start = False,\nperiodic_checkpoint_folder = None, \nverbose = 0,\n):\n\"\"\"\n        See TPOTEstimator for documentation\n        \"\"\"\nsuper(TPOTRegressor,self).__init__(\nscorers=scorers, \nscorers_weights=scorers_weights,\ncv=5,\nother_objective_functions=other_objective_functions, #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = other_objective_functions_weights,\nobjective_function_names = objective_function_names,\nbigger_is_better = bigger_is_better,\nmax_size = max_size, \nlinear_pipeline = linear_pipeline,\nroot_config_dict = root_config_dict,\ninner_config_dict=inner_config_dict,\nleaf_config_dict= leaf_config_dict,                        \ncross_val_predict_cv = cross_val_predict_cv,\ncategorical_features = categorical_features,\nsubsets = subsets,\nmemory = memory,\npreprocessing = preprocessing,\nmax_time_seconds=max_time_seconds, \nmax_eval_time_seconds=max_eval_time_seconds, \nn_jobs=n_jobs,\nvalidation_strategy = validation_strategy,\nvalidation_fraction = validation_fraction, \nearly_stop = early_stop,\nwarm_start = warm_start,\nperiodic_checkpoint_folder = periodic_checkpoint_folder, \nverbose = verbose,\nclassification=False,\n)\n</code></pre>"},{"location":"tpot2_api/regressor/#tpot2.tpot_estimator.templates.tpottemplates.TPOTRegressor.__init__","title":"<code>__init__(scorers=['neg_mean_squared_error'], scorers_weights=[1], other_objective_functions=[], other_objective_functions_weights=[], objective_function_names=None, bigger_is_better=True, max_size=np.inf, linear_pipeline=False, root_config_dict='Auto', inner_config_dict=['selectors', 'transformers'], leaf_config_dict=None, cross_val_predict_cv=0, categorical_features=None, subsets=None, memory=None, preprocessing=False, max_time_seconds=float('inf'), max_eval_time_seconds=60 * 10, n_jobs=1, validation_strategy='none', validation_fraction=0.2, early_stop=None, warm_start=False, periodic_checkpoint_folder=None, verbose=0)</code>","text":"<p>See TPOTEstimator for documentation</p> Source code in <code>tpot2/tpot_estimator/templates/tpottemplates.py</code> <pre><code>    def __init__(       self,\nscorers=['neg_mean_squared_error'], \nscorers_weights=[1],\nother_objective_functions=[], #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = [],\nobjective_function_names = None,\nbigger_is_better = True,\nmax_size = np.inf, \nlinear_pipeline = False,\nroot_config_dict= 'Auto',\ninner_config_dict=[\"selectors\", \"transformers\"],\nleaf_config_dict= None,                        \ncross_val_predict_cv = 0,\ncategorical_features = None,\nsubsets = None,\nmemory = None,\npreprocessing = False,\nmax_time_seconds=float('inf'), \nmax_eval_time_seconds=60*10, \nn_jobs = 1,\nvalidation_strategy = \"none\",\nvalidation_fraction = .2, \nearly_stop = None,\nwarm_start = False,\nperiodic_checkpoint_folder = None, \nverbose = 0,\n):\n\"\"\"\n        See TPOTEstimator for documentation\n        \"\"\"\nsuper(TPOTRegressor,self).__init__(\nscorers=scorers, \nscorers_weights=scorers_weights,\ncv=5,\nother_objective_functions=other_objective_functions, #tpot2.objectives.estimator_objective_functions.number_of_nodes_objective],\nother_objective_functions_weights = other_objective_functions_weights,\nobjective_function_names = objective_function_names,\nbigger_is_better = bigger_is_better,\nmax_size = max_size, \nlinear_pipeline = linear_pipeline,\nroot_config_dict = root_config_dict,\ninner_config_dict=inner_config_dict,\nleaf_config_dict= leaf_config_dict,                        \ncross_val_predict_cv = cross_val_predict_cv,\ncategorical_features = categorical_features,\nsubsets = subsets,\nmemory = memory,\npreprocessing = preprocessing,\nmax_time_seconds=max_time_seconds, \nmax_eval_time_seconds=max_eval_time_seconds, \nn_jobs=n_jobs,\nvalidation_strategy = validation_strategy,\nvalidation_fraction = validation_fraction, \nearly_stop = early_stop,\nwarm_start = warm_start,\nperiodic_checkpoint_folder = periodic_checkpoint_folder, \nverbose = verbose,\nclassification=False,\n)\n</code></pre>"}]}